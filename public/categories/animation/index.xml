<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>animation on Robert W. Walker</title>
    <link>/categories/animation/</link>
    <description>Recent content in animation on Robert W. Walker</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Mon, 16 Dec 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/categories/animation/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>tidyTuesday Measles</title>
      <link>/post/tidytuesday-measles/</link>
      <pubDate>Mon, 16 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/tidytuesday-measles/</guid>
      <description>tidyTuesday: December 10, 2019 Replicating plots from simplystatistics. One nice twist is the development of a tidytuesdayR package to grab the necessary data in an easy way. You can install the package via github.
tuesdata &amp;lt;- tidytuesdayR::tt_load(2019, week = 50) library(here) ## here() starts at /home/rob/Sandbox/academic-mymod/academic-mymod library(tidyverse) ## ── Attaching packages ──────────────────────────────────────────────────────────────────────────────── tidyverse 1.2.1 ── ## ✓ ggplot2 3.2.1 ✓ purrr 0.3.3 ## ✓ tibble 2.1.3 ✓ dplyr 0.8.3 ## ✓ tidyr 1.</description>
    </item>
    
    <item>
      <title>Philaselphia Parking Tickets: a tidyTuesday</title>
      <link>/post/philaselphia-parking-tickets-a-tidytuesday/</link>
      <pubDate>Mon, 09 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/philaselphia-parking-tickets-a-tidytuesday/</guid>
      <description>Philadelphia Map Use ggmap for the base layer.
library(ggmap); library(osmdata); library(tidyverse) PHI &amp;lt;- get_map(getbb(&amp;quot;Philadelphia, PA&amp;quot;), maptype = &amp;quot;stamen&amp;quot;, zoom=12)  Get the Tickets Data TidyTuesday covers 1.26 million parking tickets in Philadelphia.
tickets &amp;lt;- readr::read_csv(&amp;quot;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-12-03/tickets.csv&amp;quot;) ## Parsed with column specification: ## cols( ## violation_desc = col_character(), ## issue_datetime = col_datetime(format = &amp;quot;&amp;quot;), ## fine = col_double(), ## issuing_agency = col_character(), ## lat = col_double(), ## lon = col_double(), ## zip_code = col_double() ## )  Two Lines of Code Left library(lubridate); library(ggthemes) tickets &amp;lt;- tickets %&amp;gt;% mutate(Day = wday(issue_datetime, label=TRUE)) # use lubridate to extract the day of the month.</description>
    </item>
    
    <item>
      <title>US Census Mapping</title>
      <link>/post/uscensus-mapping/</link>
      <pubDate>Mon, 09 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/uscensus-mapping/</guid>
      <description>Searching and Mapping the Census  Searching for the Asian Population via the Census To use tidycensus, there are limitations imposed by the available tables. There is ACS – a survey of about 3 million people – and the two main decennial census files [SF1] and [SF2]. I will search SF1 for the Asian population.
library(tidycensus); library(kableExtra) library(tidyverse); library(stringr) v10 &amp;lt;- load_variables(2010, &amp;quot;sf1&amp;quot;, cache = TRUE) v10 %&amp;gt;% filter(str_detect(concept, &amp;quot;ASIAN&amp;quot;)) %&amp;gt;% filter(str_detect(label, &amp;quot;Female&amp;quot;)) %&amp;gt;% kable() %&amp;gt;% scroll_box(width = &amp;quot;100%&amp;quot;)    name  label  concept      P012D026  Total!</description>
    </item>
    
    <item>
      <title>Fariss Human Rights Data with Animation</title>
      <link>/post/fariss-human-rights-data-with-animation/</link>
      <pubDate>Thu, 10 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/fariss-human-rights-data-with-animation/</guid>
      <description>Fariss Data Is neat and complete.
load(&amp;quot;~/FarissHRData.RData&amp;quot;) skimr::skim(HR.Data) ## Skim summary statistics ## n obs: 11717 ## n variables: 27 ## ## ── Variable type:factor ──────────────────────────────────────────────────────────────────────── ## variable missing complete n n_unique top_counts ## COW_YEAR 0 11717 11717 11717 100: 1, 100: 1, 100: 1, 100: 1 ## ordered ## FALSE ## ## ── Variable type:integer ─────────────────────────────────────────────────────────────────────── ## variable missing complete n mean sd p0 p25 ## Amnesty 6140 5577 11717 2.</description>
    </item>
    
    <item>
      <title>nflscrapR is amazing</title>
      <link>/post/nflscrapr-is-amazing/</link>
      <pubDate>Tue, 30 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/nflscrapr-is-amazing/</guid>
      <description>Scraping NFL data Note: An original version of this post had issues induced by overtime games. There is a better way to handle all of this that I learned from a brief analysis of a tie game between Cleveland and Pittsburgh in Week One.
The nflscrapR package is designed to make data on NFL games more easily available. To install the package, we need to grab it from github.</description>
    </item>
    
    <item>
      <title>Visualisation with Archigos: Leaders of the World</title>
      <link>/post/visualisation-with-archigos-leaders-of-the-world/</link>
      <pubDate>Sun, 14 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/visualisation-with-archigos-leaders-of-the-world/</guid>
      <description>Archigos Is an amazing collaboration that produced a comprehensive dataset of world leaders going pretty far back; see Archigos on the web. For thinking about leadership, it is quite natural. In this post, I want to do some reshaping into country year and leader year datasets and explore the basic confines of Archigos. I also want to use gganimate for a few things. So what do we know?
library(lubridate) library(tidyverse) library(ggthemes) library(stringr) library(gganimate) library(emoGG) library(emojifont) library(haven) Archigos &amp;lt;- read_dta(url(&amp;quot;http://www.</description>
    </item>
    
  </channel>
</rss>