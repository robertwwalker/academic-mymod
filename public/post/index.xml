<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Robert W. Walker</title>
    <link>https://www.data.viajes/post/</link>
    <description>Recent content in Posts on Robert W. Walker</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sun, 01 Jan 2017 00:00:00 -0800</lastBuildDate>
    
	<atom:link href="https://www.data.viajes/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Trump Tweet Word Clouds</title>
      <link>https://www.data.viajes/post/trump-tweet-word-clouds/</link>
      <pubDate>Tue, 18 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.data.viajes/post/trump-tweet-word-clouds/</guid>
      <description>Mining Twitter Data Is rather easy. You have to arrange a developer account with Twitter and set up an app. After that, Twitter gives you access to a consumer key and secret and an access token and access secret. Those can be embedded in OAuth to automate searches and the retrieval of data. My tool of choice for this is rtweet The first section involves setting up a token.</description>
    </item>
    
    <item>
      <title>tidyTuesday meets the Economics of Majors</title>
      <link>https://www.data.viajes/post/tidytuesday-meets-the-economics-of-majors/</link>
      <pubDate>Wed, 17 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.data.viajes/post/tidytuesday-meets-the-economics-of-majors/</guid>
      <description>This week’s tidyTuesday focuses on degrees and majors and their deployment in the labor market. The original data came from 538. A description of sources and measures. The tidyTesday writeup is here.
library(tidyverse) options(scipen=6) library(extrafont) font_import() ## Importing fonts may take a few minutes, depending on the number of fonts and the speed of the system. ## Continue? [y/n] Major.Employment &amp;lt;- read.csv(&amp;quot;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2018-10-16/recent-grads.csv&amp;quot;) library(skimr) skim(Major.Employment) ## Skim summary statistics ## n obs: 173 ## n variables: 21 ## ## ── Variable type:factor ─────────────────────────────────────────────────────────────────────────────────────────────── ## variable missing complete n n_unique ## Major 0 173 173 173 ## Major_category 0 173 173 16 ## top_counts ordered ## ACC: 1, ACT: 1, ADV: 1, AER: 1 FALSE ## Eng: 29, Edu: 16, Hum: 15, Bio: 14 FALSE ## ## ── Variable type:integer ────────────────────────────────────────────────────────────────────────────────────────────── ## variable missing complete n mean sd p0 ## College_jobs 0 173 173 12322.</description>
    </item>
    
    <item>
      <title>tidyTuesday: coffee chains</title>
      <link>https://www.data.viajes/post/tidytuesday-coffee-chains/</link>
      <pubDate>Wed, 09 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.data.viajes/post/tidytuesday-coffee-chains/</guid>
      <description>The tidyTuesday for this week is coffee chain locations For this week: 1. The basic link to the #tidyTuesday shows an original article for Week 6.
First, let’s import the data; it is a single Excel spreadsheet. The page notes that starbucks, Tim Horton, and Dunkin Donuts have raw data available.
library(here) ## here() starts at /home/rob/R/MyNLWeb library(readxl) library(tidyverse) ## ── Attaching packages ────────────────────────────────────────────────────────────────────────────── tidyverse 1.2.1 ── ## ✔ ggplot2 3.</description>
    </item>
    
    <item>
      <title>Global mortality tidyTuesday</title>
      <link>https://www.data.viajes/post/tidytuesday-takes-on-global-mortality/</link>
      <pubDate>Wed, 18 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.data.viajes/post/tidytuesday-takes-on-global-mortality/</guid>
      <description>tidyTuesday on Global Mortality The three generic challenge graphics involve two global summaries, a raw count by type and a percentage by type. The individual county breakdowns are recorded for a predetermined year below. This can all be seen in the original. For whatever reason, I cannot open this data remotely.
Here is this week’s tidyTuesday.
library(here) library(readxl) library(skimr) library(tidyverse) library(rlang) global_mortality &amp;lt;- read_excel(paste0(here(),&amp;quot;/data/global_mortality.xlsx&amp;quot;)) skim(global_mortality) ## Skim summary statistics ## n obs: 6156 ## n variables: 35 ## ## ── Variable type:character ──────────────────────────────────────────────────────────────────────────────────────────── ## variable missing complete n min max empty n_unique ## country 0 6156 6156 4 32 0 228 ## country_code 864 5292 6156 3 8 0 196 ## ## ── Variable type:numeric ────────────────────────────────────────────────────────────────────────────────────────────── ## variable missing complete n mean ## Alcohol disorders (%) 0 6156 6156 0.</description>
    </item>
    
    <item>
      <title>Scraping EPL Salary Data</title>
      <link>https://www.data.viajes/post/scraping-epl-salary-data/</link>
      <pubDate>Sun, 08 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.data.viajes/post/scraping-epl-salary-data/</guid>
      <description>EPL Scraping In a previous post, I scraped some NFL data and learned the structure of Sportrac. Now, I want to scrape the available data on the EPL. The EPL data is organized in a few distinct but potentially linked tables. The basic structure is organized around team folders. Let me begin by isolating those URLs.
library(rvest) library(tidyverse) base_url &amp;lt;- &amp;quot;http://www.spotrac.com/epl/&amp;quot; read.base &amp;lt;- read_html(base_url) team.URL &amp;lt;- read.base %&amp;gt;% html_nodes(&amp;quot;.team-name&amp;quot;) %&amp;gt;% html_attr(&amp;#39;href&amp;#39;) team.</description>
    </item>
    
    <item>
      <title>Scraping the NFL Salary Cap Data with Python and R</title>
      <link>https://www.data.viajes/post/scraping-the-nfl-salary-cap-data-with-python-and-r/</link>
      <pubDate>Wed, 04 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.data.viajes/post/scraping-the-nfl-salary-cap-data-with-python-and-r/</guid>
      <description>The NFL Data [SporTrac](http://www.sportrac.com] has a wonderful array of financial data on sports. A student going to work for the Seattle Seahawks wanted the NFL salary cap data and I also found data on the English Premier League there. Now I have a source to scrape the data from.
With a source in hand, the key tool is the SelectorGadget. SelectorGadget is a browser add-in for Chrome that allows us to select text and identify the css or xpath selector to scrape the data.</description>
    </item>
    
    <item>
      <title>tidyTuesday - Tuition</title>
      <link>https://www.data.viajes/post/tidytuesday-tuition/</link>
      <pubDate>Tue, 03 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.data.viajes/post/tidytuesday-tuition/</guid>
      <description>I found a great example on tidyTuesday that I wanted to work on. @JakeKaupp tweeted his #tidyTuesday: a very cool slope plot of tuition changes averaged by state over the last decade. It is a very informative graphic. The only tweak is a simple embedded line plot that uses color in a creative way to show growth rates. All of the R code for this is on Jake Kaupp’s GitHub. The specific file is here.</description>
    </item>
    
    <item>
      <title>Pew Data on Bond Ratings and Rainy Day Funds</title>
      <link>https://www.data.viajes/post/pew-data-on-bond-ratings-and-rainy-day-funds/</link>
      <pubDate>Wed, 07 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.data.viajes/post/pew-data-on-bond-ratings-and-rainy-day-funds/</guid>
      <description>Pew on Rainy Day Funds and Credit Quality The Pew Charitable Trusts released a report last May (2017) that portrays rainy day funds that are well designed and deployed as a form of insurance against ratings downgrades. One the one hand, this is perfectly sensible because the alternatives do not sound like very good ideas. A poorly designed rainy day fund, for example, is going to have to fall short on either the rainy day or the fund.</description>
    </item>
    
    <item>
      <title>Mapping with the Government Finance Database</title>
      <link>https://www.data.viajes/post/mapping-with-the-government-finance-database/</link>
      <pubDate>Sun, 25 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.data.viajes/post/mapping-with-the-government-finance-database/</guid>
      <description>The Government Finance Database Some of my colleagues (Kawika Pierson, Mike Hand, and Fred Thompson) have put together a convenient access point for the Government Finance data available from the Census. They published an article in PLoS One with the rationale; I want to build some maps from their project with extensible code and functions. The overall dataset is enormous. I have downloaded the whole thing and filtered out the states.</description>
    </item>
    
    <item>
      <title>Longitudinal Panel Data R Packages</title>
      <link>https://www.data.viajes/post/panel-data-r-packages/</link>
      <pubDate>Sat, 24 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.data.viajes/post/panel-data-r-packages/</guid>
      <description>Longitudinal and Panel Data Analysis in R Goal: A CRAN task view for panel/longitudinal data analysis in R.
What is Panel Data? Panel data are variously called longitudinal, panel, cross-sectional time series, and pooled time series data. The most precise definition is two-dimensional data; invariably one of the dimensions is time. We can think about a general depiction of what a model with linear coefficients typical for such data structures, though ridiculously overparameterized, like so:</description>
    </item>
    
    <item>
      <title>Black Boxes: A Gender Gap Example</title>
      <link>https://www.data.viajes/post/black-boxes-a-gender-gap-example/</link>
      <pubDate>Thu, 22 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.data.viajes/post/black-boxes-a-gender-gap-example/</guid>
      <description>Variance in the Outcome: The Black Box Regression models engage an exercise in variance accounting. How much of the outcome is explained by the inputs, individually (slope divided by standard error is t) and collectively (Average explained/Average unexplained with averaging over degrees of freedom is F). This, of course, assumes normal errors. This document provides a function for making use of the black box. Just as in common parlance, a black box is the unexplained.</description>
    </item>
    
    <item>
      <title>Correlation Function</title>
      <link>https://www.data.viajes/post/correlation-function/</link>
      <pubDate>Thu, 22 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.data.viajes/post/correlation-function/</guid>
      <description>Correlations and the Impact on Sums and Differences I will use a simple R function to illustrate the effect of correlation on sums and differences of random variables. In general, the variance [and standard deviation] of a sum of random variables is the variance of the individual variables plus twice the covariance; the variance [and standard deviation] of a difference in random variables is the variance of the individual variables minus twice the (signed) covariance.</description>
    </item>
    
    <item>
      <title>tidytext is neat! White House Communications</title>
      <link>https://www.data.viajes/post/tidytext-is-neat/</link>
      <pubDate>Wed, 21 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.data.viajes/post/tidytext-is-neat/</guid>
      <description>Presidential Press The language of presidential communications is interesting and I know very little about text as data. I have a number of applications in mind for these tools but I have to learn how to use them. What does the website look like?
White House News
The site is split in four parts: all news, articles, presidential actions, and briefings and statements. The first one is a catch all and the second is news links.</description>
    </item>
    
  </channel>
</rss>