<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>tidyverse on Robert W. Walker</title>
    <link>/categories/tidyverse/</link>
    <description>Recent content in tidyverse on Robert W. Walker</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sat, 18 Jan 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/categories/tidyverse/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>a quick tidyTuesday on Passwords</title>
      <link>/post/a-quick-tidytuesday-on-passwords/</link>
      <pubDate>Sat, 18 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/a-quick-tidytuesday-on-passwords/</guid>
      <description>First, I wanted to acquire the distribution of letters and then play with that. I embedded the result here. The second step is to import the tidyTuesday data.
library(tidyverse) Letter.Freq &amp;lt;- data.frame(stringsAsFactors=FALSE, Letter = c(&amp;quot;E&amp;quot;, &amp;quot;T&amp;quot;, &amp;quot;A&amp;quot;, &amp;quot;O&amp;quot;, &amp;quot;I&amp;quot;, &amp;quot;N&amp;quot;, &amp;quot;S&amp;quot;, &amp;quot;R&amp;quot;, &amp;quot;H&amp;quot;, &amp;quot;D&amp;quot;, &amp;quot;L&amp;quot;, &amp;quot;U&amp;quot;, &amp;quot;C&amp;quot;, &amp;quot;M&amp;quot;, &amp;quot;F&amp;quot;, &amp;quot;Y&amp;quot;, &amp;quot;W&amp;quot;, &amp;quot;G&amp;quot;, &amp;quot;P&amp;quot;, &amp;quot;B&amp;quot;, &amp;quot;V&amp;quot;, &amp;quot;K&amp;quot;, &amp;quot;X&amp;quot;, &amp;quot;Q&amp;quot;, &amp;quot;J&amp;quot;, &amp;quot;Z&amp;quot;), Frequency = c(12.02, 9.1, 8.12, 7.68, 7.31, 6.95, 6.28, 6.</description>
    </item>
    
    <item>
      <title>Dog Movements: a tidyTuesday</title>
      <link>/post/dog-movements-a-tidytuesday/</link>
      <pubDate>Tue, 17 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/dog-movements-a-tidytuesday/</guid>
      <description>Adoptable Dogs # devtools::install_github(&amp;quot;thebioengineer/tidytuesdayR&amp;quot;, force=TRUE) tuesdata51 &amp;lt;- tidytuesdayR::tt_load(2019, week = 51) dog_moves &amp;lt;- tuesdata51$dog_moves dog_des &amp;lt;- readr::read_csv(&amp;#39;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-12-17/dog_descriptions.csv&amp;#39;) library(tidyverse); library(scatterpie) library(rgeos) library(maptools) library(rgdal); library(usmap); library(ggthemes)  The Base Map My.Map &amp;lt;- us_map(regions = &amp;quot;states&amp;quot;) Base.Plot &amp;lt;- ggplot() + geom_polygon(data=My.Map, aes(x=x, y=y, group=group), fill=&amp;quot;white&amp;quot;, color=&amp;quot;black&amp;quot;) + theme_map() Base.Plot A fifty state map to plot this information on.
New.Dat &amp;lt;- left_join(My.Map, dog_moves, by= c(&amp;quot;full&amp;quot; = &amp;quot;location&amp;quot;)) ggplot() + geom_polygon(data=New.</description>
    </item>
    
    <item>
      <title>tidyTuesday Measles</title>
      <link>/post/tidytuesday-measles/</link>
      <pubDate>Mon, 16 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/tidytuesday-measles/</guid>
      <description>tidyTuesday: December 10, 2019 Replicating plots from simplystatistics. One nice twist is the development of a tidytuesdayR package to grab the necessary data in an easy way. You can install the package via github. I will also use fiftystater and ggflags.
devtools::install_github(&amp;quot;thebioengineer/tidytuesdayR&amp;quot;) devtools::install_github(&amp;quot;ellisp/ggflags&amp;quot;) devtools::install_github(&amp;quot;wmurphyrd/fiftystater&amp;quot;) tuesdata &amp;lt;- tidytuesdayR::tt_load(2019, week = 50) ## --- Downloading #TidyTuesday Information for 2019-12-10 ---- ## --- Identified 4 files available for download ---- ## --- Downloading files --- ## Warning in identify_delim(temp_file): Not able to detect delimiter for the file.</description>
    </item>
    
    <item>
      <title>Philadelphia Parking Tickets: a tidyTuesday</title>
      <link>/post/philadelphia-parking-tickets-a-tidytuesday/</link>
      <pubDate>Mon, 09 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/philadelphia-parking-tickets-a-tidytuesday/</guid>
      <description>Philadelphia Map Use ggmap for the base layer.
library(ggmap); library(osmdata); library(tidyverse) PHI &amp;lt;- get_map(getbb(&amp;quot;Philadelphia, PA&amp;quot;), maptype = &amp;quot;stamen&amp;quot;, zoom=12)  Get the Tickets Data TidyTuesday covers 1.26 million parking tickets in Philadelphia.
tickets &amp;lt;- readr::read_csv(&amp;quot;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-12-03/tickets.csv&amp;quot;) ## Parsed with column specification: ## cols( ## violation_desc = col_character(), ## issue_datetime = col_datetime(format = &amp;quot;&amp;quot;), ## fine = col_double(), ## issuing_agency = col_character(), ## lat = col_double(), ## lon = col_double(), ## zip_code = col_double() ## )  Two Lines of Code Left library(lubridate); library(ggthemes) tickets &amp;lt;- tickets %&amp;gt;% mutate(Day = wday(issue_datetime, label=TRUE)) # use lubridate to extract the day of the month.</description>
    </item>
    
    <item>
      <title>US Census Mapping</title>
      <link>/post/uscensus-mapping/</link>
      <pubDate>Mon, 09 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/uscensus-mapping/</guid>
      <description>Searching and Mapping the Census  Searching for the Asian Population via the Census To use tidycensus, there are limitations imposed by the available tables. There is ACS – a survey of about 3 million people – and the two main decennial census files [SF1] and [SF2]. I will search SF1 for the Asian population.
library(tidycensus); library(kableExtra) library(tidyverse); library(stringr) v10 &amp;lt;- load_variables(2010, &amp;quot;sf1&amp;quot;, cache = TRUE) v10 %&amp;gt;% filter(str_detect(concept, &amp;quot;ASIAN&amp;quot;)) %&amp;gt;% filter(str_detect(label, &amp;quot;Female&amp;quot;)) %&amp;gt;% kable() %&amp;gt;% scroll_box(width = &amp;quot;100%&amp;quot;)    name  label  concept      P012D026  Total!</description>
    </item>
    
    <item>
      <title>A Quick and Dirty Introduction to R</title>
      <link>/r/a-quick-and-dirty-introduction-to-r/</link>
      <pubDate>Fri, 25 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/r/a-quick-and-dirty-introduction-to-r/</guid>
      <description>Some Data I will start with some inline data.
library(tidyverse); library(skimr); Support.Times &amp;lt;- structure(list(Screened = c(26.9, 28.4, 23.9, 21.8, 22.4, 25.9, 26.5, 20, 23.7, 23.7, 22.6, 19.4, 27.3, 25.3, 27.7, 25.3, 28.4, 24.2, 20.4, 29.6, 27, 23.6, 18.3, 28.1, 20.5, 24.1, 27.2, 26.4, 24.5, 25.6, 17.9, 23.5, 25.3, 20.2, 26.3, 27.9), Not.Screened = c(24.7, 19.1, 21, 17.8, 22.8, 24.4, 17.9, 20.5, 20, 26.2, 14.5, 22.4, 21.1, 24.3, 22, 24.3, 23.</description>
    </item>
    
    <item>
      <title>Fariss Human Rights Data with Animation</title>
      <link>/post/fariss-human-rights-data-with-animation/</link>
      <pubDate>Thu, 10 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/fariss-human-rights-data-with-animation/</guid>
      <description>Fariss Data Is neat and complete.
load(here(&amp;quot;data&amp;quot;,&amp;quot;FarissHRData.RData&amp;quot;)) skimr::skim(HR.Data)  Table 1: Data summary  Name HR.Data  Number of rows 11717  Number of columns 27  _______________________   Column type frequency:   factor 1  numeric 26  ________________________   Group variables None    Variable type: factor
  skim_variable n_missing complete_rate ordered n_unique top_counts    COW_YEAR 0 1 FALSE 11717 100: 1, 100: 1, 100: 1, 100: 1    Variable type: numeric</description>
    </item>
    
    <item>
      <title>Simple Oregon County Mapping</title>
      <link>/post/simple-oregon-county-mapping/</link>
      <pubDate>Wed, 09 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/simple-oregon-county-mapping/</guid>
      <description>Some Data for the Map I want to get some data to place on the map. I found a website with population and population change data for Oregon in .csv format. I cannot direct download it from R, instead I have to button download it and import it.
library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✓ ggplot2 3.2.1 ✓ purrr 0.3.3 ## ✓ tibble 2.1.3 ✓ dplyr 0.</description>
    </item>
    
    <item>
      <title>Tables, Pivots, Bars, and Mosaics</title>
      <link>/r/tables-pivots-bars-and-mosaics/</link>
      <pubDate>Wed, 09 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/r/tables-pivots-bars-and-mosaics/</guid>
      <description>R Markdown There is detailed help for all that Markdown can do under Help in the RStudio. The key to it is knitting documents with the Knit button in the RStudio. If we use helpers like the R Commander, Radiant, or esquisse, we will need the R code implanted in the Markdown document in particular ways. I will use Markdown for everything. I even use a close relation of Markdown in my scholarly pursuits.</description>
    </item>
    
    <item>
      <title>tidyTuesday does Pizza</title>
      <link>/post/tidytuesday-does-pizza/</link>
      <pubDate>Wed, 09 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/tidytuesday-does-pizza/</guid>
      <description>Pizza Ratings The #tidyTuesday for this week involves pizza shop ratings data. Let’s see what we have.
pizza_jared &amp;lt;- readr::read_csv(&amp;quot;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-10-01/pizza_jared.csv&amp;quot;) ## Parsed with column specification: ## cols( ## polla_qid = col_double(), ## answer = col_character(), ## votes = col_double(), ## pollq_id = col_double(), ## question = col_character(), ## place = col_character(), ## time = col_double(), ## total_votes = col_double(), ## percent = col_double() ## ) pizza_barstool &amp;lt;- readr::read_csv(&amp;quot;https://raw.</description>
    </item>
    
    <item>
      <title>Some Basic Text on the Mueller Report</title>
      <link>/post/some-basic-text-on-the-mueller-report/</link>
      <pubDate>Wed, 22 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/some-basic-text-on-the-mueller-report/</guid>
      <description>So this Robert Mueller guy wrote a report I may as well analyse it a bit.
First, let me see if I can get a hold of the data. I grabbed the report directly from the Department of Justice website. You can follow this link.
library(tidyverse) library(pdftools) # Download report from link above mueller_report_txt &amp;lt;- pdf_text(&amp;quot;../data/report.pdf&amp;quot;) # Create a tibble of the text with line numbers and pages mueller_report &amp;lt;- tibble( page = 1:length(mueller_report_txt), text = mueller_report_txt) %&amp;gt;% separate_rows(text, sep = &amp;quot;\n&amp;quot;) %&amp;gt;% group_by(page) %&amp;gt;% mutate(line = row_number()) %&amp;gt;% ungroup() %&amp;gt;% select(page, line, text) write_csv(mueller_report, &amp;quot;data/mueller_report.</description>
    </item>
    
    <item>
      <title>nflscrapR is amazing</title>
      <link>/post/nflscrapr-is-amazing/</link>
      <pubDate>Tue, 30 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/nflscrapr-is-amazing/</guid>
      <description>Scraping NFL data Note: An original version of this post had issues induced by overtime games. There is a better way to handle all of this that I learned from a brief analysis of a tie game between Cleveland and Pittsburgh in Week One.
The nflscrapR package is designed to make data on NFL games more easily available. To install the package, we need to grab it from github.</description>
    </item>
    
    <item>
      <title>NFL ScrapR</title>
      <link>/r/nflscrapr-test/</link>
      <pubDate>Fri, 19 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/r/nflscrapr-test/</guid>
      <description>Scraping NFL data with nflscrapr The nflscrapR package is designed to make data on NFL games more easily available. To install the package, we need to grab it from github.
devtools::install_github(repo = &amp;quot;maksimhorowitz/nflscrapR&amp;quot;) The github page for nflscrapR is quite informative. It has a lot of useful insight for working with the data; the set itself is quite large.
Getting Some Data Following the guide to the package on GitHub, let me try their example.</description>
    </item>
    
    <item>
      <title>Visualisation with Archigos: Leaders of the World</title>
      <link>/post/visualisation-with-archigos-leaders-of-the-world/</link>
      <pubDate>Sun, 14 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/visualisation-with-archigos-leaders-of-the-world/</guid>
      <description>Archigos Is an amazing collaboration that produced a comprehensive dataset of world leaders going pretty far back; see Archigos on the web. For thinking about leadership, it is quite natural. In this post, I want to do some reshaping into country year and leader year datasets and explore the basic confines of Archigos. I also want to use gganimate for a few things. So what do we know?
library(lubridate) library(tidyverse) library(ggthemes) library(stringr) library(gganimate) library(emoGG) library(emojifont) library(haven) Archigos &amp;lt;- read_dta(url(&amp;quot;http://www.</description>
    </item>
    
    <item>
      <title>fredr is very neat</title>
      <link>/post/fredr-is-very-neat/</link>
      <pubDate>Thu, 11 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/fredr-is-very-neat/</guid>
      <description>FRED via fredr The Federal Reserve Economic Database [FRED] is a wonderful public resource for data and the r api that connects to it is very easy to use for the things that I have previously needed. For example, one of my students was interested in commercial credit default data. I used the FRED search instructions from the following vignette to find that data. My first step was the vignette for using fredr.</description>
    </item>
    
    <item>
      <title>Stocks and gganimate</title>
      <link>/post/stocks-and-gganimate/</link>
      <pubDate>Sun, 17 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/stocks-and-gganimate/</guid>
      <description>tidyquant Automates a lot of equity research and calculation using tidy concepts. Here, I will first use it to get the components of the S and P 500 and pick out those with weights over 1.25 percent. In the next step, I download the data and finally calculate daily returns and a cumulative wealth index.
library(tidyquant) library(tidyverse) tq_index(&amp;quot;SP500&amp;quot;) %&amp;gt;% filter(weight &amp;gt; 0.0125) %&amp;gt;% select(symbol,company) -&amp;gt; Tickers Tickers &amp;lt;- Tickers %&amp;gt;% filter(symbol!</description>
    </item>
    
    <item>
      <title>Trump Tweet Word Clouds</title>
      <link>/post/trump-tweet-word-clouds/</link>
      <pubDate>Tue, 18 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/trump-tweet-word-clouds/</guid>
      <description>Mining Twitter Data Is rather easy. You have to arrange a developer account with Twitter and set up an app. After that, Twitter gives you access to a consumer key and secret and an access token and access secret. My tool of choice for this is rtweet because it automagically processes tweet elements and makes them easy to slice and dice. I also played with twitteR but it was harder to work with for what I wanted.</description>
    </item>
    
    <item>
      <title>tidyTuesday: coffee chains</title>
      <link>/post/tidytuesday-coffee-chains/</link>
      <pubDate>Wed, 09 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/tidytuesday-coffee-chains/</guid>
      <description>The tidyTuesday for this week is coffee chain locations For this week: 1. The basic link to the #tidyTuesday shows an original article for Week 6.
First, let’s import the data; it is a single Excel spreadsheet. The page notes that starbucks, Tim Horton, and Dunkin Donuts have raw data available.
library(readxl) library(tidyverse) ## ── Attaching packages ────────────────────────────── tidyverse 1.3.0 ── ## ✓ ggplot2 3.2.1 ✓ purrr 0.3.3 ## ✓ tibble 2.</description>
    </item>
    
    <item>
      <title>Global mortality tidyTuesday</title>
      <link>/post/tidytuesday-takes-on-global-mortality/</link>
      <pubDate>Wed, 18 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/tidytuesday-takes-on-global-mortality/</guid>
      <description>tidyTuesday on Global Mortality The three generic challenge graphics involve two global summaries, a raw count by type and a percentage by type. The individual county breakdowns are recorded for a predetermined year below. This can all be seen in the original. For whatever reason, I cannot open this data remotely.
Here is this week’s tidyTuesday.
library(skimr) library(tidyverse) library(rlang) # global_mortality &amp;lt;- readRDS(&amp;quot;../../data/global_mortality.rds&amp;quot;) global_mortality &amp;lt;- readRDS(url(&amp;quot;https://github.com/robertwwalker/academic-mymod/raw/master/data/global_mortality.rds&amp;quot;)) skim(global_mortality)  Table 1: Data summary  Name global_mortality  Number of rows 6156  Number of columns 35  _______________________   Column type frequency:   character 2  numeric 33  ________________________   Group variables None    Variable type: character</description>
    </item>
    
    <item>
      <title>Scraping the NFL Salary Cap Data with Python and R</title>
      <link>/post/scraping-the-nfl-salary-cap-data-with-python-and-r/</link>
      <pubDate>Wed, 04 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/scraping-the-nfl-salary-cap-data-with-python-and-r/</guid>
      <description>The NFL Data [SporTrac](http://www.sportrac.com] has a wonderful array of financial data on sports. A student going to work for the Seattle Seahawks wanted the NFL salary cap data and I also found data on the English Premier League there. Now I have a source to scrape the data from.
With a source in hand, the key tool is the SelectorGadget. SelectorGadget is a browser add-in for Chrome that allows us to select text and identify the css or xpath selector to scrape the data.</description>
    </item>
    
    <item>
      <title>tidyTuesday - Tuition</title>
      <link>/post/tidytuesday-tuition/</link>
      <pubDate>Tue, 03 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/tidytuesday-tuition/</guid>
      <description>I found a great example on tidyTuesday that I wanted to work on. @JakeKaupp tweeted his #tidyTuesday: a very cool slope plot of tuition changes averaged by state over the last decade. It is a very informative graphic. The only tweak is a simple embedded line plot that uses color in a creative way to show growth rates. All of the R code for this is on Jake Kaupp’s GitHub.</description>
    </item>
    
  </channel>
</rss>