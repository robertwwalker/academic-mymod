<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R Code on Robert W. Walker</title>
    <link>/r/</link>
    <description>Recent content in R Code on Robert W. Walker</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Tue, 01 Oct 2019 00:00:00 -0700</lastBuildDate>
    
	<atom:link href="/r/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Financial Analysis of SEC Reports in R</title>
      <link>/r/finreportr-is-awesome/</link>
      <pubDate>Tue, 29 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/r/finreportr-is-awesome/</guid>
      <description>The Package: finreport The key tool to facilitate the financial analysis of companies that file regular SEC reports of certain forms is finreportr. To make use of it, we must first have R install it and dependencies. To install it, install.packages(&amp;quot;finreportr&amp;quot;, dependencies=TRUE).
 The Commands The first command is CompanyInfo().
library(finreportr) CompanyInfo(&amp;quot;JPM&amp;quot;) ## company CIK SIC state state.inc FY.end ## 1 JPMORGAN CHASE &amp;amp; CO 0000019617 6021 NY DE 1231 ## street.</description>
    </item>
    
    <item>
      <title>A Quick and Dirty Introduction to R</title>
      <link>/r/a-quick-and-dirty-introduction-to-r/</link>
      <pubDate>Fri, 25 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/r/a-quick-and-dirty-introduction-to-r/</guid>
      <description>Some Data I will start with some inline data.
library(tidyverse); library(skimr); Support.Times &amp;lt;- structure(list(Screened = c(26.9, 28.4, 23.9, 21.8, 22.4, 25.9, 26.5, 20, 23.7, 23.7, 22.6, 19.4, 27.3, 25.3, 27.7, 25.3, 28.4, 24.2, 20.4, 29.6, 27, 23.6, 18.3, 28.1, 20.5, 24.1, 27.2, 26.4, 24.5, 25.6, 17.9, 23.5, 25.3, 20.2, 26.3, 27.9), Not.Screened = c(24.7, 19.1, 21, 17.8, 22.8, 24.4, 17.9, 20.5, 20, 26.2, 14.5, 22.4, 21.1, 24.3, 22, 24.3, 23.</description>
    </item>
    
    <item>
      <title>Working an Example on Proportions</title>
      <link>/r/workedproportion/</link>
      <pubDate>Sun, 20 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/r/workedproportion/</guid>
      <description>A Proportions Example We started with an equation:
\[ z = \frac{\hat{\pi} - \pi}{\sqrt{\frac{\pi(1-\pi)}{n}}} \]
In language, the difference between the sample proportion (recall that with only two outcomes the sample proportion \(\hat{\pi}\) is between 0 [all No’s] and 1 [all Yes’s]) and the true probability \(\pi\) divided by the standard error of the proportion \(\sqrt{\frac{\pi(1-\pi)}{n}}\) has a \(z\) [Normal(0,1)] distribution under the condition that \(n\pi &amp;gt; 10\) and \(n(1-\pi) &amp;gt; 10\).</description>
    </item>
    
    <item>
      <title>The Duality of Hypothesis Tests and Confidence Intervals</title>
      <link>/r/hypothesis-test-ci/</link>
      <pubDate>Wed, 16 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/r/hypothesis-test-ci/</guid>
      <description>cars data I will work with R’s internal dataset on cars: cars. There are two variables in the dataset, this is what they look like.
plot(cars) An Hypothesis Test I will work with the speed variable. The hypothesis to advance is that 17 or greater is the true average speed. The alternative must then be that the average speed is less than 17. Knowing only the sample size, I can figure out what \(t\) must be to reject 17 or greater and conclude that the true average must be less with 90% probability.</description>
    </item>
    
    <item>
      <title>Alluvial Plots</title>
      <link>/r/alluvials/</link>
      <pubDate>Wed, 09 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/r/alluvials/</guid>
      <description>Alluvial and Sankey Diagrams The aforementioned plots are methods for visualising the flow of data through a stream of markers. I was motivated to show this because enough of you deal in orders, tickets, and the like the flow visualisation of a system might prove of use. I will work with a familiar dataset. These are data on Admissions at the University of California Berkeley. The data exist as an internal R file in tabular form.</description>
    </item>
    
    <item>
      <title>Tables, Pivots, Bars, and Mosaics</title>
      <link>/r/tables-pivots-bars-and-mosaics/</link>
      <pubDate>Wed, 09 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/r/tables-pivots-bars-and-mosaics/</guid>
      <description>R Markdown There is detailed help for all that Markdown can do under Help in the RStudio. The key to it is knitting documents with the Knit button in the RStudio. If we use helpers like the R Commander, Radiant, or esquisse, we will need the R code implanted in the Markdown document in particular ways. I will use Markdown for everything. I even use a close relation of Markdown in my scholarly pursuits.</description>
    </item>
    
    <item>
      <title>Practice Midterm - 2019</title>
      <link>/r/practicemidterm/</link>
      <pubDate>Tue, 01 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/r/practicemidterm/</guid>
      <description>A Practice Midterm Answer the following questions in as much detail as possible in the time permitted. In the exam format, partial credit will be awarded in the event that the reasoning is sufficiently clear to uncover the parts of a question correctly answered. Answer the questions to the best of your ability and be systematic in solving the problems. This will always take you far. A Workbook called NaturalGas2019.</description>
    </item>
    
    <item>
      <title>NFL ScrapR</title>
      <link>/r/nflscrapr-test/</link>
      <pubDate>Fri, 19 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/r/nflscrapr-test/</guid>
      <description>Scraping NFL data with nflscrapr The nflscrapR package is designed to make data on NFL games more easily available. To install the package, we need to grab it from github.
devtools::install_github(repo = &amp;quot;maksimhorowitz/nflscrapR&amp;quot;) The github page for nflscrapR is quite informative. It has a lot of useful insight for working with the data; the set itself is quite large.
Getting Some Data Following the guide to the package on GitHub, let me try their example.</description>
    </item>
    
  </channel>
</rss>