<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Robert W. Walker on Robert W. Walker</title>
    <link>https://www.data.viajes/</link>
    <description>Recent content in Robert W. Walker on Robert W. Walker</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 -0700</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Trump Tweet Word Clouds</title>
      <link>https://www.data.viajes/post/trump-tweet-word-clouds/</link>
      <pubDate>Tue, 18 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.data.viajes/post/trump-tweet-word-clouds/</guid>
      <description>&lt;script src=&#34;https://www.data.viajes/rmarkdown-libs/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://www.data.viajes/rmarkdown-libs/wordcloud2/wordcloud.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://www.data.viajes/rmarkdown-libs/wordcloud2/wordcloud2-all.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://www.data.viajes/rmarkdown-libs/wordcloud2/hover.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://www.data.viajes/rmarkdown-libs/wordcloud2-binding/wordcloud2.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;mining-twitter-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Mining Twitter Data&lt;/h1&gt;
&lt;p&gt;Is rather easy. You have to arrange a developer account with Twitter and set up an app. After that, Twitter gives you access to a consumer key and secret and an access token and access secret. Those can be embedded in OAuth to automate searches and the retrieval of data. My tool of choice for this is &lt;em&gt;rtweet&lt;/em&gt; The first section involves setting up a token.&lt;/p&gt;
&lt;p&gt;For this one, we need &lt;code&gt;rtweet&lt;/code&gt; because the tweet text is getting cut.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Change the next four lines based on your own consumer_key, consume_secret, access_token, and access_secret. 
token &amp;lt;- create_token(
  app = &amp;quot;MyAppName&amp;quot;,
  consumer_key &amp;lt;- &amp;quot;CK&amp;quot;,
  consumer_secret &amp;lt;- &amp;quot;CS&amp;quot;,
  access_token &amp;lt;- &amp;quot;AT&amp;quot;,
  access_secret &amp;lt;- &amp;quot;AS&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now I want to collect some tweets from a particular user’s timeline and look into them.&lt;/p&gt;
&lt;div id=&#34;who-does-trump-tweet-about&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Who does Trump tweet about?&lt;/h2&gt;
&lt;p&gt;A cool post on sentiment analysis can be found &lt;a href=&#34;http://dataaspirant.com/2018/03/22/twitter-sentiment-analysis-using-r/&#34;&gt;here&lt;/a&gt;. The first step is to grab his timeline. &lt;code&gt;rtweet&lt;/code&gt; makes this quite easy. I will grab it and then save it in the code below so that I do not spam the API.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tml.djt &amp;lt;- get_timeline(&amp;quot;realDonaldTrump&amp;quot;, n = 3200)
save(tml.djt, file=&amp;quot;../data/TMLS.RData&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I start by loading the tmls object that I created above. What does it look like?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(tidytext)
load(&amp;quot;../../data/TMLS.RData&amp;quot;)
names(tml.djt)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;user_id&amp;quot;                 &amp;quot;status_id&amp;quot;              
##  [3] &amp;quot;created_at&amp;quot;              &amp;quot;screen_name&amp;quot;            
##  [5] &amp;quot;text&amp;quot;                    &amp;quot;source&amp;quot;                 
##  [7] &amp;quot;display_text_width&amp;quot;      &amp;quot;reply_to_status_id&amp;quot;     
##  [9] &amp;quot;reply_to_user_id&amp;quot;        &amp;quot;reply_to_screen_name&amp;quot;   
## [11] &amp;quot;is_quote&amp;quot;                &amp;quot;is_retweet&amp;quot;             
## [13] &amp;quot;favorite_count&amp;quot;          &amp;quot;retweet_count&amp;quot;          
## [15] &amp;quot;hashtags&amp;quot;                &amp;quot;symbols&amp;quot;                
## [17] &amp;quot;urls_url&amp;quot;                &amp;quot;urls_t.co&amp;quot;              
## [19] &amp;quot;urls_expanded_url&amp;quot;       &amp;quot;media_url&amp;quot;              
## [21] &amp;quot;media_t.co&amp;quot;              &amp;quot;media_expanded_url&amp;quot;     
## [23] &amp;quot;media_type&amp;quot;              &amp;quot;ext_media_url&amp;quot;          
## [25] &amp;quot;ext_media_t.co&amp;quot;          &amp;quot;ext_media_expanded_url&amp;quot; 
## [27] &amp;quot;ext_media_type&amp;quot;          &amp;quot;mentions_user_id&amp;quot;       
## [29] &amp;quot;mentions_screen_name&amp;quot;    &amp;quot;lang&amp;quot;                   
## [31] &amp;quot;quoted_status_id&amp;quot;        &amp;quot;quoted_text&amp;quot;            
## [33] &amp;quot;quoted_created_at&amp;quot;       &amp;quot;quoted_source&amp;quot;          
## [35] &amp;quot;quoted_favorite_count&amp;quot;   &amp;quot;quoted_retweet_count&amp;quot;   
## [37] &amp;quot;quoted_user_id&amp;quot;          &amp;quot;quoted_screen_name&amp;quot;     
## [39] &amp;quot;quoted_name&amp;quot;             &amp;quot;quoted_followers_count&amp;quot; 
## [41] &amp;quot;quoted_friends_count&amp;quot;    &amp;quot;quoted_statuses_count&amp;quot;  
## [43] &amp;quot;quoted_location&amp;quot;         &amp;quot;quoted_description&amp;quot;     
## [45] &amp;quot;quoted_verified&amp;quot;         &amp;quot;retweet_status_id&amp;quot;      
## [47] &amp;quot;retweet_text&amp;quot;            &amp;quot;retweet_created_at&amp;quot;     
## [49] &amp;quot;retweet_source&amp;quot;          &amp;quot;retweet_favorite_count&amp;quot; 
## [51] &amp;quot;retweet_retweet_count&amp;quot;   &amp;quot;retweet_user_id&amp;quot;        
## [53] &amp;quot;retweet_screen_name&amp;quot;     &amp;quot;retweet_name&amp;quot;           
## [55] &amp;quot;retweet_followers_count&amp;quot; &amp;quot;retweet_friends_count&amp;quot;  
## [57] &amp;quot;retweet_statuses_count&amp;quot;  &amp;quot;retweet_location&amp;quot;       
## [59] &amp;quot;retweet_description&amp;quot;     &amp;quot;retweet_verified&amp;quot;       
## [61] &amp;quot;place_url&amp;quot;               &amp;quot;place_name&amp;quot;             
## [63] &amp;quot;place_full_name&amp;quot;         &amp;quot;place_type&amp;quot;             
## [65] &amp;quot;country&amp;quot;                 &amp;quot;country_code&amp;quot;           
## [67] &amp;quot;geo_coords&amp;quot;              &amp;quot;coords_coords&amp;quot;          
## [69] &amp;quot;bbox_coords&amp;quot;             &amp;quot;status_url&amp;quot;             
## [71] &amp;quot;name&amp;quot;                    &amp;quot;location&amp;quot;               
## [73] &amp;quot;description&amp;quot;             &amp;quot;url&amp;quot;                    
## [75] &amp;quot;protected&amp;quot;               &amp;quot;followers_count&amp;quot;        
## [77] &amp;quot;friends_count&amp;quot;           &amp;quot;listed_count&amp;quot;           
## [79] &amp;quot;statuses_count&amp;quot;          &amp;quot;favourites_count&amp;quot;       
## [81] &amp;quot;account_created_at&amp;quot;      &amp;quot;verified&amp;quot;               
## [83] &amp;quot;profile_url&amp;quot;             &amp;quot;profile_expanded_url&amp;quot;   
## [85] &amp;quot;account_lang&amp;quot;            &amp;quot;profile_banner_url&amp;quot;     
## [87] &amp;quot;profile_background_url&amp;quot;  &amp;quot;profile_image_url&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I want to first get rid of retweets to render President Trump in his own voice.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;DJTDF &amp;lt;- tml.djt %&amp;gt;% filter(is_retweet==FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With just his tweets, a few things can be easily accomplished. Who does he mention?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(wordcloud)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: RColorBrewer&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;MNTDJT &amp;lt;- DJTDF %&amp;gt;% filter(!is.na(mentions_screen_name)) %&amp;gt;% select(mentions_screen_name)
Ments &amp;lt;- as.character(unlist(MNTDJT))
wordcloud(Ments)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required namespace: tm&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.data.viajes/post/2018-12-18-trump-tweet-word-clouds_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;That’s interesting. But that is twitter accounts. That is far less interesting that his actual text.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-does-trump-tweet-about&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What does Trump tweet about?&lt;/h2&gt;
&lt;p&gt;Some more stuff from &lt;a href=&#34;https://stackoverflow.com/questions/31348453/how-do-i-clean-twitter-data-in-r&#34;&gt;stack overflow&lt;/a&gt;. There is quite a bit of code in here. I simply wrote a function that takes an input character string and cleans it up. Uncomment the various components and pipe them.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(RColorBrewer)
TDF &amp;lt;- DJTDF %&amp;gt;% select(text)
# TDF contains the text of tweets.
library(stringr)
tweet_cleaner &amp;lt;- function(text) {
  temp1 &amp;lt;- str_replace_all(text, &amp;quot;&amp;amp;amp&amp;quot;, &amp;quot;&amp;quot;) %&amp;gt;% 
    str_replace_all(., &amp;quot;https://t+&amp;quot;, &amp;quot;&amp;quot;) %&amp;gt;%
    str_replace_all(.,&amp;quot;@[a-z,A-Z]*&amp;quot;,&amp;quot;&amp;quot;)
#    str_replace_all(., &amp;quot;[[:punct:]]&amp;quot;, &amp;quot;&amp;quot;)  
#    str_replace_all(., &amp;quot;[[:digit:]]&amp;quot;, &amp;quot;&amp;quot;) %&amp;gt;%
#    str_replace_all(., &amp;quot;[ \t]{2,}&amp;quot;, &amp;quot;&amp;quot;) %&amp;gt;%
#    str_replace_all(., &amp;quot;^\\s+|\\s+$&amp;quot;, &amp;quot;&amp;quot;)  %&amp;gt;%
#    str_replace_all(., &amp;quot; &amp;quot;,&amp;quot; &amp;quot;) %&amp;gt;%
#    str_replace_all(., &amp;quot;http://t.co/[a-z,A-Z,0-9]*{8}&amp;quot;,&amp;quot;&amp;quot;)
#    str_replace_all(.,&amp;quot;RT @[a-z,A-Z]*: &amp;quot;,&amp;quot;&amp;quot;) %&amp;gt;% 
#    str_replace_all(.,&amp;quot;#[a-z,A-Z]*&amp;quot;,&amp;quot;&amp;quot;)
  return(temp1)
}
clean_tweets &amp;lt;- data.frame(text=sapply(1:dim(TDF)[[1]], function(x) {tweet_cleaner(TDF[x,&amp;quot;text&amp;quot;])}))
clean_tweets$text &amp;lt;- as.character(clean_tweets$text)
Trumps.Words &amp;lt;- clean_tweets %&amp;gt;% unnest_tokens(., word, text) %&amp;gt;% anti_join(stop_words, &amp;quot;word&amp;quot;)
TTW &amp;lt;- table(Trumps.Words)
TTW &amp;lt;- TTW[order(TTW, decreasing = T)]
TTW &amp;lt;- data.frame(TTW)
names(TTW) &amp;lt;- c(&amp;quot;word&amp;quot;,&amp;quot;freq&amp;quot;)
pal &amp;lt;- brewer.pal(9,&amp;quot;RdBu&amp;quot;)
wordcloud(TTW$word, TTW$freq, colors=pal, random.order=F, max.words=200)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.data.viajes/post/2018-12-18-trump-tweet-word-clouds_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Well, that is kinda cool. Now, I want to do a bit more with it using more complicated word combinations.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-wonders-of-tidytext&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Wonders of tidytext&lt;/h2&gt;
&lt;p&gt;The &lt;em&gt;tidytext&lt;/em&gt; &lt;a href=&#34;https://www.tidytextmining.com/ngrams.html&#34;&gt;section on n-grams&lt;/a&gt; is great. I will start with a tweet identifier – something I should have deployed long ago – before parsing these.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyr)
CT &amp;lt;- clean_tweets %&amp;gt;% mutate(tweetno= row_number())
DJT2G &amp;lt;- clean_tweets %&amp;gt;% unnest_tokens(bigram, text, token = &amp;quot;ngrams&amp;quot;, n=2)

bigrams_separated &amp;lt;- DJT2G %&amp;gt;%
  separate(bigram, c(&amp;quot;word1&amp;quot;, &amp;quot;word2&amp;quot;), sep = &amp;quot; &amp;quot;)

bigrams_filtered &amp;lt;- bigrams_separated %&amp;gt;%
  filter(!word1 %in% stop_words$word) %&amp;gt;%
  filter(!word2 %in% stop_words$word)

# new bigram counts:
bigram_counts &amp;lt;- bigrams_filtered %&amp;gt;% 
  count(word1, word2, sort = TRUE)

bigram_counts&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10,514 x 3
##    word1   word2           n
##    &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;       &amp;lt;int&amp;gt;
##  1 fake    news          160
##  2 witch   hunt          128
##  3 north   korea          84
##  4 white   house          71
##  5 news    media          56
##  6 total   endorsement    49
##  7 law     enforcement    47
##  8 crooked hillary        43
##  9 supreme court          39
## 10 border  security       38
## # ... with 10,504 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bigrams_united &amp;lt;- bigrams_filtered %&amp;gt;%
  unite(bigram, word1, word2, sep = &amp;quot; &amp;quot;)

my.df &amp;lt;- data.frame(table(bigrams_united))
my.df &amp;lt;- my.df[order(my.df$Freq, decreasing=TRUE),]
my.df &amp;lt;- my.df[c(1:500),]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With that, we have the data for the bigram cloud.&lt;/p&gt;
&lt;p&gt;After seeing a few competing renditions, I prefer &lt;code&gt;wordcloud2&lt;/code&gt;. One thing to be careful about is scaling. In this case, the most frequent bigram is missing because the ratio makes it too large to fit. With size smaller, it can be made to show.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(wordcloud2)
wordcloud2(my.df, color=&amp;quot;random-light&amp;quot;, backgroundColor = &amp;quot;black&amp;quot;, size = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-1&#34; style=&#34;width:672px;height:480px;&#34; class=&#34;wordcloud2 html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;word&#34;:[&#34;fake news&#34;,&#34;witch hunt&#34;,&#34;north korea&#34;,&#34;white house&#34;,&#34;news media&#34;,&#34;total endorsement&#34;,&#34;law enforcement&#34;,&#34;crooked hillary&#34;,&#34;supreme court&#34;,&#34;border security&#34;,&#34;president trump&#34;,&#34;nancy pelosi&#34;,&#34;donald trump&#34;,&#34;hillary clinton&#34;,&#34;tax cuts&#34;,&#34;james comey&#34;,&#34;prime minister&#34;,&#34;trump campaign&#34;,&#34;angry democrats&#34;,&#34;trade deals&#34;,&#34;rigged witch&#34;,&#34;2nd amendment&#34;,&#34;military vets&#34;,&#34;southern border&#34;,&#34;immigration laws&#34;,&#34;york times&#34;,&#34;american people&#34;,&#34;kim jong&#34;,&#34;illegal immigration&#34;,&#34;jobs jobs&#34;,&#34;president obama&#34;,&#34;european union&#34;,&#34;god bless&#34;,&#34;13 angry&#34;,&#34;obama administration&#34;,&#34;south carolina&#34;,&#34;jeff sessions&#34;,&#34;president xi&#34;,&#34;west virginia&#34;,&#34;brett kavanaugh&#34;,&#34;ms 13&#34;,&#34;peter strzok&#34;,&#34;republican party&#34;,&#34;highly respected&#34;,&#34;lisa page&#34;,&#34;russian collusion&#34;,&#34;stock market&#34;,&#34;washington post&#34;,&#34;chuck schumer&#34;,&#34;justice department&#34;,&#34;trade barriers&#34;,&#34;00 p.m&#34;,&#34;bob mueller&#34;,&#34;mainstream media&#34;,&#34;president donald&#34;,&#34;strong endorsement&#34;,&#34;billion dollars&#34;,&#34;crime borders&#34;,&#34;national security&#34;,&#34;russian witch&#34;,&#34;border patrol&#34;,&#34;borders loves&#34;,&#34;bruce ohr&#34;,&#34;chairman kim&#34;,&#34;fantastic job&#34;,&#34;rick scott&#34;,&#34;trade deficit&#34;,&#34;trump administration&#34;,&#34;2016 election&#34;,&#34;classified information&#34;,&#34;fair trade&#34;,&#34;judge brett&#34;,&#34;mike pompeo&#34;,&#34;special counsel&#34;,&#34;17 angry&#34;,&#34;american workers&#34;,&#34;clinton campaign&#34;,&#34;criminal aliens&#34;,&#34;election day&#34;,&#34;fake dossier&#34;,&#34;illegal immigrants&#34;,&#34;president putin&#34;,&#34;press conference&#34;,&#34;puerto rico&#34;,&#34;ron desantis&#34;,&#34;russian meddling&#34;,&#34;south korea&#34;,&#34;vote republican&#34;,&#34;american history&#34;,&#34;anti trump&#34;,&#34;brian kemp&#34;,&#34;democrat party&#34;,&#34;fbi doj&#34;,&#34;hurricane florence&#34;,&#34;intelligence committee&#34;,&#34;maxine waters&#34;,&#34;national anthem&#34;,&#34;robert mueller&#34;,&#34;saudi arabia&#34;,&#34;street journal&#34;,&#34;tax cut&#34;,&#34;u.s senate&#34;,&#34;unfair trade&#34;,&#34;united nations&#34;,&#34;wall street&#34;,&#34;00 a.m&#34;,&#34;800 billion&#34;,&#34;anonymous sources&#34;,&#34;billy graham&#34;,&#34;border wall&#34;,&#34;country illegally&#34;,&#34;don’t care&#34;,&#34;federal government&#34;,&#34;frame donald&#34;,&#34;henry mcmaster&#34;,&#34;ig report&#34;,&#34;john brennan&#34;,&#34;john james&#34;,&#34;judicial watch&#34;,&#34;middle east&#34;,&#34;news cnn&#34;,&#34;news conference&#34;,&#34;north carolina&#34;,&#34;russia hoax&#34;,&#34;strong borders&#34;,&#34;trump tower&#34;,&#34;african american&#34;,&#34;agent peter&#34;,&#34;andrew brunson&#34;,&#34;border military&#34;,&#34;comey mccabe&#34;,&#34;complete endorsement&#34;,&#34;consumer confidence&#34;,&#34;cryin chuck&#34;,&#34;endorsement congressman&#34;,&#34;enforcement officers&#34;,&#34;fbi agent&#34;,&#34;gregg jarrett&#34;,&#34;happy birthday&#34;,&#34;house intelligence&#34;,&#34;iran deal&#34;,&#34;john kelly&#34;,&#34;local officials&#34;,&#34;lou barletta&#34;,&#34;merit based&#34;,&#34;michael cohen&#34;,&#34;minister abe&#34;,&#34;oil prices&#34;,&#34;pastor andrew&#34;,&#34;paul manafort&#34;,&#34;red wave&#34;,&#34;sanctuary cities&#34;,&#34;trade deal&#34;,&#34;troy balderson&#34;,&#34;unemployment rate&#34;,&#34;washington d.c&#34;,&#34;world cup&#34;,&#34;9 00&#34;,&#34;alan dershowitz&#34;,&#34;amendment loves&#34;,&#34;angry dems&#34;,&#34;approval ratings&#34;,&#34;bill nelson&#34;,&#34;border crime&#34;,&#34;carter page&#34;,&#34;christopher steele&#34;,&#34;clinton foundation&#34;,&#34;cutting taxes&#34;,&#34;dan bongino&#34;,&#34;davos switzerland&#34;,&#34;doesn’t matter&#34;,&#34;double standard&#34;,&#34;gina haspel&#34;,&#34;governor henry&#34;,&#34;harley davidson&#34;,&#34;highly conflicted&#34;,&#34;jerry brown&#34;,&#34;john cox&#34;,&#34;lady melania&#34;,&#34;lowest level&#34;,&#34;massive tariffs&#34;,&#34;matt rosendale&#34;,&#34;national guard&#34;,&#34;people including&#34;,&#34;phony witch&#34;,&#34;republican primary&#34;,&#34;ronald reagan&#34;,&#34;secret service&#34;,&#34;total witch&#34;,&#34;trade deficits&#34;,&#34;vice president&#34;,&#34;vote vote&#34;,&#34;151 billion&#34;,&#34;7 00&#34;,&#34;7 00pme&#34;,&#34;9th circuit&#34;,&#34;america safe&#34;,&#34;american worker&#34;,&#34;andrew mccabe&#34;,&#34;approval rating&#34;,&#34;beautiful evening&#34;,&#34;billion dollar&#34;,&#34;broward county&#34;,&#34;bush family&#34;,&#34;campaign finance&#34;,&#34;catch release&#34;,&#34;chain migration&#34;,&#34;cia director&#34;,&#34;conflicted democrats&#34;,&#34;congressman keith&#34;,&#34;court justice&#34;,&#34;dishonest people&#34;,&#34;dnc server&#34;,&#34;doj fbi&#34;,&#34;don mcgahn&#34;,&#34;don’t worry&#34;,&#34;dossier paid&#34;,&#34;el salvador&#34;,&#34;elizabeth warren&#34;,&#34;entire nation&#34;,&#34;farm bill&#34;,&#34;farmers workers&#34;,&#34;fisa abuse&#34;,&#34;friend president&#34;,&#34;fusion gps&#34;,&#34;george h.w&#34;,&#34;governor jerry&#34;,&#34;h.w bush&#34;,&#34;highly trained&#34;,&#34;house chief&#34;,&#34;hunt continues&#34;,&#34;hunt headed&#34;,&#34;hunt hoax&#34;,&#34;illegal aliens&#34;,&#34;illicit scheme&#34;,&#34;immigration bill&#34;,&#34;incredible job&#34;,&#34;intellectual property&#34;,&#34;it’s time&#34;,&#34;jon tester&#34;,&#34;leaked classified&#34;,&#34;level playing&#34;,&#34;manufacturing jobs&#34;,&#34;march 5th&#34;,&#34;massive amounts&#34;,&#34;massive tax&#34;,&#34;massive trade&#34;,&#34;mexico canada&#34;,&#34;military protection&#34;,&#34;mueller witch&#34;,&#34;nbc news&#34;,&#34;nuclear testing&#34;,&#34;nuclear tests&#34;,&#34;oval office&#34;,&#34;phony russia&#34;,&#34;playing field&#34;,&#34;police officers&#34;,&#34;post office&#34;,&#34;primary win&#34;,&#34;raise taxes&#34;,&#34;real deal&#34;,&#34;regulation cuts&#34;,&#34;rick saccone&#34;,&#34;russia witch&#34;,&#34;saturday night&#34;,&#34;school safety&#34;,&#34;special council&#34;,&#34;strong border&#34;,&#34;strzok page&#34;,&#34;successful businessman&#34;,&#34;talented people&#34;,&#34;ted cruz&#34;,&#34;test site&#34;,&#34;text messages&#34;,&#34;tom fitton&#34;,&#34;total disaster&#34;,&#34;total disgrace&#34;,&#34;trade talks&#34;,&#34;trillion dollars&#34;,&#34;vets 2nd&#34;,&#34;visa lottery&#34;,&#34;woodward book&#34;,&#34;world war&#34;,&#34;13 gang&#34;,&#34;abolish ice&#34;,&#34;adam schiff&#34;,&#34;african americans&#34;,&#34;air force&#34;,&#34;american citizens&#34;,&#34;american patriots&#34;,&#34;american unemployment&#34;,&#34;annapolis maryland&#34;,&#34;anonymous source&#34;,&#34;august 7th&#34;,&#34;background checks&#34;,&#34;bad trade&#34;,&#34;barack obama&#34;,&#34;based immigration&#34;,&#34;billion trade&#34;,&#34;bless america&#34;,&#34;bob dole&#34;,&#34;border laws&#34;,&#34;borders crime&#34;,&#34;bump stocks&#34;,&#34;clinton emails&#34;,&#34;conflicted bob&#34;,&#34;congressman andy&#34;,&#34;correspondents dinner&#34;,&#34;crime border&#34;,&#34;crime loves&#34;,&#34;criminal justice&#34;,&#34;cut bill&#34;,&#34;cuts military&#34;,&#34;debbie stabenow&#34;,&#34;dirty dossier&#34;,&#34;dishonest reporting&#34;,&#34;don’t exist&#34;,&#34;duluth minnesota&#34;,&#34;easily win&#34;,&#34;election victory&#34;,&#34;existing conditions&#34;,&#34;fantastic governor&#34;,&#34;fbi comey&#34;,&#34;fbi director&#34;,&#34;fbi lovers&#34;,&#34;fisa court&#34;,&#34;fought hard&#34;,&#34;fraudulent dossier&#34;,&#34;gun free&#34;,&#34;harvard law&#34;,&#34;hillary clinton’s&#34;,&#34;house correspondents&#34;,&#34;immigration system&#34;,&#34;intelligence community&#34;,&#34;jobless claims&#34;,&#34;jobs maga&#34;,&#34;joint press&#34;,&#34;joseph mccarthy&#34;,&#34;josh hawley&#34;,&#34;judge kavanaugh&#34;,&#34;june 12th&#34;,&#34;kanye west&#34;,&#34;keith rothfus&#34;,&#34;las vegas&#34;,&#34;lover peter&#34;,&#34;maga rally&#34;,&#34;mark levin&#34;,&#34;martha mcsally&#34;,&#34;mccabe strzok&#34;,&#34;memorial day&#34;,&#34;mike pence&#34;,&#34;million people&#34;,&#34;moon township&#34;,&#34;mueller team&#34;,&#34;north dakota&#34;,&#34;outstanding job&#34;,&#34;palm beach&#34;,&#34;patrol agents&#34;,&#34;pay tribute&#34;,&#34;phony crime&#34;,&#34;phony story&#34;,&#34;pre existing&#34;,&#34;president george&#34;,&#34;presidential proclamation&#34;,&#34;prison reform&#34;,&#34;pro life&#34;,&#34;public safety&#34;,&#34;rally tickets&#34;,&#34;russian hoax&#34;,&#34;safety security&#34;,&#34;sanctuary policies&#34;,&#34;school shooting&#34;,&#34;september 11th&#34;,&#34;setting records&#34;,&#34;slippery james&#34;,&#34;social media&#34;,&#34;something’s happening&#34;,&#34;spending bill&#34;,&#34;stay tuned&#34;,&#34;steel industry&#34;,&#34;tax regulation&#34;,&#34;taxes substantially&#34;,&#34;terror attack&#34;,&#34;tomorrow night&#34;,&#34;total hoax&#34;,&#34;totally conflicted&#34;,&#34;township pennsylvania&#34;,&#34;trade agreement&#34;,&#34;trade surplus&#34;,&#34;trade war&#34;,&#34;tremendous success&#34;,&#34;trump agenda&#34;,&#34;trump team&#34;,&#34;u.s history&#34;,&#34;u.s military&#34;,&#34;u.s steel&#34;,&#34;vast amounts&#34;,&#34;washington michigan&#34;,&#34;wonderful family&#34;,&#34;worst trade&#34;,&#34;xi jinping&#34;,&#34;york city&#34;,&#34;10 00&#34;,&#34;100 billion&#34;,&#34;13 thugs&#34;,&#34;2 00&#34;,&#34;2018 election&#34;,&#34;2018 world&#34;,&#34;24 7&#34;,&#34;243rd birthday&#34;,&#34;500 billion&#34;,&#34;6 00&#34;,&#34;6 months&#34;,&#34;60 minutes&#34;,&#34;7 00pm&#34;,&#34;7 trillion&#34;,&#34;716 billion&#34;,&#34;abiding americans&#34;,&#34;acting attorney&#34;,&#34;aluminum industries&#34;,&#34;american dream&#34;,&#34;american president&#34;,&#34;american public&#34;,&#34;andrew cuomo&#34;,&#34;andrew gillum&#34;,&#34;andrew mccarthy&#34;,&#34;andy barr&#34;,&#34;armed forces&#34;,&#34;arms race&#34;,&#34;bad people&#34;,&#34;bad stories&#34;,&#34;barbara bush&#34;,&#34;biggest political&#34;,&#34;bilateral meeting&#34;,&#34;birthright citizenship&#34;,&#34;black americans&#34;,&#34;blue wave&#34;,&#34;bob casey&#34;,&#34;business optimism&#34;,&#34;california wildfires&#34;,&#34;campaign colluded&#34;,&#34;can’t win&#34;,&#34;carolina governor&#34;,&#34;cars coming&#34;,&#34;catherine herridge&#34;,&#34;chris farrell&#34;,&#34;city illinois&#34;,&#34;claire mccaskill&#34;,&#34;coast guard&#34;,&#34;companies moving&#34;,&#34;complete total&#34;,&#34;confidence hits&#34;,&#34;congressman ron&#34;,&#34;corrupt cities&#34;,&#34;corrupt mainstream&#34;,&#34;country’s history&#34;,&#34;court decision&#34;,&#34;court justices&#34;,&#34;crime collusion&#34;,&#34;crime strong&#34;,&#34;crooked hillary’s&#34;,&#34;deepest sympathies&#34;,&#34;deleted emails&#34;,&#34;delivery boy&#34;,&#34;dem votes&#34;,&#34;democrat inspired&#34;,&#34;democrat votes&#34;,&#34;democrats fault&#34;,&#34;derangement syndrome&#34;,&#34;discredited dossier&#34;,&#34;discredited mueller&#34;,&#34;doesn’t exist&#34;,&#34;drug prices&#34;,&#34;economic success&#34;,&#34;elected president&#34;,&#34;endorsement _balderson&#34;,&#34;enforcement officer&#34;,&#34;enforcement professionals&#34;,&#34;erie pennsylvania&#34;,&#34;exact opposite&#34;,&#34;expensive witch&#34;,&#34;facebook ads&#34;,&#34;fair deal&#34;,&#34;fair share&#34;,&#34;fallen heroes&#34;,&#34;false statements&#34;,&#34;fantastic evening&#34;,&#34;fantastic senator&#34;,&#34;farmers ranchers&#34;,&#34;fbi investigation&#34;,&#34;federal judge&#34;,&#34;fight hard&#34;,&#34;fine person&#34;,&#34;fired fbi&#34;,&#34;florida ron&#34;,&#34;forever grateful&#34;,&#34;free trade&#34;,&#34;free zones&#34;,&#34;government shutdown&#34;,&#34;governor rick&#34;],&#34;freq&#34;:[160,128,84,71,56,49,47,43,39,38,36,33,32,32,32,31,31,30,28,28,27,25,25,25,24,24,23,23,22,22,22,21,21,20,20,20,19,18,18,17,17,17,17,16,16,16,16,16,15,15,15,14,14,14,14,14,13,13,13,13,12,12,12,12,12,12,12,12,11,11,11,11,11,11,10,10,10,10,10,10,10,10,10,10,10,10,10,10,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3],&#34;fontFamily&#34;:&#34;Segoe UI&#34;,&#34;fontWeight&#34;:&#34;bold&#34;,&#34;color&#34;:&#34;random-light&#34;,&#34;minSize&#34;:0,&#34;weightFactor&#34;:0.5625,&#34;backgroundColor&#34;:&#34;black&#34;,&#34;gridSize&#34;:0,&#34;minRotation&#34;:-0.785398163397448,&#34;maxRotation&#34;:0.785398163397448,&#34;shuffle&#34;:true,&#34;rotateRatio&#34;:0.4,&#34;shape&#34;:&#34;circle&#34;,&#34;ellipticity&#34;:0.65,&#34;figBase64&#34;:null,&#34;hover&#34;:null},&#34;evals&#34;:[],&#34;jsHooks&#34;:{&#34;render&#34;:[{&#34;code&#34;:&#34;function(el,x){\n                        console.log(123);\n                        if(!iii){\n                          window.location.reload();\n                          iii = False;\n\n                        }\n  }&#34;,&#34;data&#34;:null}]}}&lt;/script&gt;
&lt;p&gt;I think that works quite nicely. The use of jpg for shapes has not worked for me. Nor have letterCloud.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(htmlwidgets)
library(webshot)
Ments.Tab &amp;lt;- data.frame(table(Ments))
Ments.Tab &amp;lt;- Ments.Tab[order(Ments.Tab$Freq, decreasing=TRUE),]
#webshot::install_phantomjs()
my.df.short &amp;lt;- my.df[c(1:40),]
hw1 &amp;lt;- letterCloud(Ments.Tab, &amp;quot;@&amp;quot;, size=4, color=&amp;#39;random-light&amp;#39;)
saveWidget(hw1,&amp;quot;1.html&amp;quot;,selfcontained = F)
webshot::webshot(&amp;quot;1.html&amp;quot;,&amp;quot;1.png&amp;quot;,vwidth = 700, vheight = 500, delay =10)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;1.png&#34; /&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>tidyTuesday meets the Economics of Majors</title>
      <link>https://www.data.viajes/post/tidytuesday-meets-the-economics-of-majors/</link>
      <pubDate>Wed, 17 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.data.viajes/post/tidytuesday-meets-the-economics-of-majors/</guid>
      <description>&lt;p&gt;This week’s tidyTuesday focuses on degrees and majors and their deployment in the labor market. The original data came from 538. A description of sources and measures. The tidyTesday writeup is &lt;a href=&#34;https://github.com/fivethirtyeight/data/tree/master/college-majors&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
options(scipen=6)
library(extrafont)
font_import()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Importing fonts may take a few minutes, depending on the number of fonts and the speed of the system.
## Continue? [y/n]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Major.Employment &amp;lt;- read.csv(&amp;quot;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2018-10-16/recent-grads.csv&amp;quot;)
library(skimr)
skim(Major.Employment)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Skim summary statistics
##  n obs: 173 
##  n variables: 21 
## 
## ── Variable type:factor ───────────────────────────────────────────────────────────────────────────────────────────────
##        variable missing complete   n n_unique
##           Major       0      173 173      173
##  Major_category       0      173 173       16
##                          top_counts ordered
##      ACC: 1, ACT: 1, ADV: 1, AER: 1   FALSE
##  Eng: 29, Edu: 16, Hum: 15, Bio: 14   FALSE
## 
## ── Variable type:integer ──────────────────────────────────────────────────────────────────────────────────────────────
##              variable missing complete   n     mean       sd    p0
##          College_jobs       0      173 173 12322.64 21299.87     0
##              Employed       0      173 173 31192.76 50675        0
##             Full_time       0      173 173 26029.31 42869.66   111
##  Full_time_year_round       0      173 173 19694.43 33160.94   111
##         Low_wage_jobs       0      173 173  3859.02  6945        0
##            Major_code       0      173 173  3879.82  1687.75  1100
##                Median       0      173 173 40151.45 11470.18 22000
##                   Men       1      172 173 16723.41 28122.43   119
##      Non_college_jobs       0      173 173 13284.5  23789.66     0
##                 P25th       0      173 173 29501.45  9166.01 18500
##                 P75th       0      173 173 51494.22 14906.28 22000
##             Part_time       0      173 173  8832.4  14648.18     0
##                  Rank       0      173 173    87       50.08     1
##           Sample_size       0      173 173   356.08   618.36     2
##                 Total       1      172 173 39370.08 63483.49   124
##            Unemployed       0      173 173  2416.33  4112.8      0
##                 Women       1      172 173 22646.67 41057.33     0
##       p25     p50      p75   p100     hist
##   1675     4390   14444    151643 ▇▂▁▁▁▁▁▁
##   3608    11797   31433    307933 ▇▁▁▁▁▁▁▁
##   3154    10048   25147    251540 ▇▁▁▁▁▁▁▁
##   2453     7413   16891    199897 ▇▁▁▁▁▁▁▁
##    340     1231    3466     48207 ▇▁▁▁▁▁▁▁
##   2403     3608    5503      6403 ▂▇▂▃▂▂▅▆
##  33000    36000   45000    110000 ▅▇▃▂▁▁▁▁
##   2177.5   5434   14631    173809 ▇▁▁▁▁▁▁▁
##   1591     4595   11783    148395 ▇▁▁▁▁▁▁▁
##  24000    27000   33000     95000 ▇▅▂▁▁▁▁▁
##  42000    47000   60000    125000 ▁▇▅▂▁▁▁▁
##   1030     3299    9948    115172 ▇▁▁▁▁▁▁▁
##     44       87     130       173 ▇▇▇▇▇▇▇▇
##     39      130     338      4212 ▇▁▁▁▁▁▁▁
##   4549.75 15104   38909.75 393735 ▇▁▁▁▁▁▁▁
##    304      893    2393     28169 ▇▁▁▁▁▁▁▁
##   1778.25  8386.5 22553.75 307087 ▇▁▁▁▁▁▁▁
## 
## ── Variable type:numeric ──────────────────────────────────────────────────────────────────────────────────────────────
##           variable missing complete   n  mean   sd p0  p25   p50   p75
##         ShareWomen       1      172 173 0.52  0.23  0 0.34 0.53  0.7  
##  Unemployment_rate       0      173 173 0.068 0.03  0 0.05 0.068 0.088
##  p100     hist
##  0.97 ▂▅▆▆▇▇▆▃
##  0.18 ▂▃▇▇▅▁▁▁&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A scatterplot of the unemployment rate by majors is the first goal with a color scheme that reflects the proportion of females in the industry.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my.plot &amp;lt;- Major.Employment %&amp;gt;% ggplot(aes(Unemployment_rate,Median, label=str_to_title(Major), color=ShareWomen)) +
  geom_point() +
  geom_text(check_overlap = T, vjust=-0.5, nudge_y=0.1, size=2.5) +
  theme_minimal() +
  scale_color_gradient(name=&amp;quot;Share of Women&amp;quot;, low=&amp;quot;#de2d26&amp;quot;, high = &amp;quot;#e9a3c9&amp;quot;) + 
  scale_y_continuous(labels = scales::comma) +
  scale_x_continuous(labels = scales::percent) + 
  xlab(&amp;quot;Unemployment Rate&amp;quot;) +
  ylab(&amp;quot;Median Income&amp;quot;) +
  ggtitle(&amp;quot;Median Income and Unemployment&amp;quot;) +
  theme(text=element_text(family = &amp;quot;Roboto Light&amp;quot;, size=8), title = element_text(size=12)) 
my.plot&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.data.viajes/post/2018-10-17-tidytuesday-meets-the-economics-of-majors_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Major.Employment &amp;lt;- Major.Employment %&amp;gt;% mutate(ShareCol= College_jobs / Total)
my.plot &amp;lt;- Major.Employment %&amp;gt;% ggplot(aes(Unemployment_rate,ShareCol, label=str_to_title(Major), color=ShareWomen)) +
  geom_point(alpha=0.1) +
  geom_text(check_overlap = T, size=1.5) +
  theme_minimal() +
  scale_color_gradient(name=&amp;quot;Share of Women&amp;quot;, low=&amp;quot;#de2d26&amp;quot;, high = &amp;quot;#e9a3c9&amp;quot;) + 
#  scale_y_continuous(labels = scales::comma) +
  scale_x_continuous(labels = scales::percent) + 
  xlab(&amp;quot;Unemployment Rate&amp;quot;) +
  ylab(&amp;quot;College Pct.&amp;quot;) +
  ggtitle(&amp;quot;College Pct. Jobs and Unemployment&amp;quot;)
my.plot&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 1 rows containing missing values (geom_point).&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 1 rows containing missing values (geom_text).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.data.viajes/post/2018-10-17-tidytuesday-meets-the-economics-of-majors_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;An Esquisse starter. Unemployment rate is x. Median wage is y. Major categories are colors and size is a function of Total&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = Major.Employment) +
aes(x = Unemployment_rate, y = Median, color = Major_category, size = Total) +
geom_point() +
theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 1 rows containing missing values (geom_point).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.data.viajes/post/2018-10-17-tidytuesday-meets-the-economics-of-majors_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Major.Employment %&amp;gt;% drop_na() %&amp;gt;% ggplot() +
  aes(x = Unemployment_rate, y = Median, color = ShareWomen, label=str_to_title(Major)) +
#  geom_point() +
  geom_text(check_overlap = T, size=2) +
  theme_minimal() +
  scale_color_gradient(name=&amp;quot;Share of Women&amp;quot;, low=&amp;quot;#cda7ca&amp;quot;, high = &amp;quot;#3d323c&amp;quot;) + 
  scale_x_continuous(labels = scales::percent) + 
  scale_y_continuous(labels = scales::comma) +
  xlab(&amp;quot;Unemployment Rate&amp;quot;) +
  ylab(&amp;quot;Median Wage&amp;quot;) +
  ggtitle(&amp;quot;Wages and Unemployment with Women in the Profession&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.data.viajes/post/2018-10-17-tidytuesday-meets-the-economics-of-majors_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Alas.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Data Analysis, Modelling, and Decision-Making</title>
      <link>https://www.data.viajes/courses/5103dadm/</link>
      <pubDate>Tue, 02 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.data.viajes/courses/5103dadm/</guid>
      <description>&lt;p&gt;The course website for the &lt;a href=&#34;https://wise.willamette.edu/portal/site/GSM-5103-01-18_FA&#34; target=&#34;_blank&#34;&gt;Fall of 2018&lt;/a&gt;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.openintro.org/stat/textbook.php?stat_book=os&#34; target=&#34;_blank&#34;&gt;OpenIntro Stats&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://radiant-rstats.github.io/docs/index.html&#34; target=&#34;_blank&#34;&gt;Radiant&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.rstudio.com&#34; target=&#34;_blank&#34;&gt;RStudio&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cran.r-project.org&#34; target=&#34;_blank&#34;&gt;R&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://rpubs.com/rwalkerWU/&#34; target=&#34;_blank&#34;&gt;RPubs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.data.viajes/courses/practicemidterm/&#34; target=&#34;_blank&#34;&gt;Practice Midterm Solution&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>tidyTuesday: coffee chains</title>
      <link>https://www.data.viajes/post/tidytuesday-coffee-chains/</link>
      <pubDate>Wed, 09 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.data.viajes/post/tidytuesday-coffee-chains/</guid>
      <description>&lt;div id=&#34;the-tidytuesday-for-this-week-is-coffee-chain-locations&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The tidyTuesday for this week is coffee chain locations&lt;/h1&gt;
&lt;p&gt;For this week: 1. The basic link to the &lt;a href=&#34;https://github.com/rfordatascience/tidytuesday&#34;&gt;&lt;code&gt;#tidyTuesday&lt;/code&gt;&lt;/a&gt; shows &lt;a href=&#34;http://flowingdata.com/2014/03/18/coffee-place-geography/&#34;&gt;an original article&lt;/a&gt; for Week 6.&lt;/p&gt;
&lt;p&gt;First, let’s import the data; it is a single Excel spreadsheet. The page notes that starbucks, Tim Horton, and Dunkin Donuts have raw data available.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(here)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## here() starts at /home/rob/R/MyNLWeb&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(readxl)
library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching packages ────────────────────────────────────────────────────────────────────────────── tidyverse 1.2.1 ──&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ✔ ggplot2 3.1.0     ✔ purrr   0.2.5
## ✔ tibble  1.4.2     ✔ dplyr   0.7.8
## ✔ tidyr   0.8.2     ✔ stringr 1.3.1
## ✔ readr   1.3.0     ✔ forcats 0.3.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Conflicts ───────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(janitor)
library(geofacet)
library(ggbeeswarm)
library(ggrepel)
Starbucks &amp;lt;- read_excel(paste0(here(),&amp;quot;/data/week6_coffee_chains.xlsx&amp;quot;), sheet = &amp;quot;starbucks&amp;quot;)
Dunkin.Donuts &amp;lt;- read_excel(paste0(here(),&amp;quot;/data/week6_coffee_chains.xlsx&amp;quot;), sheet = &amp;quot;dunkin&amp;quot;)
Tim.Hortons &amp;lt;- read_excel(paste0(here(),&amp;quot;/data/week6_coffee_chains.xlsx&amp;quot;), sheet = &amp;quot;timhorton&amp;quot;)
# A great function appears below that I grabbed from Stack Overflow a while back.....
read_excel_allsheets &amp;lt;- function(filename, tibble = TRUE) {
    sheets &amp;lt;- readxl::excel_sheets(filename)
    x &amp;lt;- lapply(sheets, function(X) readxl::read_excel(filename, sheet = X))
    if(!tibble) x &amp;lt;- lapply(x, as.data.frame)
    names(x) &amp;lt;- sheets
    x
}
coffee.xl &amp;lt;- read_excel_allsheets(paste0(here(),&amp;quot;/data/week6_coffee_chains.xlsx&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What do the data look like?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(skimr)
skim(Starbucks)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Skim summary statistics
##  n obs: 25600 
##  n variables: 13 
## 
## ── Variable type:character ────────────────────────────────────────────────────────────────────────────────────────────
##        variable missing complete     n min max empty n_unique
##           Brand       0    25600 25600   7  21     0        4
##            City      14    25586 25600   2  29     0     5470
##         Country       0    25600 25600   2   2     0       73
##  Ownership Type       0    25600 25600   8  13     0        4
##    Phone Number    6861    18739 25600   1  18     0    18559
##        Postcode    1521    24079 25600   1   9     0    18888
##  State/Province       0    25600 25600   1   3     0      338
##      Store Name       0    25600 25600   2  60     0    25364
##    Store Number       0    25600 25600   5  12     0    25599
##  Street Address       2    25598 25600   1 234     0    25353
##        Timezone       0    25600 25600  18  30     0      101
## 
## ── Variable type:numeric ──────────────────────────────────────────────────────────────────────────────────────────────
##   variable missing complete     n   mean    sd      p0     p25    p50
##   Latitude       1    25599 25600  34.79 13.34  -46.41   31.24  36.75
##  Longitude       1    25599 25600 -27.87 96.84 -159.46 -104.66 -79.35
##     p75   p100     hist
##   41.57  64.85 ▁▁▁▁▂▇▇▁
##  100.63 176.92 ▃▇▂▁▁▁▅▁&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;skim(Dunkin.Donuts)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Skim summary statistics
##  n obs: 4898 
##  n variables: 22 
## 
## ── Variable type:character ────────────────────────────────────────────────────────────────────────────────────────────
##    variable missing complete    n min max empty n_unique
##    biz_info    4091      807 4898  14  18     0      709
##    biz_name       0     4898 4898   8  38     0       33
##   biz_phone       0     4898 4898  14  14     0     4562
##   e_address       0     4898 4898   6  61     0     4864
##      e_city       0     4898 4898   2  27     0     1770
##   e_country       0     4898 4898   3   3     0        1
##     e_state       0     4898 4898   2   2     0       41
##  e_zip_full       0     4898 4898  10  10     0      545
##  loc_county       0     4898 4898   3  21     0      395
##     loc_DST       0     4898 4898   1   1     0        3
##    loc_PMSA       0     4898 4898   2   4     0       53
##      loc_TZ       0     4898 4898   3   5     0        5
##     web_url       0     4898 4898  20 175     0       22
## 
## ── Variable type:numeric ──────────────────────────────────────────────────────────────────────────────────────────────
##           variable missing complete    n     mean       sd      p0
##           e_postal       0     4898 4898 21528.34 20311.57 1001   
##                 id       0     4898 4898  2459.46  1420.28    1   
##      loc_area_code       0     4898 4898   590.09   229.26  201   
##           loc_FIPS       0     4898 4898 27911.17 12470.14 1069   
##   loc_LAT_centroid       0     4898 4898    39.62     4.33   21.42
##       loc_LAT_poly       0     4898 4898    39.62     4.32   21.39
##  loc_LONG_centroid       0     4898 4898   -77.55     7.31 -157.93
##      loc_LONG_poly       0     4898 4898   -77.55     7.31 -157.96
##            loc_MSA       0     4898 4898  4284.65  2849.57  160   
##       p25      p50      p75     p100     hist
##   6080    13334.5  32810.75 98112    ▇▃▂▁▂▁▁▁
##   1231.25  2458.5   3686.75  4920    ▇▇▇▇▇▇▇▇
##    401      610      781      989    ▆▃▅▅▇▇▆▃
##  17031    26125    36111    55111    ▁▇▅▇▆▆▆▂
##     39.39    41.22    42.11    47.63 ▁▁▁▁▁▅▇▁
##     39.38    41.2     42.09    47.64 ▁▁▁▁▁▅▇▁
##    -81.44   -75.08   -72.66   -67.23 ▁▁▁▁▁▁▃▇
##    -81.44   -75.08   -72.66   -67.28 ▁▁▁▁▁▁▃▇
##   1520     3800     6880     9320    ▇▇▅▂▂▇▅▂&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;skim(Tim.Hortons)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Skim summary statistics
##  n obs: 4955 
##  n variables: 6 
## 
## ── Variable type:character ────────────────────────────────────────────────────────────────────────────────────────────
##     variable missing complete    n min max empty n_unique
##      address       0     4955 4955   6  51     0     4803
##         city       0     4955 4955   3  38     0     1206
##      country       0     4955 4955   2   2     0        2
##  postal_code       0     4955 4955   4   7     0     4328
##        state       0     4955 4955   2   2     0       27
##   store_name       0     4955 4955   2  63     0     3167&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A basic plot of the global Starbucks data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggmap)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Google Maps API Terms of Service: http://developers.google.com/maps/terms.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Please cite ggmap if you use it: see citation(&amp;quot;ggmap&amp;quot;) for details.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mapWorld &amp;lt;- borders(&amp;quot;world&amp;quot;, colour=&amp;quot;gray50&amp;quot;, fill=&amp;quot;gray50&amp;quot;) # create a layer of borders&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;maps&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:purrr&amp;#39;:
## 
##     map&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mp &amp;lt;- ggplot() +   mapWorld
mp &amp;lt;- mp + geom_point(aes(x=Starbucks$Longitude, y=Starbucks$Latitude) ,color=&amp;quot;dark green&amp;quot;, size=0.5) + xlab(&amp;quot;&amp;quot;) + ylab(&amp;quot;&amp;quot;)
mp &amp;lt;- mp + geom_point(aes(x=Dunkin.Donuts$loc_LONG_centroid, y=Dunkin.Donuts$loc_LAT_centroid) ,color=&amp;quot;orange&amp;quot;, size=0.5) + xlab(&amp;quot;&amp;quot;) + ylab(&amp;quot;&amp;quot;)
mp&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 1 rows containing missing values (geom_point).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.data.viajes/post/2018-05-09-tidytuesday-coffee-chains_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;starbucks-and-dunkin&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Starbucks and Dunkin&lt;/h2&gt;
&lt;p&gt;Google Maps interface changed and I have not updated this part. Shame.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Global mortality tidyTuesday</title>
      <link>https://www.data.viajes/post/tidytuesday-takes-on-global-mortality/</link>
      <pubDate>Wed, 18 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.data.viajes/post/tidytuesday-takes-on-global-mortality/</guid>
      <description>&lt;div id=&#34;tidytuesday-on-global-mortality&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;tidyTuesday on Global Mortality&lt;/h1&gt;
&lt;p&gt;The three generic challenge graphics involve two global summaries, a raw count by type and a percentage by type. The individual county breakdowns are recorded for a predetermined year below. This can all be seen in &lt;a href=&#34;https://ourworldindata.org/what-does-the-world-die-from&#34;&gt;the original&lt;/a&gt;. For whatever reason, I cannot open this data remotely.&lt;/p&gt;
&lt;p&gt;Here is this week’s tidyTuesday.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(here)
library(readxl)
library(skimr)
library(tidyverse)
library(rlang)
global_mortality &amp;lt;- read_excel(paste0(here(),&amp;quot;/data/global_mortality.xlsx&amp;quot;))
skim(global_mortality)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Skim summary statistics
##  n obs: 6156 
##  n variables: 35 
## 
## ── Variable type:character ────────────────────────────────────────────────────────────────────────────────────────────
##      variable missing complete    n min max empty n_unique
##       country       0     6156 6156   4  32     0      228
##  country_code     864     5292 6156   3   8     0      196
## 
## ── Variable type:numeric ──────────────────────────────────────────────────────────────────────────────────────────────
##                                  variable missing complete    n     mean
##                     Alcohol disorders (%)       0     6156 6156    0.32 
##                               Cancers (%)       0     6156 6156   14.39 
##               Cardiovascular diseases (%)       0     6156 6156   29.93 
##                              Conflict (%)    1398     4758 6156    0.29 
##                              Dementia (%)       0     6156 6156    3.22 
##                              Diabetes (%)       0     6156 6156    6.29 
##                    Diarrheal diseases (%)       0     6156 6156    3.2  
##                    Digestive diseases (%)       0     6156 6156    1.97 
##                              Drowning (%)       0     6156 6156    0.71 
##                        Drug disorders (%)       0     6156 6156    0.18 
##                                  Fire (%)       0     6156 6156    0.33 
##  Heat-related (hot and cold exposure) (%)       0     6156 6156    0.1  
##                             Hepatitis (%)       0     6156 6156    0.16 
##                              HIV/AIDS (%)       0     6156 6156    3.35 
##                              Homicide (%)       0     6156 6156    0.98 
##        Intestinal infectious diseases (%)       0     6156 6156    0.18 
##                        Kidney disease (%)       0     6156 6156    2.09 
##                         Liver disease (%)       0     6156 6156    2.12 
##          Lower respiratory infections (%)       0     6156 6156    5.84 
##                               Malaria (%)       0     6156 6156    1.8  
##                       Maternal deaths (%)       0     6156 6156    0.59 
##                            Meningitis (%)       0     6156 6156    0.78 
##                     Natural disasters (%)       0     6156 6156    0.095
##                       Neonatal deaths (%)       0     6156 6156    4.57 
##              Nutritional deficiencies (%)       0     6156 6156    1.1  
##                     Parkinson disease (%)       0     6156 6156    0.29 
##           Protein-energy malnutrition (%)       0     6156 6156    1    
##                  Respiratory diseases (%)       0     6156 6156    4.1  
##                        Road accidents (%)       0     6156 6156    2.53 
##                               Suicide (%)       0     6156 6156    1.39 
##                             Terrorism (%)    1398     4758 6156    0.037
##                          Tuberculosis (%)       0     6156 6156    2.13 
##                                      year       0     6156 6156 2003    
##     sd         p0        p25       p50      p75    p100     hist
##   0.41    0.013      0.078      0.16      0.39     3.08 ▇▂▁▁▁▁▁▁
##   8.15    0.58       6.93      13.31     21.36    33.62 ▃▇▅▅▃▃▃▁
##  14.02    1.43      18.74      30.65     38.45    67.39 ▂▅▃▇▆▂▂▁
##   2.4     0          0          0         0.017   82.32 ▇▁▁▁▁▁▁▁
##   2.75    0.045      1.01       2.53      4.33    16.67 ▇▆▂▂▁▁▁▁
##   4.44    0.33       3.2        4.99      7.93    35.82 ▇▆▂▁▁▁▁▁
##   4.36    0.0083     0.18       0.77      5.29    25.18 ▇▂▁▁▁▁▁▁
##   0.68    0.31       1.51       1.93      2.29     5.16 ▁▅▇▅▂▁▁▁
##   0.51    0.053      0.35       0.61      0.96     4.51 ▇▆▂▁▁▁▁▁
##   0.19    0.0018     0.058      0.12      0.23     1.31 ▇▃▁▁▁▁▁▁
##   0.18    0.057      0.19       0.32      0.43     1.34 ▇▇▆▂▁▁▁▁
##   0.13    0.0071     0.042      0.07      0.11     1.17 ▇▁▁▁▁▁▁▁
##   0.17    0.0048     0.038      0.11      0.24     1.58 ▇▃▁▁▁▁▁▁
##   7.81    0          0.081      0.44      2.33    62.19 ▇▁▁▁▁▁▁▁
##   1.45    0.045      0.24       0.52      1       14.23 ▇▁▁▁▁▁▁▁
##   0.3     7.1e-05    0.00077    0.014     0.27     2.28 ▇▂▁▁▁▁▁▁
##   1.49    0.056      0.9        1.73      2.96     9.95 ▇▇▅▂▁▁▁▁
##   1.24    0.19       1.34       1.83      2.54    11.65 ▇▇▂▁▁▁▁▁
##   3.42    0.68       3.21       5.14      8.16    20.04 ▆▇▅▃▂▁▁▁
##   4.21    0          0          0.0027    0.48    24.43 ▇▁▁▁▁▁▁▁
##   0.7     0.0019     0.032      0.24      1        3.41 ▇▂▁▂▁▁▁▁
##   0.97    0.028      0.11       0.35      1.02     6.98 ▇▁▁▁▁▁▁▁
##   1.28    0          0          0         0.017   65.29 ▇▁▁▁▁▁▁▁
##   3.85    0.041      0.69       3.89      7.74    17.81 ▇▃▃▃▂▁▁▁
##   1.87    0.0037     0.09       0.4       1.34    35.55 ▇▁▁▁▁▁▁▁
##   0.26    0.0023     0.074      0.21      0.43     1.59 ▇▅▂▂▁▁▁▁
##   1.81    0.0011     0.058      0.32      1.22    35.52 ▇▁▁▁▁▁▁▁
##   2.35    0.3        2.26       3.63      5.38    16.29 ▆▇▆▂▁▁▁▁
##   2.19    0.28       1.36       1.93      2.9     20.9  ▇▂▁▁▁▁▁▁
##   1.11    0.1        0.69       1.18      1.8     15.41 ▇▂▁▁▁▁▁▁
##   0.23    0          0          0         0.004    5.88 ▇▁▁▁▁▁▁▁
##   2.65    0.011      0.24       0.91      3.33    16.47 ▇▂▂▁▁▁▁▁
##   7.79 1990       1996       2003      2010     2016    ▇▆▆▇▆▆▆▇&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That loads the data for the challenge.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Counts &amp;lt;- read.csv(paste0(here(),&amp;quot;/data/annual-number-of-deaths-by-cause.csv&amp;quot;))
Counts %&amp;gt;% filter(Year==2016) %&amp;gt;% select(-Code,-Entity,-Execution..deaths.,-Year) %&amp;gt;% apply(., 2, function(x) { sum(x, na.rm=TRUE)}) -&amp;gt; temp1
pp.df &amp;lt;- data.frame(Total.Deaths=temp1,name=names(temp1))
pp.df &amp;lt;- pp.df %&amp;gt;% arrange(Total.Deaths)
pp.df$name &amp;lt;- factor(pp.df$name, levels = pp.df$name)
cplot &amp;lt;- ggplot(pp.df, aes(name,Total.Deaths)) + geom_bar(stat=&amp;quot;identity&amp;quot;) + coord_flip() + scale_fill_gradientn(colours = terrain.colors(10)) + ggtitle(&amp;quot;The Causes of Global Mortailty (2016)&amp;quot;)
cplot&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.data.viajes/post/2018-04-17-tidytuesday-takes-on-global-mortality_files/figure-html/COrig-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;With a reenactment of the base target plots, I can turn to new visuals. I wanted to be able to develop a comparison of the various classified causes of death and to try out my nifty function for summarizing panel data. So here goes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;xtsum &amp;lt;- function(formula, data) {
  pform &amp;lt;- terms(formula, data=data)
  unit &amp;lt;- pform[[2]]
  vars &amp;lt;- attr(pform, &amp;quot;term.labels&amp;quot;)
  cls &amp;lt;- sapply(data, class)
  data &amp;lt;- data %&amp;gt;% select(which(cls%in%c(&amp;quot;numeric&amp;quot;,&amp;quot;integer&amp;quot;)))
  varnames &amp;lt;- intersect(names(data),vars)
  sumfunc &amp;lt;- function(data=data, varname, unit) {
  loc.unit &amp;lt;- enquo(unit)
  varname &amp;lt;- ensym(varname)
    ores &amp;lt;- data %&amp;gt;% filter(!is.na(!! varname)==TRUE) %&amp;gt;% summarise(
    O.mean=round(mean(`$`(data, !! varname), na.rm=TRUE), digits=3),
    O.sd=round(sd(`$`(data, !! varname), na.rm=TRUE), digits=3), 
    O.min = min(`$`(data, !! varname), na.rm=TRUE), 
    O.max=max(`$`(data, !! varname), na.rm=TRUE), 
    O.SumSQ=round(sum(scale(`$`(data, !! varname), center=TRUE, scale=FALSE)^2, na.rm=TRUE), digits=3), 
    O.N=sum(as.numeric((!is.na(`$`(data, !! varname))))))
 bmeans &amp;lt;- data %&amp;gt;% filter(!is.na(!! varname)==TRUE) %&amp;gt;% group_by(!! loc.unit) %&amp;gt;% summarise(
   meanx=mean(`$`(.data, !! varname), na.rm=T), 
   t.count=sum(as.numeric(!is.na(`$`(.data, !! varname)))))
    bres &amp;lt;- bmeans %&amp;gt;% ungroup() %&amp;gt;% summarise(
    B.mean = round(mean(meanx, na.rm=TRUE), digits=3),
    B.sd = round(sd(meanx, na.rm=TRUE), digits=3),
    B.min = min(meanx, na.rm=TRUE), 
    B.max=max(meanx, na.rm=TRUE), 
    Units=sum(as.numeric(!is.na(t.count))), 
    t.bar=round(mean(t.count, na.rm=TRUE), digits=3))
  wdat &amp;lt;- data %&amp;gt;% filter(!is.na(!! varname)==TRUE) %&amp;gt;% group_by(!! loc.unit) %&amp;gt;% mutate(
    W.x = scale(`$`(.data,!! varname), scale=FALSE))
  wres &amp;lt;- wdat %&amp;gt;% ungroup() %&amp;gt;% summarise(
    W.sd=round(sd(W.x, na.rm=TRUE), digits=3), 
    W.min=min(W.x, na.rm=TRUE), 
    W.max=max(W.x, na.rm=TRUE), 
    W.SumSQ=round(sum(W.x^2, na.rm=TRUE), digits=3))
    W.Ratio &amp;lt;- round(wres$W.SumSQ/ores$O.SumSQ, digits=3)
  return(c(ores,bres,wres,Within.Ovr.Ratio=W.Ratio))
  }
res1 &amp;lt;- sapply(varnames, function(x) {sumfunc(data, !!x, !!unit)})
return(t(res1))
}  
global_mortality$countryF &amp;lt;- as.factor(global_mortality$country)
global_mortality$countryN &amp;lt;- as.numeric(as.factor(global_mortality$country))
names(global_mortality) &amp;lt;- gsub(&amp;quot; \\(%\\)&amp;quot;,&amp;quot;&amp;quot;,names(global_mortality))
# For some reason, the xtsum function does not respond to the weird variable names but will accept them devoid of (%)
myxt.res &amp;lt;- xtsum(countryN~., data=global_mortality)
myxt.res&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              O.mean O.sd  O.min       O.max    O.SumSQ  O.N  B.mean B.sd 
## year         2003   7.79  1990        2016     373464   6156 2003   0    
## Cancers      14.387 8.154 0.5822726   33.6175  409267.1 6156 14.387 8.034
## Diabetes     6.286  4.436 0.3271329   35.81619 121099.2 6156 6.286  4.24 
## Dementia     3.221  2.746 0.04475178  16.67248 46417.87 6156 3.221  2.618
## Tuberculosis 2.133  2.648 0.01088091  16.46586 43159.87 6156 2.133  2.554
## Suicide      1.391  1.111 0.1016103   15.41202 7591.252 6156 1.391  1.086
## Malaria      1.801  4.213 0           24.42596 109251.4 6156 1.801  4.11 
## Homicide     0.983  1.454 0.04520209  14.22926 13007.95 6156 0.983  1.425
## Meningitis   0.783  0.967 0.02798604  6.981346 5760.664 6156 0.783  0.951
## Drowning     0.714  0.514 0.05330638  4.510948 1628.778 6156 0.714  0.487
## Hepatitis    0.161  0.165 0.004847886 1.583289 167.675  6156 0.161  0.158
## Fire         0.334  0.181 0.05691324  1.343686 201.769  6156 0.334  0.168
## Conflict     0.291  2.399 0           82.317   27372.61 4758 0.291  0.921
## Terrorism    0.037  0.229 0           5.877    250.161  4758 0.037  0.121
##              B.min       B.max     Units t.bar W.sd  W.min      W.max    
## year         2003        2003      228   27    7.79  -13        13       
## Cancers      3.109526    30.19716  228   27    1.489 -8.017757  5.449552 
## Diabetes     1.183787    27.70547  228   27    1.331 -9.750057  10.92867 
## Dementia     0.2734327   11.82133  228   27    0.847 -5.183193  5.145011 
## Tuberculosis 0.03205589  14.84698  228   27    0.72  -6.169898  7.156982 
## Suicide      0.2441106   12.16457  228   27    0.243 -2.210464  3.247445 
## Malaria      0           20.10408  228   27    0.963 -7.916651  6.903596 
## Homicide     0.05593046  11.06133  228   27    0.303 -3.487857  3.167933 
## Meningitis   0.04221932  5.315156  228   27    0.187 -1.984723  2.166714 
## Drowning     0.06142987  3.515331  228   27    0.168 -1.775808  1.274717 
## Hepatitis    0.008156611 0.9932521 228   27    0.048 -0.3946904 0.8763799
## Fire         0.07003296  0.9950066 228   27    0.069 -0.4329619 0.8721761
## Conflict     0           9.320885  183   26    2.216 -9.320885  78.38404 
## Terrorism    0           1.298962  183   26    0.195 -1.298962  4.578038 
##              W.SumSQ  Within.Ovr.Ratio
## year         373464   1               
## Cancers      13651.25 0.033           
## Diabetes     10897.8  0.09            
## Dementia     4420.371 0.095           
## Tuberculosis 3193.133 0.074           
## Suicide      364.808  0.048           
## Malaria      5711.216 0.052           
## Homicide     564.186  0.043           
## Meningitis   216.175  0.038           
## Drowning     174.405  0.107           
## Hepatitis    14.147   0.084           
## Fire         29.045   0.144           
## Conflict     23357.85 0.853           
## Terrorism    180.881  0.723&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The function output can be read as follows. It needs better formatting as a table. For now, O. is an overall measure, overall mean, standard deviation, minimum, maximum, sum of squares and total observations. A between mean, standard deviation, minimum, and maximum with the number of units and the average number of time points. Finally, we have a within standard deviation, minimum, maximum, sum of squares, and a within proportion of the overall variance. In this case, terrorism and conflict are the two variables that vary almost entirely within and far less between countries. I suspect this is because they are rather high in a few places and consistently so.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot.res &amp;lt;- data.frame(myxt.res)
ggplot.res &amp;lt;- ggplot.res[-1,]; ggplot.res &amp;lt;- ggplot.res[,17]
mydf &amp;lt;- t(data.frame(ggplot.res))
mydf &amp;lt;- data.frame(value=mydf,name=rownames(mydf))
mydf &amp;lt;- mydf %&amp;gt;% arrange(value)
mydf$name &amp;lt;- factor(mydf$name, levels = mydf$name)
mydf &amp;lt;- mydf %&amp;gt;% mutate(Emph=as.numeric(value&amp;gt;0.5))
mydf %&amp;gt;% ggplot(aes(name,value, fill=Emph)) + geom_bar(stat=&amp;quot;identity&amp;quot;) + coord_flip() + ylab(&amp;quot;Within Percent of Total Variation&amp;quot;) + xlab(&amp;quot;Cause of Mortality&amp;quot;) + ggtitle(&amp;quot;Within-country variation in the Causes of Death&amp;quot;) + guides(fill=&amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.data.viajes/post/2018-04-17-tidytuesday-takes-on-global-mortality_files/figure-html/Result-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Mean.Homicide &amp;lt;- global_mortality %&amp;gt;% group_by(countryF) %&amp;gt;% summarise(mymean=mean(Homicide, na.rm=TRUE))
Mean.Homicide %&amp;gt;% ungroup() %&amp;gt;% arrange(mymean) -&amp;gt; mydf
mydf$countryF &amp;lt;- factor(mydf$countryF, levels = mydf$countryF)
mydf %&amp;gt;% ungroup() %&amp;gt;% top_n(30, mymean) %&amp;gt;% arrange(mymean) %&amp;gt;% mutate(Emph=c(rep(1,10),rep(2,10),rep(3,10))) -&amp;gt; mydf
mydf %&amp;gt;% ggplot(aes(countryF,mymean, fill=Emph)) + geom_bar(stat=&amp;quot;identity&amp;quot;) + coord_flip() + ylab(&amp;quot;Homicides&amp;quot;) + ggtitle(&amp;quot;Top 30 Countries/Places in Homicide&amp;quot;) + xlab(&amp;quot;&amp;quot;) + guides(fill=&amp;quot;none&amp;quot;) -&amp;gt; Homicideplot
Homicideplot&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.data.viajes/post/2018-04-17-tidytuesday-takes-on-global-mortality_files/figure-html/MHG30-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;global_mortality %&amp;gt;% filter(year==2016) %&amp;gt;% ggplot(aes(`Drug disorders`,`Alcohol disorders`)) + geom_point() + ggtitle(&amp;quot;Drugs and Alcohol in 2016&amp;quot;) -&amp;gt; scatterDA
scatterDA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.data.viajes/post/2018-04-17-tidytuesday-takes-on-global-mortality_files/figure-html/DandA-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Scraping EPL Salary Data</title>
      <link>https://www.data.viajes/post/scraping-epl-salary-data/</link>
      <pubDate>Sun, 08 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.data.viajes/post/scraping-epl-salary-data/</guid>
      <description>&lt;div id=&#34;epl-scraping&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;EPL Scraping&lt;/h1&gt;
&lt;p&gt;In a previous &lt;a href=&#34;https://www.data.viajes/post/scraping-the-nfl-salary-cap-data-with-python-and-r/&#34;&gt;post&lt;/a&gt;, I scraped some NFL data and learned the structure of Sportrac. Now, I want to scrape the available data on the EPL. The EPL data is organized in a few distinct but potentially linked tables. The basic structure is organized around team folders. Let me begin by isolating those URLs.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rvest)
library(tidyverse)
base_url &amp;lt;- &amp;quot;http://www.spotrac.com/epl/&amp;quot;
read.base &amp;lt;- read_html(base_url)
team.URL &amp;lt;- read.base %&amp;gt;% html_nodes(&amp;quot;.team-name&amp;quot;) %&amp;gt;% html_attr(&amp;#39;href&amp;#39;)
team.URL&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;https://www.spotrac.com/epl/afc-bournemouth/&amp;quot;            
##  [2] &amp;quot;https://www.spotrac.com/epl/arsenal-f.c/&amp;quot;                
##  [3] &amp;quot;https://www.spotrac.com/epl/brighton-hove-albion/&amp;quot;       
##  [4] &amp;quot;https://www.spotrac.com/epl/burnley-f.c/&amp;quot;                
##  [5] &amp;quot;https://www.spotrac.com/epl/cardiff-city-f.c/&amp;quot;           
##  [6] &amp;quot;https://www.spotrac.com/epl/chelsea-f.c/&amp;quot;                
##  [7] &amp;quot;https://www.spotrac.com/epl/crystal-palace/&amp;quot;             
##  [8] &amp;quot;https://www.spotrac.com/epl/everton-f.c/&amp;quot;                
##  [9] &amp;quot;https://www.spotrac.com/epl/fulham-f.c/&amp;quot;                 
## [10] &amp;quot;https://www.spotrac.com/epl/huddersfield-town/&amp;quot;          
## [11] &amp;quot;https://www.spotrac.com/epl/leicester-city/&amp;quot;             
## [12] &amp;quot;https://www.spotrac.com/epl/liverpool-f.c/&amp;quot;              
## [13] &amp;quot;https://www.spotrac.com/epl/manchester-city-f.c/&amp;quot;        
## [14] &amp;quot;https://www.spotrac.com/epl/manchester-united-f.c/&amp;quot;      
## [15] &amp;quot;https://www.spotrac.com/epl/newcastle-united-f.c/&amp;quot;       
## [16] &amp;quot;https://www.spotrac.com/epl/southampton-f.c/&amp;quot;            
## [17] &amp;quot;https://www.spotrac.com/epl/tottenham-hotspur-f.c/&amp;quot;      
## [18] &amp;quot;https://www.spotrac.com/epl/watford/&amp;quot;                    
## [19] &amp;quot;https://www.spotrac.com/epl/west-ham-united-f.c/&amp;quot;        
## [20] &amp;quot;https://www.spotrac.com/epl/wolverhampton-wanderers-f.c/&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Clean up the URLs to get the team names by themselves.
team.names &amp;lt;- gsub(base_url, &amp;quot;&amp;quot;, team.URL)
team.names &amp;lt;- gsub(&amp;quot;-f.c&amp;quot;, &amp;quot; FC&amp;quot;, team.names)
team.names &amp;lt;- gsub(&amp;quot;afc&amp;quot;, &amp;quot;AFC&amp;quot;, team.names)
team.names &amp;lt;- gsub(&amp;quot;a.f.c&amp;quot;, &amp;quot;AFC&amp;quot;, team.names)
# Dashes and slashes need to  removed.
team.names &amp;lt;- gsub(&amp;quot;-&amp;quot;, &amp;quot; &amp;quot;, team.names)
team.names &amp;lt;- gsub(&amp;quot;/&amp;quot;, &amp;quot;&amp;quot;, team.names)
# Fix FC and AFC for Bournemouth
simpleCap &amp;lt;- function(x) {
  s &amp;lt;- strsplit(x, &amp;quot; &amp;quot;)[[1]]
  paste(toupper(substring(s, 1,1)), substring(s, 2), sep=&amp;quot;&amp;quot;, collapse=&amp;quot; &amp;quot;)
  }
# Capitalise and trim white space
team.names &amp;lt;- sapply(team.names, simpleCap)
#team.names &amp;lt;- sapply(team.names, trimws)
names(team.names) &amp;lt;- NULL
# Now I have a vector of 20 names.
short.names &amp;lt;- gsub(&amp;quot; FC&amp;quot;,&amp;quot;&amp;quot;, team.names)
short.names &amp;lt;- gsub(&amp;quot; AFC&amp;quot;,&amp;quot;&amp;quot;, short.names)
EPL.names &amp;lt;- data.frame(team.names,short.names,team.URL)
EPL.names&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                            team.names
## 1             Https:www.spotrac.comeplAFC Bournemouth
## 2                  Https:www.spotrac.comeplarsenal FC
## 3        Https:www.spotrac.comeplbrighton Hove Albion
## 4                  Https:www.spotrac.comeplburnley FC
## 5             Https:www.spotrac.comeplcardiff City FC
## 6                  Https:www.spotrac.comeplchelsea FC
## 7              Https:www.spotrac.comeplcrystal Palace
## 8                  Https:www.spotrac.comepleverton FC
## 9                   Https:www.spotrac.comeplfulham FC
## 10          Https:www.spotrac.comeplhuddersfield Town
## 11             Https:www.spotrac.comeplleicester City
## 12               Https:www.spotrac.comeplliverpool FC
## 13         Https:www.spotrac.comeplmanchester City FC
## 14       Https:www.spotrac.comeplmanchester United FC
## 15        Https:www.spotrac.comeplnewcastle United FC
## 16             Https:www.spotrac.comeplsouthampton FC
## 17       Https:www.spotrac.comepltottenham Hotspur FC
## 18                    Https:www.spotrac.comeplwatford
## 19         Https:www.spotrac.comeplwest Ham United FC
## 20 Https:www.spotrac.comeplwolverhampton Wanderers FC
##                                        short.names
## 1          Https:www.spotrac.comeplAFC Bournemouth
## 2                  Https:www.spotrac.comeplarsenal
## 3     Https:www.spotrac.comeplbrighton Hove Albion
## 4                  Https:www.spotrac.comeplburnley
## 5             Https:www.spotrac.comeplcardiff City
## 6                  Https:www.spotrac.comeplchelsea
## 7           Https:www.spotrac.comeplcrystal Palace
## 8                  Https:www.spotrac.comepleverton
## 9                   Https:www.spotrac.comeplfulham
## 10       Https:www.spotrac.comeplhuddersfield Town
## 11          Https:www.spotrac.comeplleicester City
## 12               Https:www.spotrac.comeplliverpool
## 13         Https:www.spotrac.comeplmanchester City
## 14       Https:www.spotrac.comeplmanchester United
## 15        Https:www.spotrac.comeplnewcastle United
## 16             Https:www.spotrac.comeplsouthampton
## 17       Https:www.spotrac.comepltottenham Hotspur
## 18                 Https:www.spotrac.comeplwatford
## 19         Https:www.spotrac.comeplwest Ham United
## 20 Https:www.spotrac.comeplwolverhampton Wanderers
##                                                    team.URL
## 1              https://www.spotrac.com/epl/afc-bournemouth/
## 2                  https://www.spotrac.com/epl/arsenal-f.c/
## 3         https://www.spotrac.com/epl/brighton-hove-albion/
## 4                  https://www.spotrac.com/epl/burnley-f.c/
## 5             https://www.spotrac.com/epl/cardiff-city-f.c/
## 6                  https://www.spotrac.com/epl/chelsea-f.c/
## 7               https://www.spotrac.com/epl/crystal-palace/
## 8                  https://www.spotrac.com/epl/everton-f.c/
## 9                   https://www.spotrac.com/epl/fulham-f.c/
## 10           https://www.spotrac.com/epl/huddersfield-town/
## 11              https://www.spotrac.com/epl/leicester-city/
## 12               https://www.spotrac.com/epl/liverpool-f.c/
## 13         https://www.spotrac.com/epl/manchester-city-f.c/
## 14       https://www.spotrac.com/epl/manchester-united-f.c/
## 15        https://www.spotrac.com/epl/newcastle-united-f.c/
## 16             https://www.spotrac.com/epl/southampton-f.c/
## 17       https://www.spotrac.com/epl/tottenham-hotspur-f.c/
## 18                     https://www.spotrac.com/epl/watford/
## 19         https://www.spotrac.com/epl/west-ham-united-f.c/
## 20 https://www.spotrac.com/epl/wolverhampton-wanderers-f.c/&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With clean names, I can take each of the scraping tasks in order.&lt;/p&gt;
&lt;div id=&#34;payroll-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Payroll Data&lt;/h2&gt;
&lt;p&gt;The teams have payroll information that is broken down into active players, reserves, and loanees. The workflow is first to create the relevant URLs to scrape the payroll data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;team_links &amp;lt;- paste0(team.URL,&amp;quot;payroll/&amp;quot;,sep=&amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With URLs, I am going to set forth on the task. First, the &lt;em&gt;SelectorGadget&lt;/em&gt; and a glimpse of the documents suggests an easy solution. I want to isolate the table nodes and keep the tables. First, a function for the URLs.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data.creator &amp;lt;- function(link) {
read_html(link) %&amp;gt;% html_nodes(&amp;quot;table&amp;quot;) %&amp;gt;% html_table(header=TRUE, fill=TRUE)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now I want to apply data scraping function to the URLs. Then, I want to name the list items, assess the size of the active roster, and then clean up the relevant data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;EPL.salary &amp;lt;- sapply(team_links, function(x) {data.creator(x)})
names(EPL.salary) &amp;lt;- EPL.names$short.names
team.len &amp;lt;- sapply(seq(1,20), function(x) { dim(EPL.salary[[x]][[1]])[[1]]})
Team &amp;lt;- rep(EPL.names$short.names, team.len)
Players &amp;lt;- sapply(seq(1,20), function(x) { str_split(EPL.salary[[x]][[1]][,1], &amp;quot;\t&amp;quot;, simplify=TRUE)[,31]})
Position &amp;lt;- sapply(seq(1,20), function(x) { EPL.salary[[x]][[1]][,2]})
Base.Salary &amp;lt;- sapply(seq(1,20), function(x) { Res &amp;lt;- gsub(&amp;quot;£&amp;quot;, &amp;quot;&amp;quot;, EPL.salary[[x]][[1]][,3]); gsub(&amp;quot;,&amp;quot;,&amp;quot;&amp;quot;,Res)})
EPL.Result &amp;lt;- data.frame(Players=unlist(Players), Team=Team, Position=unlist(Position), Base.Salary=unlist(Base.Salary))
EPL.Result$Base.Salary &amp;lt;- str_replace(EPL.Result$Base.Salary, &amp;quot;-&amp;quot;, NA_character_)
EPL.Result$Base.Num &amp;lt;- as.numeric(EPL.Result$Base.Salary)
EPL.Result %&amp;gt;% group_by(Position) %&amp;gt;% summarise(Mean.Base.Salary=mean(Base.Num, na.rm=TRUE),sdBS=sd(Base.Num, na.rm = TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 4 x 3
##   Position Mean.Base.Salary     sdBS
##   &amp;lt;fct&amp;gt;               &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 D                2678118. 1573524.
## 2 F                3808467. 3225632.
## 3 GK               2625278. 2442130.
## 4 M                3368859. 3077304.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;EPL.Result %&amp;gt;% group_by(Position,Team) %&amp;gt;% summarise(Mean.Base.Salary=mean(Base.Num, na.rm=TRUE),sdBS=sd(Base.Num, na.rm = TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 80 x 4
## # Groups:   Position [?]
##    Position Team                                   Mean.Base.Salary    sdBS
##    &amp;lt;fct&amp;gt;    &amp;lt;fct&amp;gt;                                             &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
##  1 D        Https:www.spotrac.comeplAFC Bournemou…         1759333.  3.38e5
##  2 D        Https:www.spotrac.comeplarsenal                3855800   1.56e6
##  3 D        Https:www.spotrac.comeplbrighton Hove…         1369333.  3.82e5
##  4 D        Https:www.spotrac.comeplburnley                1255429.  2.62e5
##  5 D        Https:www.spotrac.comeplcardiff City            728000   4.53e5
##  6 D        Https:www.spotrac.comeplchelsea                4593333.  8.72e5
##  7 D        Https:www.spotrac.comeplcrystal Palace         2801500   1.18e6
##  8 D        Https:www.spotrac.comepleverton                2124571.  1.16e6
##  9 D        Https:www.spotrac.comeplfulham                 1040000   3.68e5
## 10 D        Https:www.spotrac.comeplhuddersfield …         1092000   4.92e5
## # ... with 70 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, a little picture to describe spending on the active roster.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fplot &amp;lt;- ggplot(EPL.Result, aes(Base.Num,Team))
gpl &amp;lt;- fplot + geom_jitter(height=0.25, width=0) + facet_wrap(~Position) + labs(x=&amp;quot;Base Salary&amp;quot;)
gpl&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.data.viajes/post/2018-04-08-scraping-epl-salary-data_files/figure-html/Picture-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;contracts&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Contracts&lt;/h2&gt;
&lt;p&gt;The contracts are stored in a different URL structure that is accessible via &lt;em&gt;contracts&lt;/em&gt; in the html tree by tean. Firstm I want to paste the names together with links to explore.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;team_links &amp;lt;- paste0(team.URL,&amp;quot;contracts/&amp;quot;,sep=&amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now I have all the links that I need and can turn to processing the data. This is something of a mess. Let me first grab some data to showcase the problem. In what follows, first I will grab the HTML files.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Base.Contracts &amp;lt;- lapply(team_links, read_html)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Processing them is a bit more difficult. What does the basic table look like?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Base.Contracts[[1]] %&amp;gt;% html_nodes(&amp;quot;table&amp;quot;) %&amp;gt;% html_table(header=TRUE, fill=TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
##                  Player (29) Pos. Age
## 1         DefoeJermain Defoe    F  36
## 2       BegovicAsmir Begovic   GK  31
## 3        WilsonCallum Wilson    F  26
## 4            KingJoshua King    F  26
## 5              AkeNathan Ake    D  23
## 6        SurmanAndrew Surman    M  32
## 7           ArterHarry Arter    M  28
## 8             CookSteve Cook    D  27
## 9              IbeJordon Ibe    M  23
## 10        GoslingDan Gosling    M  28
## 11           SmithAdam Smith    D  27
## 12            CookLewis Cook    M  21
## 13      FrancisSimon Francis    D  33
## 14         FraserRyan Fraser    M  24
## 15    DanielsCharlie Daniels    D  32
## 16           SmithBrad Smith    D  24
## 17         MingsTyrone Mings    D  25
## 18        MoussetLys Mousset    F  22
## 19             PughMarc Pugh    M  31
## 20    HyndmanEmerson Hyndman    M  22
## 21     MahoneyConnor Mahoney    F  21
## 22 StanislasJunior Stanislas    M  29
## 23          BorucArtur Boruc   GK  38
## 24    RamsdaleAaron Ramsdale   GK  20
## 25       SimpsonJack Simpson    D  21
## 26         TaylorKyle Taylor    M  19
## 27        BrooksDavid Brooks    M  21
## 28      LermaJefferson Lerma    M  24
## 29            RicoDiego Rico    D  25
##                                                             Contract Terms
## 1  16140000\n\t\t\t\t\t\t\t\t\t\t\t3 yr\n\t\t\t\t\t\t\t\t\t\t\t£16,140,000
## 2  14560000\n\t\t\t\t\t\t\t\t\t\t\t4 yr\n\t\t\t\t\t\t\t\t\t\t\t£14,560,000
## 3  12480000\n\t\t\t\t\t\t\t\t\t\t\t4 yr\n\t\t\t\t\t\t\t\t\t\t\t£12,480,000
## 4  11700000\n\t\t\t\t\t\t\t\t\t\t\t5 yr\n\t\t\t\t\t\t\t\t\t\t\t£11,700,000
## 5  10400000\n\t\t\t\t\t\t\t\t\t\t\t5 yr\n\t\t\t\t\t\t\t\t\t\t\t£10,400,000
## 6    9100000\n\t\t\t\t\t\t\t\t\t\t\t5 yr\n\t\t\t\t\t\t\t\t\t\t\t£9,100,000
## 7    8320000\n\t\t\t\t\t\t\t\t\t\t\t4 yr\n\t\t\t\t\t\t\t\t\t\t\t£8,320,000
## 8    8320000\n\t\t\t\t\t\t\t\t\t\t\t4 yr\n\t\t\t\t\t\t\t\t\t\t\t£8,320,000
## 9    7904000\n\t\t\t\t\t\t\t\t\t\t\t4 yr\n\t\t\t\t\t\t\t\t\t\t\t£7,904,000
## 10   7280000\n\t\t\t\t\t\t\t\t\t\t\t4 yr\n\t\t\t\t\t\t\t\t\t\t\t£7,280,000
## 11   7280000\n\t\t\t\t\t\t\t\t\t\t\t4 yr\n\t\t\t\t\t\t\t\t\t\t\t£7,280,000
## 12   6240000\n\t\t\t\t\t\t\t\t\t\t\t4 yr\n\t\t\t\t\t\t\t\t\t\t\t£6,240,000
## 13   6240000\n\t\t\t\t\t\t\t\t\t\t\t4 yr\n\t\t\t\t\t\t\t\t\t\t\t£6,240,000
## 14   5616000\n\t\t\t\t\t\t\t\t\t\t\t4 yr\n\t\t\t\t\t\t\t\t\t\t\t£5,616,000
## 15   5460000\n\t\t\t\t\t\t\t\t\t\t\t3 yr\n\t\t\t\t\t\t\t\t\t\t\t£5,460,000
## 16   5200000\n\t\t\t\t\t\t\t\t\t\t\t4 yr\n\t\t\t\t\t\t\t\t\t\t\t£5,200,000
## 17   4784000\n\t\t\t\t\t\t\t\t\t\t\t4 yr\n\t\t\t\t\t\t\t\t\t\t\t£4,784,000
## 18   4160000\n\t\t\t\t\t\t\t\t\t\t\t4 yr\n\t\t\t\t\t\t\t\t\t\t\t£4,160,000
## 19   3900000\n\t\t\t\t\t\t\t\t\t\t\t3 yr\n\t\t\t\t\t\t\t\t\t\t\t£3,900,000
## 20   3120000\n\t\t\t\t\t\t\t\t\t\t\t4 yr\n\t\t\t\t\t\t\t\t\t\t\t£3,120,000
## 21   2496000\n\t\t\t\t\t\t\t\t\t\t\t4 yr\n\t\t\t\t\t\t\t\t\t\t\t£2,496,000
## 22   2340000\n\t\t\t\t\t\t\t\t\t\t\t3 yr\n\t\t\t\t\t\t\t\t\t\t\t£2,340,000
## 23   2080000\n\t\t\t\t\t\t\t\t\t\t\t1 yr\n\t\t\t\t\t\t\t\t\t\t\t£2,080,000
## 24      780000\n\t\t\t\t\t\t\t\t\t\t\t5 yr\n\t\t\t\t\t\t\t\t\t\t\t£780,000
## 25                  0\n\t\t\t\t\t\t\t\t\t\t\t3 yr\n\t\t\t\t\t\t\t\t\t\t\t-
## 26                  0\n\t\t\t\t\t\t\t\t\t\t\t2 yr\n\t\t\t\t\t\t\t\t\t\t\t-
## 27                  0\n\t\t\t\t\t\t\t\t\t\t\t4 yr\n\t\t\t\t\t\t\t\t\t\t\t-
## 28                  0\n\t\t\t\t\t\t\t\t\t\t\t5 yr\n\t\t\t\t\t\t\t\t\t\t\t-
## 29                  0\n\t\t\t\t\t\t\t\t\t\t\t4 yr\n\t\t\t\t\t\t\t\t\t\t\t-
##    Avg. Salary Transfer Fee Expires
## 1   £5,380,000            -    2020
## 2   £3,640,000   £9,780,000    2021
## 3   £3,120,000   £3,000,000    2022
## 4   £2,340,000            -    2021
## 5   £2,080,000  £19,380,000    2022
## 6   £1,820,000     £536,000    2020
## 7   £2,080,000            -    2021
## 8   £2,080,000     £150,000    2021
## 9   £1,976,000  £15,000,000    2020
## 10  £1,820,000            -    2021
## 11  £1,820,000            -    2021
## 12  £1,560,000   £5,950,000    2020
## 13  £1,560,000            -    2019
## 14  £1,404,000     £400,000    2020
## 15  £1,820,000     £191,000    2020
## 16  £1,300,000   £3,060,000    2020
## 17  £1,196,000   £9,610,000    2021
## 18  £1,040,000   £5,530,000    2020
## 19  £1,300,000     £100,000    2019
## 20    £780,000            -    2020
## 21    £624,000            -    2021
## 22    £780,000            -    2020
## 23  £2,080,000  £10,000,000    2019
## 24    £156,000     £799,000    2021
## 25           -            -    2020
## 26           -            -    2019
## 27           -  £10,125,000    2022
## 28           -  £25,125,000    2023
## 29           -  £10,750,000    2022&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The names and the contract year and terms are going to require parsing. I have chosen the first html that corresponds to Bournemouth; other teams are worse because loan players are in a second table. That impacts the wage bill, perhaps, depending on the arrangement in the loan, but the contract details from the player do not have that team as signatory. This has to be fixed. That is easy enough to fix, there are two embedded tables and I can select the first one. When it comes to the names, there is no easy separation for the first column; I will grab them from nodes in the html.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data.creator &amp;lt;- function(data) { 
  data %&amp;gt;% html_nodes(&amp;quot;table&amp;quot;) %&amp;gt;% html_table(header=TRUE, fill=TRUE) -&amp;gt; ret.tab
  nrowsm &amp;lt;- dim(ret.tab[[1]])[[1]]
  split.me &amp;lt;- ret.tab[[1]][,4]
  tempdf &amp;lt;- data.frame(matrix(data=gsub(&amp;quot;\t|-&amp;quot;,&amp;quot;&amp;quot;,unlist(strsplit(split.me, &amp;quot;\\n&amp;quot;))), nrow=nrowsm, byrow=TRUE))
  names(tempdf) &amp;lt;- c(&amp;quot;value&amp;quot;,&amp;quot;years&amp;quot;,&amp;quot;value.pds&amp;quot;)
  data %&amp;gt;% html_nodes(&amp;quot;.player&amp;quot;) %&amp;gt;% html_nodes(&amp;quot;a&amp;quot;) %&amp;gt;% html_text() -&amp;gt; Player.Names
  Player.Names &amp;lt;- Player.Names[c(1:nrowsm)]
  data %&amp;gt;% html_nodes(&amp;quot;.player&amp;quot;) %&amp;gt;% html_nodes(&amp;quot;a&amp;quot;) %&amp;gt;% html_attr(&amp;quot;href&amp;quot;) -&amp;gt; Player.Links
  Player.links &amp;lt;- Player.Links[c(1:nrowsm)]
  data %&amp;gt;% html_nodes(&amp;quot;.player&amp;quot;) %&amp;gt;% html_nodes(&amp;quot;span&amp;quot;) %&amp;gt;% html_text() -&amp;gt; Last.Name
  Last.Name &amp;lt;- Last.Name[c(1:nrowsm)]
  names(ret.tab[1][[1]])[c(1:2)] &amp;lt;- c(&amp;quot;Player&amp;quot;,&amp;quot;Position&amp;quot;)
#  data.frame(ret.tab[,c(5,6,7)]) 
  return(data.frame(ret.tab[1][[1]],tempdf,Player.Names,Player.links,Last.Name))
}
EPL.Contracts &amp;lt;- lapply(Base.Contracts, data.creator)
names(EPL.Contracts) &amp;lt;- EPL.names$short.names
EPL.Contracts[[1]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                       Player Position Age
## 1         DefoeJermain Defoe        F  36
## 2       BegovicAsmir Begovic       GK  31
## 3        WilsonCallum Wilson        F  26
## 4            KingJoshua King        F  26
## 5              AkeNathan Ake        D  23
## 6        SurmanAndrew Surman        M  32
## 7           ArterHarry Arter        M  28
## 8             CookSteve Cook        D  27
## 9              IbeJordon Ibe        M  23
## 10        GoslingDan Gosling        M  28
## 11           SmithAdam Smith        D  27
## 12            CookLewis Cook        M  21
## 13      FrancisSimon Francis        D  33
## 14         FraserRyan Fraser        M  24
## 15    DanielsCharlie Daniels        D  32
## 16           SmithBrad Smith        D  24
## 17         MingsTyrone Mings        D  25
## 18        MoussetLys Mousset        F  22
## 19             PughMarc Pugh        M  31
## 20    HyndmanEmerson Hyndman        M  22
## 21     MahoneyConnor Mahoney        F  21
## 22 StanislasJunior Stanislas        M  29
## 23          BorucArtur Boruc       GK  38
## 24    RamsdaleAaron Ramsdale       GK  20
## 25       SimpsonJack Simpson        D  21
## 26         TaylorKyle Taylor        M  19
## 27        BrooksDavid Brooks        M  21
## 28      LermaJefferson Lerma        M  24
## 29            RicoDiego Rico        D  25
##                                                             Contract.Terms
## 1  16140000\n\t\t\t\t\t\t\t\t\t\t\t3 yr\n\t\t\t\t\t\t\t\t\t\t\t£16,140,000
## 2  14560000\n\t\t\t\t\t\t\t\t\t\t\t4 yr\n\t\t\t\t\t\t\t\t\t\t\t£14,560,000
## 3  12480000\n\t\t\t\t\t\t\t\t\t\t\t4 yr\n\t\t\t\t\t\t\t\t\t\t\t£12,480,000
## 4  11700000\n\t\t\t\t\t\t\t\t\t\t\t5 yr\n\t\t\t\t\t\t\t\t\t\t\t£11,700,000
## 5  10400000\n\t\t\t\t\t\t\t\t\t\t\t5 yr\n\t\t\t\t\t\t\t\t\t\t\t£10,400,000
## 6    9100000\n\t\t\t\t\t\t\t\t\t\t\t5 yr\n\t\t\t\t\t\t\t\t\t\t\t£9,100,000
## 7    8320000\n\t\t\t\t\t\t\t\t\t\t\t4 yr\n\t\t\t\t\t\t\t\t\t\t\t£8,320,000
## 8    8320000\n\t\t\t\t\t\t\t\t\t\t\t4 yr\n\t\t\t\t\t\t\t\t\t\t\t£8,320,000
## 9    7904000\n\t\t\t\t\t\t\t\t\t\t\t4 yr\n\t\t\t\t\t\t\t\t\t\t\t£7,904,000
## 10   7280000\n\t\t\t\t\t\t\t\t\t\t\t4 yr\n\t\t\t\t\t\t\t\t\t\t\t£7,280,000
## 11   7280000\n\t\t\t\t\t\t\t\t\t\t\t4 yr\n\t\t\t\t\t\t\t\t\t\t\t£7,280,000
## 12   6240000\n\t\t\t\t\t\t\t\t\t\t\t4 yr\n\t\t\t\t\t\t\t\t\t\t\t£6,240,000
## 13   6240000\n\t\t\t\t\t\t\t\t\t\t\t4 yr\n\t\t\t\t\t\t\t\t\t\t\t£6,240,000
## 14   5616000\n\t\t\t\t\t\t\t\t\t\t\t4 yr\n\t\t\t\t\t\t\t\t\t\t\t£5,616,000
## 15   5460000\n\t\t\t\t\t\t\t\t\t\t\t3 yr\n\t\t\t\t\t\t\t\t\t\t\t£5,460,000
## 16   5200000\n\t\t\t\t\t\t\t\t\t\t\t4 yr\n\t\t\t\t\t\t\t\t\t\t\t£5,200,000
## 17   4784000\n\t\t\t\t\t\t\t\t\t\t\t4 yr\n\t\t\t\t\t\t\t\t\t\t\t£4,784,000
## 18   4160000\n\t\t\t\t\t\t\t\t\t\t\t4 yr\n\t\t\t\t\t\t\t\t\t\t\t£4,160,000
## 19   3900000\n\t\t\t\t\t\t\t\t\t\t\t3 yr\n\t\t\t\t\t\t\t\t\t\t\t£3,900,000
## 20   3120000\n\t\t\t\t\t\t\t\t\t\t\t4 yr\n\t\t\t\t\t\t\t\t\t\t\t£3,120,000
## 21   2496000\n\t\t\t\t\t\t\t\t\t\t\t4 yr\n\t\t\t\t\t\t\t\t\t\t\t£2,496,000
## 22   2340000\n\t\t\t\t\t\t\t\t\t\t\t3 yr\n\t\t\t\t\t\t\t\t\t\t\t£2,340,000
## 23   2080000\n\t\t\t\t\t\t\t\t\t\t\t1 yr\n\t\t\t\t\t\t\t\t\t\t\t£2,080,000
## 24      780000\n\t\t\t\t\t\t\t\t\t\t\t5 yr\n\t\t\t\t\t\t\t\t\t\t\t£780,000
## 25                  0\n\t\t\t\t\t\t\t\t\t\t\t3 yr\n\t\t\t\t\t\t\t\t\t\t\t-
## 26                  0\n\t\t\t\t\t\t\t\t\t\t\t2 yr\n\t\t\t\t\t\t\t\t\t\t\t-
## 27                  0\n\t\t\t\t\t\t\t\t\t\t\t4 yr\n\t\t\t\t\t\t\t\t\t\t\t-
## 28                  0\n\t\t\t\t\t\t\t\t\t\t\t5 yr\n\t\t\t\t\t\t\t\t\t\t\t-
## 29                  0\n\t\t\t\t\t\t\t\t\t\t\t4 yr\n\t\t\t\t\t\t\t\t\t\t\t-
##    Avg..Salary Transfer.Fee Expires    value years   value.pds
## 1   £5,380,000            -    2020 16140000  3 yr £16,140,000
## 2   £3,640,000   £9,780,000    2021 14560000  4 yr £14,560,000
## 3   £3,120,000   £3,000,000    2022 12480000  4 yr £12,480,000
## 4   £2,340,000            -    2021 11700000  5 yr £11,700,000
## 5   £2,080,000  £19,380,000    2022 10400000  5 yr £10,400,000
## 6   £1,820,000     £536,000    2020  9100000  5 yr  £9,100,000
## 7   £2,080,000            -    2021  8320000  4 yr  £8,320,000
## 8   £2,080,000     £150,000    2021  8320000  4 yr  £8,320,000
## 9   £1,976,000  £15,000,000    2020  7904000  4 yr  £7,904,000
## 10  £1,820,000            -    2021  7280000  4 yr  £7,280,000
## 11  £1,820,000            -    2021  7280000  4 yr  £7,280,000
## 12  £1,560,000   £5,950,000    2020  6240000  4 yr  £6,240,000
## 13  £1,560,000            -    2019  6240000  4 yr  £6,240,000
## 14  £1,404,000     £400,000    2020  5616000  4 yr  £5,616,000
## 15  £1,820,000     £191,000    2020  5460000  3 yr  £5,460,000
## 16  £1,300,000   £3,060,000    2020  5200000  4 yr  £5,200,000
## 17  £1,196,000   £9,610,000    2021  4784000  4 yr  £4,784,000
## 18  £1,040,000   £5,530,000    2020  4160000  4 yr  £4,160,000
## 19  £1,300,000     £100,000    2019  3900000  3 yr  £3,900,000
## 20    £780,000            -    2020  3120000  4 yr  £3,120,000
## 21    £624,000            -    2021  2496000  4 yr  £2,496,000
## 22    £780,000            -    2020  2340000  3 yr  £2,340,000
## 23  £2,080,000  £10,000,000    2019  2080000  1 yr  £2,080,000
## 24    £156,000     £799,000    2021   780000  5 yr    £780,000
## 25           -            -    2020        0  3 yr            
## 26           -            -    2019        0  2 yr            
## 27           -  £10,125,000    2022        0  4 yr            
## 28           -  £25,125,000    2023        0  5 yr            
## 29           -  £10,750,000    2022        0  4 yr            
##        Player.Names                                   Player.links
## 1     Jermain Defoe https://www.spotrac.com/redirect/player/23836/
## 2     Asmir Begovic https://www.spotrac.com/redirect/player/22625/
## 3     Callum Wilson https://www.spotrac.com/redirect/player/22694/
## 4       Joshua King https://www.spotrac.com/redirect/player/22685/
## 5        Nathan Ake https://www.spotrac.com/redirect/player/15521/
## 6     Andrew Surman https://www.spotrac.com/redirect/player/22692/
## 7       Harry Arter https://www.spotrac.com/redirect/player/22674/
## 8        Steve Cook https://www.spotrac.com/redirect/player/22677/
## 9        Jordon Ibe https://www.spotrac.com/redirect/player/22684/
## 10      Dan Gosling https://www.spotrac.com/redirect/player/22682/
## 11       Adam Smith https://www.spotrac.com/redirect/player/22689/
## 12       Lewis Cook https://www.spotrac.com/redirect/player/22676/
## 13    Simon Francis https://www.spotrac.com/redirect/player/22679/
## 14      Ryan Fraser https://www.spotrac.com/redirect/player/22680/
## 15  Charlie Daniels https://www.spotrac.com/redirect/player/22678/
## 16       Brad Smith https://www.spotrac.com/redirect/player/22690/
## 17     Tyrone Mings https://www.spotrac.com/redirect/player/22686/
## 18      Lys Mousset https://www.spotrac.com/redirect/player/22687/
## 19        Marc Pugh https://www.spotrac.com/redirect/player/22688/
## 20  Emerson Hyndman https://www.spotrac.com/redirect/player/23139/
## 21   Connor Mahoney https://www.spotrac.com/redirect/player/24030/
## 22 Junior Stanislas https://www.spotrac.com/redirect/player/22691/
## 23      Artur Boruc https://www.spotrac.com/redirect/player/22660/
## 24   Aaron Ramsdale https://www.spotrac.com/redirect/player/22661/
## 25     Jack Simpson https://www.spotrac.com/redirect/player/24102/
## 26      Kyle Taylor https://www.spotrac.com/redirect/player/27876/
## 27     David Brooks https://www.spotrac.com/redirect/player/27877/
## 28  Jefferson Lerma https://www.spotrac.com/redirect/player/27878/
## 29       Diego Rico https://www.spotrac.com/redirect/player/27879/
##    Last.Name
## 1      Defoe
## 2    Begovic
## 3     Wilson
## 4       King
## 5        Ake
## 6     Surman
## 7      Arter
## 8       Cook
## 9        Ibe
## 10   Gosling
## 11     Smith
## 12      Cook
## 13   Francis
## 14    Fraser
## 15   Daniels
## 16     Smith
## 17     Mings
## 18   Mousset
## 19      Pugh
## 20   Hyndman
## 21   Mahoney
## 22 Stanislas
## 23     Boruc
## 24  Ramsdale
## 25   Simpson
## 26    Taylor
## 27    Brooks
## 28     Lerma
## 29      Rico&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data now have some junk alongside workable versions of the variables of interest. It is worth noting that the header of the contracts data allows us to verify the size of the table as we picked it up [though I do rename them to allow the rbind to work]. This also suggests a strategy for picking up the rownames that is different than the above method that uses the dimension of the html table. Perhaps I should just gsub the header to recover the integer number of players. To tidy the data, they need to be stacked. A simple do.call and row bind will probably work.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Team.Base &amp;lt;- sapply(EPL.Contracts, dim)[1,]
Team &amp;lt;- rep(as.character(names(Team.Base)),Team.Base)
EPL.Contracts.df &amp;lt;- do.call(&amp;quot;rbind&amp;quot;,EPL.Contracts)
rownames(EPL.Contracts.df) &amp;lt;- NULL
EPL.Contracts.df$Team &amp;lt;- Team
EPL.Contracts.df$value &amp;lt;- as.numeric(as.character(EPL.Contracts.df$value))
EPL.Contracts.df %&amp;gt;% group_by(Team) %&amp;gt;% summarise(Team.Mean=mean(value, na.rm=TRUE)/1e3, Team.SD=sd(value, na.rm=TRUE)) -&amp;gt; Team.mean
pp &amp;lt;- Team.mean %&amp;gt;% arrange(Team.Mean)
pp$Team &amp;lt;- factor(pp$Team, levels = pp$Team)
pp %&amp;gt;% ggplot(aes(Team.Mean,Team, size=Team.SD)) + geom_point() + labs(x=&amp;quot;Avg. Contract (1000s)&amp;quot;) -&amp;gt; cplot
cplot&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.data.viajes/post/2018-04-08-scraping-epl-salary-data_files/figure-html/EPLT-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;EPL.Contracts.df %&amp;gt;% group_by(Team) %&amp;gt;% summarise(Age.Mean=mean(Age, na.rm=TRUE), Age.SD=sd(Age, na.rm=TRUE)) -&amp;gt; Team.mean
Team.mean %&amp;gt;% ungroup() %&amp;gt;% arrange(., Age.Mean) -&amp;gt; pp
pp$Team &amp;lt;- factor(pp$Team, levels = pp$Team)
pp %&amp;gt;% ggplot(aes(Age.Mean,Team,size=Age.SD)) + geom_point() + labs(x=&amp;quot;Age&amp;quot;) -&amp;gt; cplot
cplot&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.data.viajes/post/2018-04-08-scraping-epl-salary-data_files/figure-html/EPLT2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Scraping the NFL Salary Cap Data with Python and R</title>
      <link>https://www.data.viajes/post/scraping-the-nfl-salary-cap-data-with-python-and-r/</link>
      <pubDate>Wed, 04 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.data.viajes/post/scraping-the-nfl-salary-cap-data-with-python-and-r/</guid>
      <description>&lt;div id=&#34;the-nfl-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The NFL Data&lt;/h1&gt;
&lt;p&gt;[SporTrac](&lt;a href=&#34;http://www.sportrac.com&#34; class=&#34;uri&#34;&gt;http://www.sportrac.com&lt;/a&gt;] has a wonderful array of financial data on sports. A student going to work for the Seattle Seahawks wanted the NFL salary cap data and I also found data on the English Premier League there. Now I have a source to scrape the data from.&lt;/p&gt;
&lt;p&gt;With a source in hand, the key tool is the &lt;em&gt;SelectorGadget&lt;/em&gt;. &lt;em&gt;SelectorGadget&lt;/em&gt; is a browser add-in for Chrome that allows us to select text and identify the css or xpath selector to scrape the data. With that, it becomes easy to identify what we need. I navgiated to the website in base_url and found the team names had links to the cap data. I will use those links first. Let’s build that.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rvest)
base_url &amp;lt;- &amp;quot;http://www.spotrac.com/nfl/&amp;quot;
read.base &amp;lt;- read_html(base_url)
team.URL &amp;lt;- read.base %&amp;gt;% html_nodes(&amp;quot;.team-name&amp;quot;) %&amp;gt;% html_attr(&amp;#39;href&amp;#39;)
team.URL&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;https://www.spotrac.com/nfl/arizona-cardinals/cap/&amp;quot;   
##  [2] &amp;quot;https://www.spotrac.com/nfl/atlanta-falcons/cap/&amp;quot;     
##  [3] &amp;quot;https://www.spotrac.com/nfl/baltimore-ravens/cap/&amp;quot;    
##  [4] &amp;quot;https://www.spotrac.com/nfl/buffalo-bills/cap/&amp;quot;       
##  [5] &amp;quot;https://www.spotrac.com/nfl/carolina-panthers/cap/&amp;quot;   
##  [6] &amp;quot;https://www.spotrac.com/nfl/chicago-bears/cap/&amp;quot;       
##  [7] &amp;quot;https://www.spotrac.com/nfl/cincinnati-bengals/cap/&amp;quot;  
##  [8] &amp;quot;https://www.spotrac.com/nfl/cleveland-browns/cap/&amp;quot;    
##  [9] &amp;quot;https://www.spotrac.com/nfl/dallas-cowboys/cap/&amp;quot;      
## [10] &amp;quot;https://www.spotrac.com/nfl/denver-broncos/cap/&amp;quot;      
## [11] &amp;quot;https://www.spotrac.com/nfl/detroit-lions/cap/&amp;quot;       
## [12] &amp;quot;https://www.spotrac.com/nfl/green-bay-packers/cap/&amp;quot;   
## [13] &amp;quot;https://www.spotrac.com/nfl/houston-texans/cap/&amp;quot;      
## [14] &amp;quot;https://www.spotrac.com/nfl/indianapolis-colts/cap/&amp;quot;  
## [15] &amp;quot;https://www.spotrac.com/nfl/jacksonville-jaguars/cap/&amp;quot;
## [16] &amp;quot;https://www.spotrac.com/nfl/kansas-city-chiefs/cap/&amp;quot;  
## [17] &amp;quot;https://www.spotrac.com/nfl/los-angeles-chargers/cap/&amp;quot;
## [18] &amp;quot;https://www.spotrac.com/nfl/los-angeles-rams/cap/&amp;quot;    
## [19] &amp;quot;https://www.spotrac.com/nfl/miami-dolphins/cap/&amp;quot;      
## [20] &amp;quot;https://www.spotrac.com/nfl/minnesota-vikings/cap/&amp;quot;   
## [21] &amp;quot;https://www.spotrac.com/nfl/new-england-patriots/cap/&amp;quot;
## [22] &amp;quot;https://www.spotrac.com/nfl/new-orleans-saints/cap/&amp;quot;  
## [23] &amp;quot;https://www.spotrac.com/nfl/new-york-giants/cap/&amp;quot;     
## [24] &amp;quot;https://www.spotrac.com/nfl/new-york-jets/cap/&amp;quot;       
## [25] &amp;quot;https://www.spotrac.com/nfl/oakland-raiders/cap/&amp;quot;     
## [26] &amp;quot;https://www.spotrac.com/nfl/philadelphia-eagles/cap/&amp;quot; 
## [27] &amp;quot;https://www.spotrac.com/nfl/pittsburgh-steelers/cap/&amp;quot; 
## [28] &amp;quot;https://www.spotrac.com/nfl/san-francisco-49ers/cap/&amp;quot; 
## [29] &amp;quot;https://www.spotrac.com/nfl/seattle-seahawks/cap/&amp;quot;    
## [30] &amp;quot;https://www.spotrac.com/nfl/tampa-bay-buccaneers/cap/&amp;quot;
## [31] &amp;quot;https://www.spotrac.com/nfl/tennessee-titans/cap/&amp;quot;    
## [32] &amp;quot;https://www.spotrac.com/nfl/washington-redskins/cap/&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Clean up the URLs to get the team names by themselves.
team.names &amp;lt;- gsub(&amp;quot;/cap/&amp;quot;,&amp;quot;&amp;quot;, gsub(base_url, &amp;quot;&amp;quot;, team.URL))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With 32 team links to scrape, I can now scrape them.&lt;/p&gt;
&lt;div id=&#34;grabbing-team-tables&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Grabbing Team Tables&lt;/h2&gt;
&lt;p&gt;Now I need to explore those links. When I started this, I wanted to learn some Python along the way. I found the following &lt;a href=&#34;https://wcontractor.github.io/nfl-salary-part2.html&#34;&gt;Python code&lt;/a&gt; that worked for the task. For completeness, I leave it here.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import pandas as pd
import bs4
import re
import urllib2
#import requests
from urllib import urlopen
from bs4 import BeautifulSoup
base_url = &amp;quot;http://www.spotrac.com/nfl/&amp;quot;
def get_page(url):
    page = urlopen(base_url)
    soup = BeautifulSoup(page, &amp;#39;lxml&amp;#39;)
    file = open(&amp;quot;spotrac_urls.txt&amp;quot;, &amp;#39;w&amp;#39;)
    file.write(str(soup))
    file.close()
def get_team_table(url):
    page = urlopen(url)
    soup = BeautifulSoup(page, &amp;#39;lxml&amp;#39;)
get_page(base_url)
with open(&amp;quot;spotrac_urls.txt&amp;quot;, &amp;#39;r&amp;#39;) as file:
    for line in file:
        line = line.strip()
from bs4 import BeautifulSoup
page = open(&amp;quot;spotrac_urls.txt&amp;quot;, &amp;#39;r&amp;#39;)
soup = BeautifulSoup(page, &amp;quot;lxml&amp;quot;)
div = soup.find(&amp;quot;div&amp;quot;,&amp;quot;subnav-posts&amp;quot;)
links = div.find_all(&amp;#39;a&amp;#39;)
for link in links:
    print(link.get(&amp;#39;href&amp;#39;))
len(links)
def get_team_table(url):
    page = urlopen(url)
    soup = BeautifulSoup(page, &amp;#39;lxml&amp;#39;)
    data_rows = [row for row in soup.find(&amp;quot;table&amp;quot;, &amp;quot;datatable&amp;quot;).find_all(&amp;quot;tr&amp;quot;)]
    return data_rows
# create an empty list
team_data = []
for link in links:
    team_data.append(get_team_table(link.get(&amp;#39;href&amp;#39;)))
len(team_data)
#data_rows = [row for row in soup.find(&amp;quot;td&amp;quot;, &amp;quot;center&amp;quot;).find_all(&amp;quot;tr&amp;quot;)]
table_data = []
#soup = BeautifulSoup(team_data[0], &amp;#39;lxml&amp;#39;)
#This needs to be a nested for loop because inner items of the list are BeautifulSoup Elements
for row in team_data:
    for element in row:
        #print(type(element))
        if soup.find_all(&amp;quot;td&amp;quot;, attrs={&amp;quot;class&amp;quot;:&amp;quot; right xs-hide &amp;quot;}) is not None:
            table_data.append(element.get_text())
player_data = []
for row in table_data:
    player_data.append(row.split(&amp;quot;\n&amp;quot;))
    #print(player_data)
len(player_data)
import pandas as pd
df = pd.DataFrame(player_data)
df = df.drop(14, 1)
df = df.drop(0, 1)
df = df.drop(1, 1)
df = df.drop(df.index[[0]])
df.set_index(1, inplace=True)
print(df.shape)
df.head()
players = []
for row in team_data[0]:
    if row.get_text(&amp;quot;tr&amp;quot;) is not None:
        players.append(row) 
column_headers = [col.get_text() for col in players[0].find_all(&amp;quot;th&amp;quot;) if col.get_text()]
len(column_headers)
df.columns = column_headers
df.head()
#The header repeated itself in the data.  This didn&amp;#39;t reveal itself until the data type conversion step below
#but this fixes all occurrences of it.
rows_to_be_dropped = df.loc[df[&amp;#39;Cap Hit&amp;#39;] == &amp;#39;Cap %&amp;#39;].index
rows_to_be_kept = df.loc[df[&amp;#39;Cap Hit&amp;#39;] != &amp;#39;Cap %&amp;#39;].index
totlen = len(df)
df2a = df.drop(rows_to_be_kept)
df = df.drop(rows_to_be_dropped)
df2 = pd.Series(data=rows_to_be_dropped)
#Apply a regex to convert the &amp;#39;Cap Hit&amp;#39; column from a string to a float.  
# df[&amp;#39;Cap Hit&amp;#39;] =(df[&amp;#39;Cap Hit&amp;#39;].replace(&amp;#39;[\$,)]&amp;#39;, &amp;quot;&amp;quot;, regex=True).replace( &amp;#39;[(]&amp;#39;,&amp;#39;-&amp;#39;,   regex=True ).astype(float))
# My fix
df[&amp;#39;Cap.Hit&amp;#39;] = (df[&amp;#39;Cap Hit&amp;#39;].replace(&amp;#39;[\$,)]&amp;#39;, &amp;quot;&amp;quot;, regex=True).replace(&amp;#39;[-,)]&amp;#39;, &amp;quot;&amp;quot;, regex=True).replace(&amp;#39;\s\s.*&amp;#39;, &amp;quot;&amp;quot;, regex=True).astype(float))
df[&amp;#39;Base.Salary&amp;#39;] = (df[&amp;#39;Base Salary&amp;#39;].replace(&amp;#39;[\$,)]&amp;#39;, &amp;quot;&amp;quot;, regex=True).replace(&amp;#39;[-,)]&amp;#39;, &amp;quot;&amp;quot;, regex=True).astype(float))
df[&amp;#39;Signing.Bonus&amp;#39;] = (df[&amp;#39;Signing Bonus&amp;#39;].replace(&amp;#39;[\$,)]&amp;#39;, &amp;quot;&amp;quot;, regex=True).replace(&amp;#39;[-,)]&amp;#39;, &amp;quot;&amp;quot;, regex=True).astype(float))
df[&amp;#39;Roster.Bonus&amp;#39;] = (df[&amp;#39;Roster Bonus&amp;#39;].replace(&amp;#39;[\$,)]&amp;#39;, &amp;quot;&amp;quot;, regex=True).replace(&amp;#39;[-,)]&amp;#39;, &amp;quot;&amp;quot;, regex=True).astype(float))
df[&amp;#39;Option.Bonus&amp;#39;] = (df[&amp;#39;Option Bonus&amp;#39;].replace(&amp;#39;[\$,)]&amp;#39;, &amp;quot;&amp;quot;, regex=True).replace(&amp;#39;[-,)]&amp;#39;, &amp;quot;&amp;quot;, regex=True).astype(float))
df[&amp;#39;Workout.Bonus&amp;#39;] = (df[&amp;#39;Workout Bonus&amp;#39;].replace(&amp;#39;[\$,)]&amp;#39;, &amp;quot;&amp;quot;, regex=True).replace(&amp;#39;[-,)]&amp;#39;, &amp;quot;&amp;quot;, regex=True).astype(float))
df[&amp;#39;Restruc.Bonus&amp;#39;] = (df[&amp;#39;Restruc. Bonus&amp;#39;].replace(&amp;#39;[\$,)]&amp;#39;, &amp;quot;&amp;quot;, regex=True).replace(&amp;#39;[-,)]&amp;#39;, &amp;quot;&amp;quot;, regex=True).astype(float))
df[&amp;#39;Misc&amp;#39;] = (df[&amp;#39;Misc.&amp;#39;].replace(&amp;#39;[\$,)]&amp;#39;, &amp;quot;&amp;quot;, regex=True).replace(&amp;#39;[-,)]&amp;#39;, &amp;quot;&amp;quot;, regex=True).astype(float))
df[&amp;#39;Dead.Cap&amp;#39;] = (df[&amp;#39;Dead Cap&amp;#39;].replace(&amp;#39;\(&amp;#39;,&amp;quot;&amp;quot;,regex=True).replace(&amp;#39;\)&amp;#39;,&amp;quot;&amp;quot;,regex=True).replace(&amp;#39;[\$,)]&amp;#39;, &amp;quot;&amp;quot;, regex=True).replace(&amp;#39;[-,)]&amp;#39;, &amp;quot;&amp;quot;, regex=True).astype(float))
#Sanity check to make sure it worked.
df[&amp;#39;Cap Hit&amp;#39;].sum()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are a few wonderful things about Beautiful Soup. But it is still faster and easier for me to do things in R. Here is some R code for the same task. The R table is a messy data structure but is easier to get.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(stringr)
data.creator &amp;lt;- function(link) {
read_html(link) %&amp;gt;% html_nodes(&amp;quot;table&amp;quot;) %&amp;gt;% html_table(header=TRUE, fill=TRUE) -&amp;gt; res
names(res) &amp;lt;- c(&amp;quot;Active&amp;quot;,&amp;quot;Draft&amp;quot;,&amp;quot;Inactive&amp;quot;,&amp;quot;Team1&amp;quot;,&amp;quot;Team2&amp;quot;)
return(res)
  }
team.names &amp;lt;- gsub(&amp;quot;-&amp;quot;, &amp;quot; &amp;quot;, team.names)
simpleCap &amp;lt;- function(x) {
  s &amp;lt;- strsplit(x, &amp;quot; &amp;quot;)[[1]]
  paste(toupper(substring(s, 1,1)), substring(s, 2),
      sep=&amp;quot;&amp;quot;, collapse=&amp;quot; &amp;quot;)
}
team.names &amp;lt;- sapply(team.names, simpleCap)
NFL.scrape &amp;lt;- sapply(team.URL, function(x) {data.creator(x)})
names(NFL.scrape) &amp;lt;- team.names 
# This is a hack but it works
Actives &amp;lt;- lapply(NFL.scrape, function(x){x$Active})
rep.res &amp;lt;- sapply(seq(1,32), function(x) {dim(Actives[[x]])[[1]]})
clean.me.2 &amp;lt;- function(data) {
data %&amp;gt;% str_remove_all(&amp;quot;[\\t]&amp;quot;) %&amp;gt;% str_split(&amp;quot;\\n\\n\\n&amp;quot;) %&amp;gt;% unlist() %&amp;gt;% matrix(data=., ncol=2, byrow=TRUE) -&amp;gt; dat
return(dat)
}
clean.me &amp;lt;- function(data) {
str_remove_all(data, &amp;quot;[\\t]&amp;quot;)
}
clean.me.num &amp;lt;- function(data) {
str_remove_all(data, &amp;quot;[\\$,()\\-]&amp;quot;)
  }
Players &amp;lt;- lapply(Actives, function(x){ clean.me.2(x[,1])})
Last.Name &amp;lt;- unlist(lapply(Players, function(x) {x[,1]}))
Player.Name &amp;lt;- unlist(lapply(Players, function(x) {x[,2]}))
Team &amp;lt;- rep(names(Actives),rep.res)
Position &amp;lt;- unlist(lapply(Actives, function(x){ clean.me(x[,2])}))
Base.Salary &amp;lt;- unlist(lapply(Actives, function(x){ clean.me.num(x[,&amp;#39;Base Salary&amp;#39;])}))
Base.Salary &amp;lt;- as.numeric(Base.Salary)
Signing.Bonus &amp;lt;- unlist(lapply(Actives, function(x){ clean.me.num(x[,&amp;#39;Signing Bonus&amp;#39;])}))
Signing.Bonus &amp;lt;- as.numeric(Signing.Bonus)
Roster.Bonus &amp;lt;- unlist(lapply(Actives, function(x){ clean.me.num(x[,&amp;#39;Roster Bonus&amp;#39;])}))
Roster.Bonus &amp;lt;- as.numeric(Roster.Bonus)
Option.Bonus &amp;lt;- unlist(lapply(Actives, function(x){ clean.me.num(x[,&amp;#39;Option Bonus&amp;#39;])}))
Option.Bonus &amp;lt;- as.numeric(Option.Bonus)
Workout.Bonus &amp;lt;- unlist(lapply(Actives, function(x){ clean.me.num(x[,&amp;#39;Workout Bonus&amp;#39;])}))
Workout.Bonus &amp;lt;- as.numeric(Workout.Bonus)
Cap.Pct &amp;lt;- unlist(lapply(Actives, function(x){ x[,&amp;#39;Cap %&amp;#39;]}))
Restruc.Bonus &amp;lt;- unlist(lapply(Actives, function(x){ clean.me.num(x[,&amp;#39;Restruc. Bonus&amp;#39;])}))
Restruc.Bonus &amp;lt;- as.numeric(Restruc.Bonus)
Dead.Cap &amp;lt;- unlist(lapply(Actives, function(x){ clean.me.num(x[,&amp;#39;Dead Cap&amp;#39;])}))
Dead.Cap &amp;lt;- as.numeric(Dead.Cap)
Misc.Cap &amp;lt;- unlist(lapply(Actives, function(x){ clean.me.num(x[,&amp;#39;Misc.&amp;#39;])}))
Misc.Cap &amp;lt;- as.numeric(Misc.Cap)
Cap.Hit &amp;lt;- unlist(lapply(Actives, function(x){ clean.me.num(x[,&amp;#39;Cap Hit&amp;#39;])}))
Cap.Hit &amp;lt;- str_replace_all(Cap.Hit, pattern=&amp;quot;\\s\\s*&amp;quot;, &amp;quot;&amp;quot;)
NFL.Salary.Data &amp;lt;- data.frame(Player=Player.Name, 
                       Last.Name = Last.Name,
                       Team=Team, 
                       Position=Position, 
                       Base.Salary=Base.Salary,
                       Signing.Bonus=Signing.Bonus,
                       Roster.Bonus=Roster.Bonus,
                       Option.Bonus=Option.Bonus,
                       Workout.Bonus=Workout.Bonus,
                       Cap.Hit=Cap.Hit,
                       Restruc.Bonus=Restruc.Bonus,
                       Dead.Cap=Dead.Cap,
                       Misc.Cap=Misc.Cap,
                       Cap.Pct=Cap.Pct)
save.image(&amp;quot;~/NFL-Data.RData&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Load up a local copy of the data for now.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;load(&amp;quot;~/NFL-Data.RData&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;summaries&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summaries&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(skimr)
skim(NFL.Salary.Data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Skim summary statistics
##  n obs: 2239 
##  n variables: 14 
## 
## ── Variable type:factor ───────────────────────────────────────────────────────────────────────────────────────────────
##   variable missing complete    n n_unique
##    Cap.Hit       0     2239 2239     1043
##  Last.Name       0     2239 2239     1489
##     Player       0     2239 2239     2226
##   Position       0     2239 2239       22
##       Team       0     2239 2239       32
##                             top_counts ordered
##  480: 279, 555: 230, 630: 185, 705: 66   FALSE
##     Jon: 41, Wil: 39, Smi: 31, Joh: 30   FALSE
##         Aus: 2, Bra: 2, Bra: 2, Chr: 2   FALSE
##    WR: 289, CB: 242, DE: 179, OLB: 171   FALSE
##     New: 86, Ind: 78, New: 78, Cle: 77   FALSE
## 
## ── Variable type:numeric ──────────────────────────────────────────────────────────────────────────────────────────────
##       variable missing complete    n       mean         sd p0       p25
##    Base.Salary       0     2239 2239 1702378.82 2559999.2   0 555000   
##        Cap.Pct       0     2239 2239       1.36       2.05  0      0.31
##       Dead.Cap       0     2239 2239 2237274.35 5646281.55  0      0   
##       Misc.Cap       0     2239 2239    9149.17   81182.62  0      0   
##   Option.Bonus       0     2239 2239   13272.29  168050.78  0      0   
##  Restruc.Bonus       0     2239 2239   44915.31  341687.19  0      0   
##   Roster.Bonus       0     2239 2239  206966.64 1021892.52  0      0   
##  Signing.Bonus       0     2239 2239  449440.82  961034.55  0      0   
##  Workout.Bonus       0     2239 2239   20166.59   76589.41  0      0   
##        p50        p75        p100     hist
##  700000    1500000    22500000    ▇▁▁▁▁▁▁▁
##       0.41       1.42       18.02 ▇▁▁▁▁▁▁▁
##   50178    1330354.5  82500000    ▇▁▁▁▁▁▁▁
##       0          0     2085000    ▇▁▁▁▁▁▁▁
##       0          0     4750000    ▇▁▁▁▁▁▁▁
##       0          0     5561666    ▇▁▁▁▁▁▁▁
##       0          0    28800000    ▇▁▁▁▁▁▁▁
##    3333     397413.5  10500000    ▇▁▁▁▁▁▁▁
##       0          0     1335000    ▇▁▁▁▁▁▁▁&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;NFL.Salary.Data %&amp;gt;% group_by(Team) %&amp;gt;% summarise(Total.Base.Salary=sum(Base.Salary))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 32 x 2
##    Team               Total.Base.Salary
##    &amp;lt;fct&amp;gt;                          &amp;lt;dbl&amp;gt;
##  1 Arizona Cardinals          105629569
##  2 Atlanta Falcons            132367353
##  3 Baltimore Ravens           111906351
##  4 Buffalo Bills               94577460
##  5 Carolina Panthers          110158355
##  6 Chicago Bears               85128180
##  7 Cincinnati Bengals         134855807
##  8 Cleveland Browns           122139767
##  9 Dallas Cowboys             128347113
## 10 Denver Broncos             140672444
## # ... with 22 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;NFL.Salary.Data %&amp;gt;% group_by(Position) %&amp;gt;% skim(Base.Salary)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Skim summary statistics
##  n obs: 2239 
##  n variables: 14 
##  group variables: Position 
## 
## ── Variable type:numeric ──────────────────────────────────────────────────────────────────────────────────────────────
##  Position    variable missing complete   n       mean         sd     p0
##         C Base.Salary       0       62  62 1964378.97 2173761.05      0
##        CB Base.Salary       0      242 242 1594781.37 2433196.22      0
##        DE Base.Salary       0      179 179 2007139.09 3065728.54      0
##        DT Base.Salary       0      143 143 2014004.03 3128524.16      0
##        FB Base.Salary       0       26  26  815000     560564         0
##        FS Base.Salary       0       57  57 2641985.07 2783070.36      0
##         G Base.Salary       0      136 136 1704599.21 2115607.14      0
##       ILB Base.Salary       0      120 120 1378939.22 1901654.33      0
##         K Base.Salary       0       38  38 1383498.11  946083.47 480000
##        LB Base.Salary       0        1   1  616500         NA    616500
##        LS Base.Salary       0       34  34  761617.65  201707.56 480000
##        LT Base.Salary       0       38  38 4587732.76 4142986.8       0
##       OLB Base.Salary       0      171 171 1938283.55 2869077.82      0
##         P Base.Salary       0       37  37 1242027.03  803853.93 480000
##        QB Base.Salary       0       96  96 3408382.24 5078200.92      0
##        RB Base.Salary       0      151 151 1088425.34 1459116.95      0
##        RT Base.Salary       0       52  52 2314986.23 1962841.68      0
##         S Base.Salary       0       68  68  576320.49  123126.23      0
##        SS Base.Salary       0       53  53 1566535.64 1521405.49      0
##         T Base.Salary       0       94  94  587082.02  222252.32 480000
##        TE Base.Salary       0      152 152 1301806.18 1570488.64 480000
##        WR Base.Salary       0      289 289 1624579.17 2486576.11      0
##        p25       p50     p75     p100     hist
##  630000     862290.5 2693750  9000000 ▇▁▂▁▁▁▁▁
##  555000     630000   1329725 13500000 ▇▁▁▁▁▁▁▁
##  555000     790000   1812276 17143000 ▇▁▁▁▁▁▁▁
##  555000     630000   1760393 16985000 ▇▁▁▁▁▁▁▁
##  555000     667500    837500  2750000 ▂▇▆▁▂▁▁▁
##  762911    1426150   3250000 11287000 ▇▃▁▁▁▁▁▁
##  555000     790000   1880750 10000000 ▇▂▁▁▁▁▁▁
##  555000     705000   1049127 10000000 ▇▁▁▁▁▁▁▁
##  555000     899464   2037500  3400000 ▇▁▁▁▂▁▁▁
##  616500     616500    616500   616500 ▁▁▁▇▁▁▁▁
##  573750     790000    915000  1100000 ▇▃▂▃▂▅▆▁
##  961056.25 2310342   8825000 12496000 ▇▂▁▂▁▂▂▂
##  555000     705000   1639242 14750000 ▇▁▁▁▁▁▁▁
##  555000    1000000   1500000  3000000 ▇▂▃▁▁▂▁▂
##  592500     831832   3242341 22500000 ▇▁▁▁▁▁▁▁
##  555000     630000   1000000 14544000 ▇▁▁▁▁▁▁▁
##  768750    1897621   3918750  9341000 ▇▅▂▃▁▁▁▁
##  480000     555000    630000  1037723 ▁▁▁▃▇▁▁▁
##  630000     875000   1750000  6800000 ▇▃▁▁▁▁▁▁
##  480000     555000    630000  2312212 ▇▁▁▁▁▁▁▁
##  555000     630000   1048771  8250000 ▇▁▁▁▁▁▁▁
##  555000     630000   1200000 15982000 ▇▁▁▁▁▁▁▁&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now I have my salary data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gplots::plotmeans(Base.Salary~Position, data=NFL.Salary.Data, n.label=FALSE, cex=0.6)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.data.viajes/post/2018-04-04-scraping-the-nfl-salary-cap-data-with-python-and-r_files/figure-html/Plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;boxplot(Base.Salary~Position, data=NFL.Salary.Data)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.data.viajes/post/2018-04-04-scraping-the-nfl-salary-cap-data-with-python-and-r_files/figure-html/Plot2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;combining-and-categorizing-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Combining and Categorizing Data&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;NFL.Salary.Data$Pos.Char &amp;lt;- NFL.Salary.Data$SideOfBall &amp;lt;- NFL.SubTeam &amp;lt;- as.character(NFL.Salary.Data$Position)
O.Line &amp;lt;- c(&amp;quot;LT&amp;quot;,&amp;quot;RT&amp;quot;,&amp;quot;C&amp;quot;,&amp;quot;G&amp;quot;,&amp;quot;T&amp;quot;)
D.Line &amp;lt;- c(&amp;quot;DT&amp;quot;,&amp;quot;DE&amp;quot;)
LineBackers &amp;lt;- c(&amp;quot;LB&amp;quot;,&amp;quot;ILB&amp;quot;,&amp;quot;OLB&amp;quot;)
Safeties &amp;lt;- c(&amp;quot;S&amp;quot;,&amp;quot;SS&amp;quot;,&amp;quot;FS&amp;quot;)
CBs &amp;lt;- c(&amp;quot;CB&amp;quot;)
Special.Teams &amp;lt;- c(&amp;quot;K&amp;quot;,&amp;quot;P&amp;quot;,&amp;quot;LS&amp;quot;)
RBs &amp;lt;- c(&amp;quot;RB&amp;quot;,&amp;quot;FB&amp;quot;)
TEs &amp;lt;- c(&amp;quot;TE&amp;quot;)
WRs &amp;lt;- c(&amp;quot;WR&amp;quot;)
QBs &amp;lt;- c(&amp;quot;QB&amp;quot;)
Offense &amp;lt;- c(QBs,RBs,WRs,TEs,O.Line)
Defense &amp;lt;- c(D.Line,LineBackers,Safeties,CBs)
NFL.Salary.Data$SideOfBall[NFL.Salary.Data$Pos.Char %in% Offense] &amp;lt;- &amp;quot;Offense&amp;quot;
NFL.Salary.Data$SideOfBall[NFL.Salary.Data$Pos.Char %in% Special.Teams] &amp;lt;- &amp;quot;Special Teams&amp;quot;
NFL.Salary.Data$SideOfBall[NFL.Salary.Data$Pos.Char %in% Defense] &amp;lt;- &amp;quot;Defense&amp;quot;
NFL.Salary.Data$SubTeam[NFL.Salary.Data$Pos.Char %in% Special.Teams] &amp;lt;- &amp;quot;Special Teams&amp;quot;
NFL.Salary.Data$SubTeam[NFL.Salary.Data$Pos.Char %in% QBs] &amp;lt;- &amp;quot;Quarterbacks&amp;quot;
NFL.Salary.Data$SubTeam[NFL.Salary.Data$Pos.Char %in% RBs] &amp;lt;- &amp;quot;Running Backs&amp;quot;
NFL.Salary.Data$SubTeam[NFL.Salary.Data$Pos.Char %in% TEs] &amp;lt;- &amp;quot;Tight Ends&amp;quot;
NFL.Salary.Data$SubTeam[NFL.Salary.Data$Pos.Char %in% WRs] &amp;lt;- &amp;quot;Wide Receivers&amp;quot;
NFL.Salary.Data$SubTeam[NFL.Salary.Data$Pos.Char %in% O.Line] &amp;lt;- &amp;quot;Offensive Line&amp;quot;
NFL.Salary.Data$SubTeam[NFL.Salary.Data$Pos.Char %in% LineBackers] &amp;lt;- &amp;quot;Linebackers&amp;quot;
NFL.Salary.Data$SubTeam[NFL.Salary.Data$Pos.Char %in% CBs] &amp;lt;- &amp;quot;Cornerbacks&amp;quot;
NFL.Salary.Data$SubTeam[NFL.Salary.Data$Pos.Char %in% D.Line] &amp;lt;- &amp;quot;Defensive Line&amp;quot;
NFL.Salary.Data$SubTeam[NFL.Salary.Data$Pos.Char %in% Safeties] &amp;lt;- &amp;quot;Safeties&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That gives some positional details to the data also.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>tidyTuesday - Tuition</title>
      <link>https://www.data.viajes/post/tidytuesday-tuition/</link>
      <pubDate>Tue, 03 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.data.viajes/post/tidytuesday-tuition/</guid>
      <description>&lt;p&gt;I found a great example on tidyTuesday that I wanted to work on. &lt;code&gt;@JakeKaupp&lt;/code&gt; tweeted his &lt;code&gt;#tidyTuesday&lt;/code&gt;: a very cool slope plot of tuition changes averaged by state over the last decade. It is a very informative graphic. The only tweak is a simple embedded line plot that uses color in a creative way to show growth rates. All of the R code for this is on &lt;a href=&#34;https://github.com/jkaupp&#34;&gt;Jake Kaupp’s GitHub&lt;/a&gt;. The specific file is &lt;a href=&#34;https://raw.githubusercontent.com/jkaupp/tidyweek/master/R/plot.R&#34;&gt;here&lt;/a&gt;. I did not add much. All I wanted was some idea of how the growth rates correspond over the period. I added a cumulative growth rate and encoded the color scheme with it enabling me to play with &lt;em&gt;viridis&lt;/em&gt;. The &lt;em&gt;col_tab&lt;/em&gt; is my creation. I also moved around the labels so that everyone gets a color coded label and we can see who is where, more or less. Enjoy!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(here)
library(readxl)
library(tidyverse)
library(glue)
library(ggrepel)
library(viridis)
tidy_data &amp;lt;-  read_excel(paste0(here(),&amp;quot;/data/tuition/us_avg_tuition.xlsx&amp;quot;)) %&amp;gt;%
  gather(year, avg_tuition, -State) %&amp;gt;%
  rename(state = State)

nat_avg &amp;lt;- tidy_data %&amp;gt;%
  filter(year %in% c(&amp;quot;2005-06&amp;quot;, &amp;quot;2015-16&amp;quot;)) %&amp;gt;%
  group_by(year) %&amp;gt;%
  summarize(avg_tuition = mean(avg_tuition)) %&amp;gt;%
  mutate(state = &amp;quot;National Average&amp;quot;)


plot_data &amp;lt;- tidy_data %&amp;gt;%
  filter(year %in% c(&amp;quot;2005-06&amp;quot;, &amp;quot;2015-16&amp;quot;)) %&amp;gt;%
  left_join(select(nat_avg, year, nat_avg = avg_tuition), by = &amp;quot;year&amp;quot;) %&amp;gt;%
  bind_rows(nat_avg)

labels &amp;lt;- plot_data %&amp;gt;%
  group_by(state) %&amp;gt;%
  filter(all(avg_tuition &amp;gt; nat_avg)) %&amp;gt;%
  pull(state) %&amp;gt;%
  unique()

plot &amp;lt;- plot_data %&amp;gt;%
  ggplot(., aes(x = year, y = avg_tuition, group = state)) +
  geom_text_repel(data = filter(plot_data, state %in% labels, year == &amp;quot;2015-16&amp;quot;), aes(label = state), direction = &amp;quot;y&amp;quot;, nudge_x = 0.1, segment.size = 0.1, hjust = 0, family = &amp;quot;Oxygen&amp;quot;, size = 3) +
  geom_path(color = &amp;quot;grey50&amp;quot;, size = 0.5, alpha = 0.5) +
  geom_point(color = &amp;quot;grey50&amp;quot;) +
  geom_path(data = nat_avg, color = &amp;quot;red&amp;quot;, size = 1) +
  geom_point(data = nat_avg, color = &amp;quot;red&amp;quot;) +
  scale_y_continuous(labels = scales::dollar) +
  labs(x = NULL, y = NULL, title = &amp;quot;Comparison of the average US tuition growth between 2005 and 2015&amp;quot;, subtitle = &amp;quot;Eastern and Northeastern students consistently face tutition above the national average, indicated by the red line.&amp;quot;, caption = &amp;quot;\nData: http://trends.collegeboard.org/ | Graphic: @jakekaupp&amp;quot;) +
  theme_minimal(base_family = &amp;quot;Oswald-Light&amp;quot;) +
  theme(panel.grid.minor = element_blank())
print(plot)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.data.viajes/post/2018-04-03-tidytuesday-tuition_files/figure-html/BorrowedCode-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now I will modify the original that is kept intact above.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nat_avg &amp;lt;- tidy_data %&amp;gt;%
  filter(year %in% c(&amp;quot;2005-06&amp;quot;, &amp;quot;2015-16&amp;quot;)) %&amp;gt;%
  group_by(year) %&amp;gt;%
  summarize(avg_tuition = mean(avg_tuition)) %&amp;gt;%
  mutate(state = &amp;quot;National Average&amp;quot;)


plot_data &amp;lt;- tidy_data %&amp;gt;%
  filter(year %in% c(&amp;quot;2005-06&amp;quot;, &amp;quot;2015-16&amp;quot;)) %&amp;gt;%
  left_join(select(nat_avg, year, nat_avg = avg_tuition), by = &amp;quot;year&amp;quot;) %&amp;gt;%
  bind_rows(nat_avg)

col_tab &amp;lt;- plot_data %&amp;gt;%
    filter(year %in% c(&amp;quot;2005-06&amp;quot;, &amp;quot;2015-16&amp;quot;)) %&amp;gt;%
    group_by(state) %&amp;gt;% 
    mutate(Cum.Growth.Rate = ((avg_tuition - lag(avg_tuition))/lag(avg_tuition)))
col_tab &amp;lt;- col_tab %&amp;gt;% drop_na()
# Join Up the color table
plot_data &amp;lt;- plot_data %&amp;gt;% left_join(select(col_tab, state, Cum.Growth.Rate), by=&amp;quot;state&amp;quot;)
plot_data &amp;lt;- plot_data %&amp;gt;% 
              arrange(avg_tuition)

labels &amp;lt;- plot_data %&amp;gt;%
      pull(state) %&amp;gt;%
      unique()

my.plot &amp;lt;- plot_data %&amp;gt;%
  ggplot(., aes(x = year, y = avg_tuition, group = state, colour=Cum.Growth.Rate)) +
  geom_path(size = 0.5, alpha = 0.5) + 
  geom_point() + scale_color_viridis(name=&amp;quot;Growth&amp;quot;) +
  geom_path(data = nat_avg, color = &amp;quot;red&amp;quot;, size = 1) +
  geom_point(data = nat_avg, color = &amp;quot;red&amp;quot;) +
  scale_y_continuous(labels = scales::dollar) + 
  geom_text_repel(data = filter(plot_data, state %in% labels, year == &amp;quot;2015-16&amp;quot;), aes(label = state), direction = &amp;quot;both&amp;quot;, nudge_x = 0.4, nudge_y = 1, segment.size = 0.1, family = &amp;quot;Oxygen&amp;quot;, size = 2) +
  labs(x = NULL, y = NULL, title = &amp;quot;Comparison of the average US tuition growth between 2005 and 2015&amp;quot;, subtitle = &amp;quot;Eastern and Northeastern students consistently face tutition above the national average [red line].&amp;quot;, caption = &amp;quot;\nData: http://trends.collegeboard.org/ | Graphic: @jakekaupp | Tweak color:@PieRatio&amp;quot;, cex=0.7) +
  theme_minimal(base_family = &amp;quot;Oswald-Light&amp;quot;) +
  theme(panel.grid.minor = element_blank())
ggsave(my.plot, filename = glue(&amp;#39;tidyweek-{Sys.Date()}.png&amp;#39;), height = 8, width = 8, dpi = 300)
print(my.plot)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.data.viajes/post/2018-04-03-tidytuesday-tuition_files/figure-html/ModPlot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Pew Data on Bond Ratings and Rainy Day Funds</title>
      <link>https://www.data.viajes/post/pew-data-on-bond-ratings-and-rainy-day-funds/</link>
      <pubDate>Wed, 07 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.data.viajes/post/pew-data-on-bond-ratings-and-rainy-day-funds/</guid>
      <description>&lt;div id=&#34;pew-on-rainy-day-funds-and-credit-quality&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Pew on Rainy Day Funds and Credit Quality&lt;/h1&gt;
&lt;p&gt;The Pew Charitable Trusts released a report last May (2017) that portrays rainy day funds that are well designed and deployed as a form of insurance against ratings downgrades. One the one hand, this is perfectly sensible because the alternatives do not sound like very good ideas. A poorly designed rainy day fund, for example, is going to have to fall short on either the rainy day or the fund. A poorly deployed savings device for cash flow management over the not-so-short term also seems unlikely to bolster market confidence in the repayment abilities of an issuer. If this very simple perspective that seems plausible is true, then a simple replication should be easy. And it is. Pew gladly shared the data and code. If one has access to Stata, the study is easy to replicate.&lt;/p&gt;
&lt;p&gt;Taken from the website above:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;../../img/PewCTRecs.png&#34; alt=&#34;Pew Recommendations&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Pew Recommendations&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;on-the-other-hand&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;On the other hand&lt;/h2&gt;
&lt;p&gt;The variation in the data may leave a good bit to be desired. Let’s have a look at some basic features of the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(haven)
library(dplyr)
Pew.Data &amp;lt;- read_dta(&amp;quot;~/Desktop/Pew/modeledforprediction.dta&amp;quot;)
glimpse(Pew.Data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 966
## Variables: 45
## $ fyear            &amp;lt;dbl&amp;gt; 1994, 1995, 1996, 1997, 1998, 1999, 2000, 200...
## $ statefips        &amp;lt;dbl&amp;gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
## $ state            &amp;lt;chr&amp;gt; &amp;quot;Alabama&amp;quot;, &amp;quot;Alabama&amp;quot;, &amp;quot;Alabama&amp;quot;, &amp;quot;Alabama&amp;quot;, &amp;quot;...
## $ bbalance         &amp;lt;dbl&amp;gt; 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, ...
## $ withdraw         &amp;lt;dbl&amp;gt; 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.000...
## $ deposit          &amp;lt;dbl&amp;gt; 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.0...
## $ interest         &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...
## $ ebalance         &amp;lt;dbl&amp;gt; 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, ...
## $ fund             &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...
## $ spo              &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0,...
## $ moodyo           &amp;lt;dbl&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 0, 0,...
## $ fitcho           &amp;lt;dbl&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N...
## $ spnum            &amp;lt;dbl&amp;gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...
## $ moodynum         &amp;lt;dbl&amp;gt; 6, 6, 6, 6, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, ...
## $ fitchnum         &amp;lt;dbl&amp;gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...
## $ surplus          &amp;lt;dbl&amp;gt; -3, -73, 4, -35, 27, 21, 30, -34, -47, -177, ...
## $ gfebal           &amp;lt;dbl&amp;gt; 128, 54, 58, 23, 51, 72, 101, 67, 19, 113, 34...
## $ longdebt         &amp;lt;dbl&amp;gt; 0.10800536, 0.09275389, 0.08864151, 0.0814541...
## $ shortdebt        &amp;lt;dbl&amp;gt; 2.790155e-05, 8.648518e-05, 7.688679e-06, 4.9...
## $ totaldebt        &amp;lt;dbl&amp;gt; 0.10803326, 0.09284038, 0.08864920, 0.0814590...
## $ population       &amp;lt;dbl&amp;gt; 8.346216, 8.357078, 8.365626, 8.373577, 8.382...
## $ pop65            &amp;lt;dbl&amp;gt; 548.045, 554.718, 561.331, 567.094, 571.722, ...
## $ gfe              &amp;lt;dbl&amp;gt; 3860, 4151, 4240, 4475, 4688, 4919, 5215, 521...
## $ gfr              &amp;lt;dbl&amp;gt; 3857, 4078, 4244, 4440, 4715, 4940, 5245, 517...
## $ spi              &amp;lt;dbl&amp;gt; 79368.02, 84194.33, 88048.51, 92206.95, 98699...
## $ rdfbal           &amp;lt;dbl&amp;gt; 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0...
## $ rdfdep           &amp;lt;dbl&amp;gt; 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, ...
## $ rdfwit           &amp;lt;dbl&amp;gt; 0.000000, 0.000000, 0.000000, 0.000000, 0.000...
## $ gfebalgfe        &amp;lt;dbl&amp;gt; 3.3160622, 1.3008914, 1.3679246, 0.5139665, 1...
## $ spratingshift    &amp;lt;dbl&amp;gt; NA, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ spupgrade        &amp;lt;dbl&amp;gt; NA, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ spdowngrade      &amp;lt;dbl&amp;gt; NA, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ moodyratingshift &amp;lt;dbl&amp;gt; NA, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0...
## $ moodyupgrade     &amp;lt;dbl&amp;gt; NA, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,...
## $ moodydowngrade   &amp;lt;dbl&amp;gt; NA, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ fitchratingshift &amp;lt;dbl&amp;gt; NA, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ fitchupgrade     &amp;lt;dbl&amp;gt; NA, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ fitchdowngrade   &amp;lt;dbl&amp;gt; NA, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ rdfnet           &amp;lt;dbl&amp;gt; 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.000...
## $ rdfnetgfe        &amp;lt;dbl&amp;gt; 0.000000, 0.000000, 0.000000, 0.000000, 0.000...
## $ revenuebw        &amp;lt;dbl&amp;gt; 4.613832, 15.427544, -32.826214, -55.708824, ...
## $ revenuebwtrend   &amp;lt;dbl&amp;gt; 3852.386, 4062.573, 4276.826, 4495.709, 4709....
## $ trendstanding    &amp;lt;dbl&amp;gt; 1, 1, -1, -1, 1, 1, 1, 1, -1, -1, -1, -1, 1, ...
## $ valenceusage     &amp;lt;dbl&amp;gt; 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.000...
## $ valenceusagegfe  &amp;lt;dbl&amp;gt; 0.000000, 0.000000, 0.000000, 0.000000, 0.000...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(Pew.Data$state)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##        Alabama         Alaska        Arizona       Arkansas     California 
##             21             21             21             21             21 
##    Connecticut       Delaware        Florida        Georgia         Hawaii 
##             21             21             21             21             21 
##          Idaho        Indiana           Iowa       Kentucky      Louisiana 
##             21             21             21             21             21 
##          Maine       Maryland  Massachusetts       Michigan      Minnesota 
##             21             21             21             21             21 
##    Mississippi       Missouri       Nebraska         Nevada  New Hampshire 
##             21             21             21             21             21 
##     New Jersey     New Mexico       New York North Carolina   North Dakota 
##             21             21             21             21             21 
##           Ohio       Oklahoma         Oregon   Pennsylvania   Rhode Island 
##             21             21             21             21             21 
## South Carolina   South Dakota      Tennessee          Texas           Utah 
##             21             21             21             21             21 
##        Vermont       Virginia     Washington  West Virginia      Wisconsin 
##             21             21             21             21             21 
##        Wyoming 
##             21&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The panel is balanced; in the original, New Mexico, New York, South Carolina, and Vermont are duplicated but the Stata code writes out a transformed dataset for analysis that is recorded. The technical report accompanying the study and the stata code give us some insights. In all cases, there are two or more RDF’s and they require combining.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;combining-ratings&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Combining Ratings&lt;/h1&gt;
&lt;p&gt;In previous work, Skip Krueger and I have treated bond ratings as a multiple rater problem and have deployed cumulative IRT models to measure latent credit quality. One of the methodologically desireable approaches to the Pew study was a model deploying state-level fixed effects but the ordinal data precludes doing this reliably because states that have always experienced the highest rating will have unbounded fixed effects. The continuous latent scale post measurement allows us to sidestep that problem. First, let me scale the data&lt;/p&gt;
&lt;div id=&#34;scaling-the-ratings&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Scaling the Ratings&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(MCMCpack)
Scaled.BR &amp;lt;- MCMCordfactanal(~spnum+fitchnum+moodynum, data=Pew.Data, factors=1, burnin = 1e6, mcmc=1e6, thin=100, store.scores=TRUE, tune=0.7, lambda.constraints=list(fitchnum=list(2,&amp;quot;+&amp;quot;)), verbose=50000)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
load(&amp;quot;~/Desktop/Pew/Scaled-BR-Pew.RData&amp;quot;)
BR.Data &amp;lt;- t(Scaled.BR[,c(26:991)])
BR.Parms &amp;lt;- Scaled.BR[,c(1:25)]
state.ratings &amp;lt;- data.frame(state=Pew.Data$state, statefips=Pew.Data$statefips, year=Pew.Data$fyear, BR.Data)
state.ratings.long &amp;lt;- tidyr::gather(state.ratings, sampleno, value, -statefips, -year, -state)
state.SE &amp;lt;- state.ratings.long %&amp;gt;% group_by(state,year) %&amp;gt;% summarise(Credit.Quality=mean(value), t1=quantile(value, probs=0.025), t2=quantile(value, probs=0.975))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;what-does-the-scaling-look-like&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What does the scaling look like?&lt;/h2&gt;
&lt;div id=&#34;the-first-group&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The First Group&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stored &amp;lt;- list()
stored &amp;lt;- state.SE %&amp;gt;% group_by(state) %&amp;gt;% filter(state%in%c(names(table(state.SE$state))[c(1:16)])) %&amp;gt;%
  ggplot(., aes(x=year, y=Credit.Quality)) + theme_minimal() + theme(axis.text.x  = element_text(angle=60)) +
    geom_ribbon(aes(ymin=t1, ymax=t2, colour=state, fill=state), alpha=0.4) + guides(fill=&amp;quot;none&amp;quot;, alpha=&amp;quot;none&amp;quot;) +
    geom_line() + guides(colour=&amp;quot;none&amp;quot;) +
#    geom_jitter(width=0.2) +
#    geom_point(shape=21, size=3, fill=&amp;quot;white&amp;quot;) +
    ylim(-4,4) + facet_wrap(~state)
stored&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.data.viajes/post/2018-03-07-pew-data-on-bond-ratings-and-rainy-day-funds_files/figure-html/Plot5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-second-group&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Second Group&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stored &amp;lt;- list()
stored &amp;lt;- state.SE %&amp;gt;% group_by(state) %&amp;gt;% filter(state%in%c(names(table(state.SE$state))[c(17:32)])) %&amp;gt;%
  ggplot(., aes(x=year, y=Credit.Quality)) + theme_minimal() + theme(axis.text.x  = element_text(angle=60)) +
    geom_ribbon(aes(ymin=t1, ymax=t2, colour=state, fill=state), alpha=0.4) + guides(fill=&amp;quot;none&amp;quot;, alpha=&amp;quot;none&amp;quot;) +
    geom_line() + guides(colour=&amp;quot;none&amp;quot;) +
#    geom_jitter(width=0.2) +
#    geom_point(shape=21, size=3, fill=&amp;quot;white&amp;quot;) +
    ylim(-4,4) + facet_wrap(~state)
stored&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.data.viajes/post/2018-03-07-pew-data-on-bond-ratings-and-rainy-day-funds_files/figure-html/Plot5b-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-third-group&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Third Group&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stored &amp;lt;- list()
stored &amp;lt;- state.SE %&amp;gt;% group_by(state) %&amp;gt;% filter(state%in%c(names(table(state.SE$state))[c(33:46)])) %&amp;gt;%
  ggplot(., aes(x=year, y=Credit.Quality)) + theme_minimal() + theme(axis.text.x  = element_text(angle=60)) +
    geom_ribbon(aes(ymin=t1, ymax=t2, colour=state, fill=state), alpha=0.4) + guides(fill=&amp;quot;none&amp;quot;, alpha=&amp;quot;none&amp;quot;) +
    geom_line() + guides(colour=&amp;quot;none&amp;quot;) +
#    geom_jitter(width=0.2) +
#    geom_point(shape=21, size=3, fill=&amp;quot;white&amp;quot;) +
    ylim(-4,4) + facet_wrap(~state)
stored&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.data.viajes/post/2018-03-07-pew-data-on-bond-ratings-and-rainy-day-funds_files/figure-html/Plot5c-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-panel-data-properties&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Panel Data Properties&lt;/h2&gt;
&lt;p&gt;Panel data estmators for linear problems benefit from a very useful decomposition from ANOVA. The total variation in a variable can be decomposed into two components: a within-unit or short-run component and a between-unit averages component (that is constant for any given unit). It is always important, as emphasised in the modelling in Mundlak (1977), to consider the variance components because they conttribute insights into the nature of inferences by telling us how much information and of what sort is contained in each indicator. The number of controls in the study is manageable so in depth analysis is possible.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;analysing-the-scaled-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Analysing the Scaled Data&lt;/h2&gt;
&lt;p&gt;With continuous measures on the response imputed over 10,000 samples, we can turn to an analysis of these samples to reexamine the dynamics of rainy day fund expenditures on bond ratings.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(haven)
nlswork &amp;lt;- read_stata(&amp;quot;http://www.stata-press.com/data/r13/nlswork.dta&amp;quot;)
library(dplyr)
XTSUM &amp;lt;- function(data, varname, unit) {
  varname &amp;lt;- enquo(varname)
  unit &amp;lt;- enquo(unit)
  ores &amp;lt;- nlswork %&amp;gt;% summarise(ovr.mean=mean(!! varname, na.rm=TRUE), ovr.sd=sd(!! varname, na.rm=TRUE), ovr.min = min(!! varname, na.rm=TRUE), ovr.max=max(!! varname, na.rm=TRUE), N.overall=sum(as.numeric(!is.na(!! varname))))
  bmeans &amp;lt;- nlswork %&amp;gt;% group_by(!!unit) %&amp;gt;% summarise(meanx=mean(!! varname, na.rm=T), t.count=sum(as.numeric(!is.na(!! varname))))
  bres &amp;lt;- bmeans %&amp;gt;% summarise(between.sd = sd(meanx, na.rm=TRUE), between.min = min(meanx, na.rm=TRUE), between.max=max(meanx, na.rm=TRUE), t.bar=mean(t.count, na.rm=TRUE), Groups=n())
  wdat &amp;lt;- nlswork %&amp;gt;% group_by(!!unit) %&amp;gt;% mutate(W.x = scale(!! varname, scale=FALSE))
  wres &amp;lt;- wdat %&amp;gt;% ungroup() %&amp;gt;% summarise(within.sd=sd(W.x, na.rm=TRUE), within.min=min(W.x, na.rm=TRUE), within.max=max(W.x, na.rm=TRUE))
  return(list(ores=ores,bres=bres,wres=wres))
}
XTSUM(nlswork, varname=hours, unit=idcode)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $ores
## # A tibble: 1 x 5
##   ovr.mean ovr.sd ovr.min ovr.max N.overall
##      &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
## 1     36.6   9.87       1     168     28467
## 
## $bres
## # A tibble: 1 x 5
##   between.sd between.min between.max t.bar Groups
##        &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;int&amp;gt;
## 1       7.85           1        83.5  6.04   4711
## 
## $wres
## # A tibble: 1 x 3
##   within.sd within.min within.max
##       &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;
## 1      7.52      -38.7       93.5&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Mapping with the Government Finance Database</title>
      <link>https://www.data.viajes/post/mapping-with-the-government-finance-database/</link>
      <pubDate>Sun, 25 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.data.viajes/post/mapping-with-the-government-finance-database/</guid>
      <description>&lt;div id=&#34;the-government-finance-database&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The Government Finance Database&lt;/h1&gt;
&lt;p&gt;Some of my colleagues (Kawika Pierson, Mike Hand, and Fred Thompson) have put together a convenient access point for the &lt;a href=&#34;http://willamette.edu/mba/research_impact/public_datasets/index.html&#34;&gt;Government Finance data&lt;/a&gt; available from the Census. They published an article in &lt;a href=&#34;http://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0130119&amp;amp;type=printable&#34;&gt;PLoS One&lt;/a&gt; with the rationale; I want to build some maps from their project with extensible code and functions. The overall dataset is enormous. I have downloaded the whole thing and filtered out the states. It seems that &lt;code&gt;read.csv&lt;/code&gt; works better than &lt;code&gt;read_csv&lt;/code&gt; for this task.&lt;/p&gt;
&lt;div id=&#34;splitting-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Splitting the Data&lt;/h2&gt;
&lt;p&gt;A link to the .zip version for states is &lt;a href=&#34;http://www.willamette.edu/~kpierson/TheGovernmentFinanceDatabase_StateData.zip&#34;&gt;here&lt;/a&gt;. A link to the .zip version for counties is &lt;a href=&#34;http://www.willamette.edu/~kpierson/TheGovernmentFinanceDatabase_CountyData.zip&#34;&gt;here&lt;/a&gt;. You have to download their data directly as a .zip and unzip. If you downbload the entire file, as I did, unzip it, and you can import it to &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; with &lt;code&gt;read.csv&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;library(dplyr)
GFD &amp;lt;- read.csv(&amp;quot;~/Documents/The Government Finance Database_All Data.csv&amp;quot;)
State.Data &amp;lt;- GFD %&amp;gt;% filter(Type_Code==0)
County.Data &amp;lt;- GFD %&amp;gt;% filter(Type_Code==1)
rm(GFD)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-basic-maps&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The basic maps&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;choroplethr&lt;/em&gt; makes the process of mapping data a bit too easy. It is locked down but it works very well. What I will require is a simple function to cleave off a particular year, the state name, and whatever series of data I wish to make; I will need to store the latter two with the name &lt;code&gt;region&lt;/code&gt; for the state name and &lt;code&gt;value&lt;/code&gt; for whatever data we wish to plot. Why? That is the way that &lt;em&gt;state_choropleth()&lt;/em&gt; is designed. For the automated functions &lt;em&gt;state_choropleth()&lt;/em&gt; and &lt;em&gt;county_choropleth()&lt;/em&gt;, this is quite easy. You will notice that there are FIPS and name inner joins available for this.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(choroplethr)
library(choroplethrMaps)
data(state.map)
names(state.map)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;long&amp;quot;       &amp;quot;lat&amp;quot;        &amp;quot;order&amp;quot;      &amp;quot;hole&amp;quot;       &amp;quot;piece&amp;quot;     
##  [6] &amp;quot;group&amp;quot;      &amp;quot;id&amp;quot;         &amp;quot;GEO_ID&amp;quot;     &amp;quot;STATE&amp;quot;      &amp;quot;region&amp;quot;    
## [11] &amp;quot;LSAD&amp;quot;       &amp;quot;CENSUSAREA&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(county.regions)
names(county.regions)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;region&amp;quot;                &amp;quot;county.fips.character&amp;quot; &amp;quot;county.name&amp;quot;          
## [4] &amp;quot;state.name&amp;quot;            &amp;quot;state.fips.character&amp;quot;  &amp;quot;state.abb&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;putting-them-together-states&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Putting them together: States&lt;/h1&gt;
&lt;p&gt;Let’s see what we can do. The first step is that the state name conventions are inconsistent, though the FIPS codes, thankfully, do not change. I will convert them to strings, to drop the unused levels of the factors from import, and replace the state delineated names with a single standard name. The second relevant selector is going to be the year, &lt;em&gt;Year4&lt;/em&gt;. Finally, I have to choose something to put in values. Let me try total revenue per capita in year 2015. &lt;em&gt;dplyr&lt;/em&gt; is so neat that this can be done in only five lines. Define and name the object; filter by the year; mutate the data into the two things I need; select them off.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(tidyverse)
State.Data$Name &amp;lt;- as.character(State.Data$Name)
State.Data$NameFixed &amp;lt;- gsub(&amp;quot; STATE GOVT&amp;quot;,&amp;quot;&amp;quot;,State.Data$Name)
State.Data$NameFLower &amp;lt;- tolower(State.Data$NameFixed)
# Clean up the names and store the cleaned result
my.GFD.2015 &amp;lt;- State.Data %&amp;gt;% 
  filter(Year4==2015) %&amp;gt;%
  mutate(value = Total_Taxes / Population, region = NameFLower) %&amp;gt;%
  select(region, value)
my.GFD.2015&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            region    value
## 1         alabama 2.007714
## 2          alaska 1.169672
## 3         arizona 2.062385
## 4        arkansas 3.085824
## 5      california 3.861881
## 6        colorado 2.347743
## 7     connecticut 4.518299
## 8        delaware 3.714758
## 9         florida 1.835985
## 10        georgia 1.930886
## 11         hawaii 4.530280
## 12          idaho 2.402183
## 13       illinois 3.174292
## 14        indiana 2.628473
## 15           iowa 2.941598
## 16         kansas 2.707783
## 17       kentucky 2.620959
## 18      louisiana 2.076066
## 19          maine 3.057240
## 20       maryland 3.329998
## 21  massachusetts 3.975644
## 22       michigan 2.716768
## 23      minnesota 4.451924
## 24    mississippi 2.641045
## 25       missouri 1.965284
## 26        montana 2.752764
## 27       nebraska 2.682621
## 28         nevada 2.605809
## 29  new hampshire 1.869624
## 30     new jersey 3.523957
## 31     new mexico 2.882076
## 32       new york 3.950608
## 33 north carolina 2.497637
## 34   north dakota 7.583087
## 35           ohio 2.436590
## 36       oklahoma 2.405160
## 37         oregon 2.624777
## 38   pennsylvania 2.842799
## 39   rhode island 3.026298
## 40 south carolina 1.967472
## 41   south dakota 1.950109
## 42      tennessee 1.923927
## 43          texas 2.005396
## 44           utah 2.237496
## 45        vermont 4.860939
## 46       virginia 2.449827
## 47     washington 2.879141
## 48  west virginia 3.018220
## 49      wisconsin 2.948908
## 50        wyoming 4.020295&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That is exactly the data that I need. Finally, add a title and continuous color scheming.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;state_choropleth(my.GFD.2015, title=&amp;quot;Total Taxes per capita&amp;quot;, num_colors=1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in self$bind(): The following regions were missing and are being
## set to NA: district of columbia&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.data.viajes/post/2018-02-25-mapping-with-the-government-finance-database_files/figure-html/MapTheData-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;That’s far too easy. Total taxes, which I have not bothered to look up, appear to contain some unexpected details. What else can be done with it? The final code chunk does it all. To calculate a new map, just change the name of the thing per capitized. What data are available? See below….&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;names(State.Data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   [1] &amp;quot;SurveyYr&amp;quot;                     &amp;quot;Year4&amp;quot;                       
##   [3] &amp;quot;YearofData&amp;quot;                   &amp;quot;ID&amp;quot;                          
##   [5] &amp;quot;IDChanged&amp;quot;                    &amp;quot;State_Code&amp;quot;                  
##   [7] &amp;quot;Type_Code&amp;quot;                    &amp;quot;County&amp;quot;                      
##   [9] &amp;quot;Name&amp;quot;                         &amp;quot;FIPS_Code_State&amp;quot;             
##  [11] &amp;quot;FIPS_County&amp;quot;                  &amp;quot;FIPS_Place&amp;quot;                  
##  [13] &amp;quot;FYEndDate&amp;quot;                    &amp;quot;YearPop&amp;quot;                     
##  [15] &amp;quot;SchLevCode&amp;quot;                   &amp;quot;Population&amp;quot;                  
##  [17] &amp;quot;FunctionCode&amp;quot;                 &amp;quot;Enrollment&amp;quot;                  
##  [19] &amp;quot;Total_Revenue&amp;quot;                &amp;quot;Total_Rev_Own_Sources&amp;quot;       
##  [21] &amp;quot;General_Revenue&amp;quot;              &amp;quot;Gen_Rev_Own_Sources&amp;quot;         
##  [23] &amp;quot;Total_Taxes&amp;quot;                  &amp;quot;Property_Tax&amp;quot;                
##  [25] &amp;quot;Tot_Sales___Gr_Rec_Tax&amp;quot;       &amp;quot;Total_Gen_Sales_Tax&amp;quot;         
##  [27] &amp;quot;Total_Select_Sales_Tax&amp;quot;       &amp;quot;Alcoholic_Beverage_Tax&amp;quot;      
##  [29] &amp;quot;Amusement_Tax&amp;quot;                &amp;quot;Insurance_Premium_Tax&amp;quot;       
##  [31] &amp;quot;Motor_Fuels_Tax&amp;quot;              &amp;quot;Pari_mutuels_Tax&amp;quot;            
##  [33] &amp;quot;Public_Utility_Tax&amp;quot;           &amp;quot;Tobacco_Tax&amp;quot;                 
##  [35] &amp;quot;Other_Select_Sales_Tax&amp;quot;       &amp;quot;Total_License_Taxes&amp;quot;         
##  [37] &amp;quot;Alcoholic_Beverage_Lic&amp;quot;       &amp;quot;Amusement_License&amp;quot;           
##  [39] &amp;quot;Corporation_License&amp;quot;          &amp;quot;Hunting___Fishing_License&amp;quot;   
##  [41] &amp;quot;Motor_Vehicle_License&amp;quot;        &amp;quot;Motor_Veh_Oper_License&amp;quot;      
##  [43] &amp;quot;Motor_Vehicle_License_Total&amp;quot;  &amp;quot;Public_Utility_License&amp;quot;      
##  [45] &amp;quot;Occup_and_Bus_Lic_NEC&amp;quot;        &amp;quot;Other_License_Taxes&amp;quot;         
##  [47] &amp;quot;Total_Income_Taxes&amp;quot;           &amp;quot;Individual_Income_Tax&amp;quot;       
##  [49] &amp;quot;Corp_Net_Income_Tax&amp;quot;          &amp;quot;Death_and_Gift_Tax&amp;quot;          
##  [51] &amp;quot;Docum_and_Stock_Tr_Tax&amp;quot;       &amp;quot;Severance_Tax&amp;quot;               
##  [53] &amp;quot;Taxes_NEC&amp;quot;                    &amp;quot;Total_IG_Revenue&amp;quot;            
##  [55] &amp;quot;Total_Fed_IG_Revenue&amp;quot;         &amp;quot;Fed_IGR_Air_Transport&amp;quot;       
##  [57] &amp;quot;Fed_IGR_Education&amp;quot;            &amp;quot;Fed_IGR_Emp_Sec_Adm&amp;quot;         
##  [59] &amp;quot;Fed_IGR_Gen_Rev_Shar&amp;quot;         &amp;quot;Fed_IGR_Gen_Support&amp;quot;         
##  [61] &amp;quot;Fed_IGR_Health___Hos&amp;quot;         &amp;quot;Fed_IGR_Highways&amp;quot;            
##  [63] &amp;quot;Fed_IGR_Transit_Sub&amp;quot;          &amp;quot;Fed_IGR_Hous_Com_Dev&amp;quot;        
##  [65] &amp;quot;Fed_IGR_Natural_Res&amp;quot;          &amp;quot;Fed_IGR_Public_Welf&amp;quot;         
##  [67] &amp;quot;Fed_IGR_Sewerage&amp;quot;             &amp;quot;Fed_IGR_Other&amp;quot;               
##  [69] &amp;quot;Total_State_IG_Revenue&amp;quot;       &amp;quot;State_IGR_Education&amp;quot;         
##  [71] &amp;quot;State_IGR_Tax_Relief&amp;quot;         &amp;quot;State_IGR_Oth_Gen_Sup&amp;quot;       
##  [73] &amp;quot;State_IGR_Gen_Sup&amp;quot;            &amp;quot;State_IGR_Health___Hos&amp;quot;      
##  [75] &amp;quot;State_IGR_Highways&amp;quot;           &amp;quot;State_IGR_Transit_Sub&amp;quot;       
##  [77] &amp;quot;State_IGR_Hous_Com_Dev&amp;quot;       &amp;quot;State_IGR_Public_Welf&amp;quot;       
##  [79] &amp;quot;State_IGR_Sewerage&amp;quot;           &amp;quot;State_IGR_Other&amp;quot;             
##  [81] &amp;quot;Tot_Local_IG_Rev&amp;quot;             &amp;quot;Local_IGR_InterSchool_Aid&amp;quot;   
##  [83] &amp;quot;Local_IGR_Other_Education&amp;quot;    &amp;quot;Local_IGR_Oth_Gen_Sup&amp;quot;       
##  [85] &amp;quot;Local_IGR_Health___Hos&amp;quot;       &amp;quot;Local_IGR_Highways&amp;quot;          
##  [87] &amp;quot;Local_IGR_Transit_Sub&amp;quot;        &amp;quot;Local_IGR_Hous_Com_Dev&amp;quot;      
##  [89] &amp;quot;Local_IGR_Public_Welf&amp;quot;        &amp;quot;Local_IGR_Sewerage&amp;quot;          
##  [91] &amp;quot;Local_IGR_Other&amp;quot;              &amp;quot;Tot_Chgs_and_Misc_Rev&amp;quot;       
##  [93] &amp;quot;Total_General_Charges&amp;quot;        &amp;quot;Chg_Air_Transportation&amp;quot;      
##  [95] &amp;quot;Chg_Misc_Com_Activ&amp;quot;           &amp;quot;Chg_Total_Education&amp;quot;         
##  [97] &amp;quot;Chg_Total_Elem_Education&amp;quot;     &amp;quot;Chg_Elem_Ed_Sch_Lunch&amp;quot;       
##  [99] &amp;quot;Chg_Elem_Ed_Tuition&amp;quot;          &amp;quot;Chg_Elem_Ed_NEC&amp;quot;             
## [101] &amp;quot;Chg_Total_High_Ed&amp;quot;            &amp;quot;Chg_Hospitals&amp;quot;               
## [103] &amp;quot;Chg_Highways&amp;quot;                 &amp;quot;Chg_Regular_Highways&amp;quot;        
## [105] &amp;quot;Chg_Toll_Highways&amp;quot;            &amp;quot;Chg_Housing___Comm_Dev&amp;quot;      
## [107] &amp;quot;Chg_Total_Nat_Res&amp;quot;            &amp;quot;Chg_Parking&amp;quot;                 
## [109] &amp;quot;Chg_Parks___Recreation&amp;quot;       &amp;quot;Chg_Sewerage&amp;quot;                
## [111] &amp;quot;Chg_Solid_Waste_Mgmt&amp;quot;         &amp;quot;Chg_Water_Transport&amp;quot;         
## [113] &amp;quot;Chg_All_Other_NEC&amp;quot;            &amp;quot;Misc_General_Revenue&amp;quot;        
## [115] &amp;quot;Special_Assessments&amp;quot;          &amp;quot;Prop_Sale_Total&amp;quot;             
## [117] &amp;quot;Prop_Sale_Hous_Com_Dev&amp;quot;       &amp;quot;Prop_Sale_Other&amp;quot;             
## [119] &amp;quot;Interest_Revenue&amp;quot;             &amp;quot;Fines_and_Forfeits&amp;quot;          
## [121] &amp;quot;Rents_and_Royalties&amp;quot;          &amp;quot;Net_Lottery_Revenue&amp;quot;         
## [123] &amp;quot;Misc_General_Rev_NEC&amp;quot;         &amp;quot;Liquor_Stores_Revenue&amp;quot;       
## [125] &amp;quot;Total_Utility_Revenue&amp;quot;        &amp;quot;Water_Utility_Revenue&amp;quot;       
## [127] &amp;quot;Electric_Utility_Rev&amp;quot;         &amp;quot;Gas_Utility_Rev&amp;quot;             
## [129] &amp;quot;Transit_Utility_Rev&amp;quot;          &amp;quot;Total_Insur_Trust_Rev&amp;quot;       
## [131] &amp;quot;Total_Insur_Trust_Ctrb&amp;quot;       &amp;quot;Tot_Ins_Trust_Inv_Rev&amp;quot;       
## [133] &amp;quot;Total_Emp_Ret_Rev&amp;quot;            &amp;quot;Emp_Ret_Total_Ctrib&amp;quot;         
## [135] &amp;quot;Emp_Ret_Loc_Emp_Ctrib&amp;quot;        &amp;quot;Emp_Ret_Loc_To_Loc_Sys&amp;quot;      
## [137] &amp;quot;Emp_Ret_From_Other_Gov&amp;quot;       &amp;quot;Emp_Ret_Sta_To_Sta_Ctr&amp;quot;      
## [139] &amp;quot;Emp_Ret_Int_Rev&amp;quot;              &amp;quot;Emp_Ret_Other_Earnings&amp;quot;      
## [141] &amp;quot;Total_Unemp_Rev&amp;quot;              &amp;quot;Unemp_Payroll_Tax&amp;quot;           
## [143] &amp;quot;Unemp_Int_Revenue&amp;quot;            &amp;quot;Unemp_Federal_Advances&amp;quot;      
## [145] &amp;quot;Total_Expenditure&amp;quot;            &amp;quot;Total_IG_Expenditure&amp;quot;        
## [147] &amp;quot;Direct_Expenditure&amp;quot;           &amp;quot;Total_Current_Expend&amp;quot;        
## [149] &amp;quot;Total_Current_Oper&amp;quot;           &amp;quot;Total_Capital_Outlays&amp;quot;       
## [151] &amp;quot;Total_Construction&amp;quot;           &amp;quot;Total_Other_Capital_Outlays&amp;quot; 
## [153] &amp;quot;Tot_Assist___Subsidies&amp;quot;       &amp;quot;Total_Interest_on_Debt&amp;quot;      
## [155] &amp;quot;Total_Insur_Trust_Ben&amp;quot;        &amp;quot;Total_Salaries___Wages&amp;quot;      
## [157] &amp;quot;General_Expenditure&amp;quot;          &amp;quot;IG_Exp_To_State_Govt&amp;quot;        
## [159] &amp;quot;IG_Exp_To_Local_Govts&amp;quot;        &amp;quot;IG_Exp_To_Federal_Govt&amp;quot;      
## [161] &amp;quot;Direct_General_Expend&amp;quot;        &amp;quot;General_Current_Expend&amp;quot;      
## [163] &amp;quot;General_Current_Oper&amp;quot;         &amp;quot;General_Capital_Outlay&amp;quot;      
## [165] &amp;quot;General_Construction&amp;quot;         &amp;quot;General_Capital_Outlay_Other&amp;quot;
## [167] &amp;quot;General_Assist___Sub&amp;quot;         &amp;quot;General_Debt_Interest&amp;quot;       
## [169] &amp;quot;Air_Trans_Total_Expend&amp;quot;       &amp;quot;Air_Trans_Direct_Expend&amp;quot;     
## [171] &amp;quot;Air_Trans_Cap_Outlay&amp;quot;         &amp;quot;Air_Trans_Current_Exp&amp;quot;       
## [173] &amp;quot;Air_Trans_Construction&amp;quot;       &amp;quot;Air_Trans_IG_To_State&amp;quot;       
## [175] &amp;quot;Air_Trans_IG_Local_Govts&amp;quot;     &amp;quot;Misc_Com_Activ_Tot_Exp&amp;quot;      
## [177] &amp;quot;Misc_Com_Activ_Cap_Out&amp;quot;       &amp;quot;Misc_Com_Activ_Current_Exp&amp;quot;  
## [179] &amp;quot;Misc_Com_Activ_Constr&amp;quot;        &amp;quot;Correct_Total_Exp&amp;quot;           
## [181] &amp;quot;Correct_Direct_Exp&amp;quot;           &amp;quot;Correct_Cap_Outlay&amp;quot;          
## [183] &amp;quot;Correct_Current_Exp&amp;quot;          &amp;quot;Correct_Construct&amp;quot;           
## [185] &amp;quot;Correct_IG_To_St&amp;quot;             &amp;quot;Correct_IG_Loc_Govts&amp;quot;        
## [187] &amp;quot;Total_Educ_Total_Exp&amp;quot;         &amp;quot;Total_Educ_Direct_Exp&amp;quot;       
## [189] &amp;quot;Total_Educ_Assist___Sub&amp;quot;      &amp;quot;Total_Educ_Cap_Outlay&amp;quot;       
## [191] &amp;quot;Total_Educ_Current_Exp&amp;quot;       &amp;quot;Total_Educ_Construct&amp;quot;        
## [193] &amp;quot;Elem_Educ_Total_Exp&amp;quot;          &amp;quot;Elem_Educ_Direct_Exp&amp;quot;        
## [195] &amp;quot;Elem_Educ_Cap_Outlay&amp;quot;         &amp;quot;Elem_Educ_Current_Exp&amp;quot;       
## [197] &amp;quot;Elem_Educ_Construction&amp;quot;       &amp;quot;Elem_Educ_IG_To_State&amp;quot;       
## [199] &amp;quot;Elem_Educ_IG_Local_Govts&amp;quot;     &amp;quot;Elem_Educ_IG_Sch_to_Sch&amp;quot;     
## [201] &amp;quot;Higher_Ed_Total_Exp&amp;quot;          &amp;quot;Higher_Ed_Direct_Exp&amp;quot;        
## [203] &amp;quot;Higher_Ed_Cap_Outlay&amp;quot;         &amp;quot;Higher_Ed_Current_Exp&amp;quot;       
## [205] &amp;quot;Higher_Ed_Construct&amp;quot;          &amp;quot;Higher_Ed_IG_To_St&amp;quot;          
## [207] &amp;quot;Higher_Ed_IG_Loc_Govts&amp;quot;       &amp;quot;Educ_NEC_Total_Expend&amp;quot;       
## [209] &amp;quot;Educ_NEC_Direct_Expend&amp;quot;       &amp;quot;Educ_NEC_Assistance&amp;quot;         
## [211] &amp;quot;Educ_NEC_Cap_Outlay&amp;quot;          &amp;quot;Educ_NEC_Current_Exp&amp;quot;        
## [213] &amp;quot;Educ_NEC_Construction&amp;quot;        &amp;quot;Educ_NEC_IG_To_State&amp;quot;        
## [215] &amp;quot;Educ_NEC_IG_Local_Govts&amp;quot;      &amp;quot;Emp_Sec_Adm_Direct_Exp&amp;quot;      
## [217] &amp;quot;Emp_Sec_Adm_Cap_Outlay&amp;quot;       &amp;quot;Emp_Sec_Adm_Current_Exp&amp;quot;     
## [219] &amp;quot;Emp_Sec_Adm_Construct&amp;quot;        &amp;quot;Fin_Admin_Total_Exp&amp;quot;         
## [221] &amp;quot;Fin_Admin_Direct_Exp&amp;quot;         &amp;quot;Fin_Admin_Cap_Outlay&amp;quot;        
## [223] &amp;quot;Fin_Admin_Current_Exp&amp;quot;        &amp;quot;Fin_Admin_Construction&amp;quot;      
## [225] &amp;quot;Fin_Admin_IG_To_State&amp;quot;        &amp;quot;Fin_Admin_IG_Local_Govts&amp;quot;    
## [227] &amp;quot;Fire_Prot_Total_Expend&amp;quot;       &amp;quot;Fire_Prot_Direct_Exp&amp;quot;        
## [229] &amp;quot;Fire_Prot_Cap_Outlay&amp;quot;         &amp;quot;Fire_Prot_Current_Exp&amp;quot;       
## [231] &amp;quot;Fire_Prot_Construction&amp;quot;       &amp;quot;Fire_Prot_IG_To_State&amp;quot;       
## [233] &amp;quot;Fire_Prot_IG_Local_Govts&amp;quot;     &amp;quot;Judicial_Total_Expend&amp;quot;       
## [235] &amp;quot;Judicial_Direct_Expend&amp;quot;       &amp;quot;Judicial_Cap_Outlay&amp;quot;         
## [237] &amp;quot;Judicial_Current_Exp&amp;quot;         &amp;quot;Judicial_Construction&amp;quot;       
## [239] &amp;quot;Judicial_IG_To_State&amp;quot;         &amp;quot;Judicial_IG_Local_Govts&amp;quot;     
## [241] &amp;quot;Cen_Staff_Total_Expend&amp;quot;       &amp;quot;Cen_Staff_Direct_Exp&amp;quot;        
## [243] &amp;quot;Cen_Staff_Cap_Outlay&amp;quot;         &amp;quot;Cen_Staff_Current_Exp&amp;quot;       
## [245] &amp;quot;Cen_Staff_Construction&amp;quot;       &amp;quot;Cen_Staff_IG_To_State&amp;quot;       
## [247] &amp;quot;Cen_Staff_IG_Local_Govts&amp;quot;     &amp;quot;Gen_Pub_Bldg_Total_Exp&amp;quot;      
## [249] &amp;quot;Gen_Pub_Bldg_Cap_Out&amp;quot;         &amp;quot;Gen_Pub_Bldg_Current_Exp&amp;quot;    
## [251] &amp;quot;Gen_Pub_Bldg_Construct&amp;quot;       &amp;quot;Health_Total_Expend&amp;quot;         
## [253] &amp;quot;Health_Direct_Expend&amp;quot;         &amp;quot;Health_Capital_Outlay&amp;quot;       
## [255] &amp;quot;Health_Current_Exp&amp;quot;           &amp;quot;Health_Construction&amp;quot;         
## [257] &amp;quot;Health_IG_To_State&amp;quot;           &amp;quot;Health_IG_Local_Govts&amp;quot;       
## [259] &amp;quot;Total_Hospital_Total_Exp&amp;quot;     &amp;quot;Total_Hospital_Dir_Exp&amp;quot;      
## [261] &amp;quot;Total_Hospital_Cap_Out&amp;quot;       &amp;quot;Total_Hospital_Current_Exp&amp;quot;  
## [263] &amp;quot;Total_Hospital_Construct&amp;quot;     &amp;quot;Total_Hospital_IG_To_State&amp;quot;  
## [265] &amp;quot;Total_Hospital_IG_Loc_Govts&amp;quot;  &amp;quot;Own_Hospital_Total_Exp&amp;quot;      
## [267] &amp;quot;Own_Hospital_Cap_Out&amp;quot;         &amp;quot;Own_Hospital_Current_Exp&amp;quot;    
## [269] &amp;quot;Own_Hospital_Construct&amp;quot;       &amp;quot;Hosp_Other_Total_Exp&amp;quot;        
## [271] &amp;quot;Hosp_Other_Direct_Exp&amp;quot;        &amp;quot;Hosp_Other_Cap_Outlay&amp;quot;       
## [273] &amp;quot;Hosp_Other_Current_Exp&amp;quot;       &amp;quot;Hosp_Other_Construct&amp;quot;        
## [275] &amp;quot;Hosp_Other_IG_To_State&amp;quot;       &amp;quot;Hosp_Other_IG_Loc_Govts&amp;quot;     
## [277] &amp;quot;Total_Highways_Tot_Exp&amp;quot;       &amp;quot;Total_Highways_Dir_Exp&amp;quot;      
## [279] &amp;quot;Total_Highways_Cap_Out&amp;quot;       &amp;quot;Total_Highways_Current_Exp&amp;quot;  
## [281] &amp;quot;Total_Highways_Construct&amp;quot;     &amp;quot;Regular_Hwy_Total_Exp&amp;quot;       
## [283] &amp;quot;Regular_Hwy_Direct_Exp&amp;quot;       &amp;quot;Regular_Hwy_Cap_Outlay&amp;quot;      
## [285] &amp;quot;Regular_Hwy_Current_Exp&amp;quot;      &amp;quot;Regular_Hwy_Construct&amp;quot;       
## [287] &amp;quot;Regular_Hwy_IG_To_Sta&amp;quot;        &amp;quot;Regular_Hwy_IG_Loc_Govts&amp;quot;    
## [289] &amp;quot;Toll_Hwy_Total_Expend&amp;quot;        &amp;quot;Toll_Hwy_Cap_Outlay&amp;quot;         
## [291] &amp;quot;Toll_Hwy_Current_Exp&amp;quot;         &amp;quot;Toll_Hwy_Construction&amp;quot;       
## [293] &amp;quot;Transit_Sub_Total_Exp&amp;quot;        &amp;quot;Transit_Sub_Direct_Sub&amp;quot;      
## [295] &amp;quot;Transit_Sub_IG_To_Sta&amp;quot;        &amp;quot;Transit_Sub_IG_Loc_Govts&amp;quot;    
## [297] &amp;quot;Transit_Sub_To_Own_Sys&amp;quot;       &amp;quot;Hous___Com_Total_Exp&amp;quot;        
## [299] &amp;quot;Hous___Com_Direct_Exp&amp;quot;        &amp;quot;Hous___Com_Cap_Outlay&amp;quot;       
## [301] &amp;quot;Hous___Com_Current_Exp&amp;quot;       &amp;quot;Hous___Com_Construct&amp;quot;        
## [303] &amp;quot;Hous___Com_IG_To_State&amp;quot;       &amp;quot;Hous___Com_IG_Loc_Govts&amp;quot;     
## [305] &amp;quot;Libraries_Total_Expend&amp;quot;       &amp;quot;Libraries_Direct_Exp&amp;quot;        
## [307] &amp;quot;Libraries_Cap_Outlay&amp;quot;         &amp;quot;Libraries_Current_Exp&amp;quot;       
## [309] &amp;quot;Libraries_Construction&amp;quot;       &amp;quot;Libraries_IG_To_State&amp;quot;       
## [311] &amp;quot;Libraries_IG_Local_Govts&amp;quot;     &amp;quot;Natural_Res_Total_Exp&amp;quot;       
## [313] &amp;quot;Natural_Res_Direct_Exp&amp;quot;       &amp;quot;Natural_Res_Cap_Outlay&amp;quot;      
## [315] &amp;quot;Natural_Res_Current_Exp&amp;quot;      &amp;quot;Natural_Res_Construct&amp;quot;       
## [317] &amp;quot;Natural_Res_IG_To_Sta&amp;quot;        &amp;quot;Natural_Res_IG_Loc_Govts&amp;quot;    
## [319] &amp;quot;Parking_Total_Expend&amp;quot;         &amp;quot;Parking_Direct_Expend&amp;quot;       
## [321] &amp;quot;Parking_Capital_Outlay&amp;quot;       &amp;quot;Parking_Current_Exp&amp;quot;         
## [323] &amp;quot;Parking_Construction&amp;quot;         &amp;quot;Parking_IG_To_State&amp;quot;         
## [325] &amp;quot;Parking_IG_Local_Govts&amp;quot;       &amp;quot;Parks___Rec_Total_Exp&amp;quot;       
## [327] &amp;quot;Parks___Rec_Direct_Exp&amp;quot;       &amp;quot;Parks___Rec_Cap_Outlay&amp;quot;      
## [329] &amp;quot;Parks___Rec_Current_Exp&amp;quot;      &amp;quot;Parks___Rec_Construct&amp;quot;       
## [331] &amp;quot;Parks___Rec_IG_To_Sta&amp;quot;        &amp;quot;Parks___Rec_IG_Loc_Govts&amp;quot;    
## [333] &amp;quot;Police_Prot_Total_Exp&amp;quot;        &amp;quot;Police_Prot_Direct_Exp&amp;quot;      
## [335] &amp;quot;Police_Prot_Cap_Outlay&amp;quot;       &amp;quot;Police_Prot_Current_Exp&amp;quot;     
## [337] &amp;quot;Police_Prot_Construct&amp;quot;        &amp;quot;Police_Prot_IG_To_Sta&amp;quot;       
## [339] &amp;quot;Police_Prot_IG_Loc_Govts&amp;quot;     &amp;quot;Prot_Insp_Total_Exp&amp;quot;         
## [341] &amp;quot;Prot_Insp_Direct_Exp&amp;quot;         &amp;quot;Prot_Insp_Cap_Outlay&amp;quot;        
## [343] &amp;quot;Prot_Insp_Current_Exp&amp;quot;        &amp;quot;Prot_Insp_Construction&amp;quot;      
## [345] &amp;quot;Prot_Insp_IG_To_State&amp;quot;        &amp;quot;Prot_Insp_IG_Local_Govts&amp;quot;    
## [347] &amp;quot;Public_Welf_Total_Exp&amp;quot;        &amp;quot;Public_Welf_Direct_Exp&amp;quot;      
## [349] &amp;quot;Public_Welf_Cash_Asst&amp;quot;        &amp;quot;Public_Welf_Cap_Outlay&amp;quot;      
## [351] &amp;quot;Public_Welf_Current_Exp&amp;quot;      &amp;quot;Public_Welf_Construct&amp;quot;       
## [353] &amp;quot;Welf_Categ_Total_Exp&amp;quot;         &amp;quot;Welf_Categ_Cash_Assist&amp;quot;      
## [355] &amp;quot;Welf_Categ_IG_To_State&amp;quot;       &amp;quot;Welf_Categ_IG_Loc_Govts&amp;quot;     
## [357] &amp;quot;Welf_Cash_Total_Exp&amp;quot;          &amp;quot;Welf_Cash_Cash_Assist&amp;quot;       
## [359] &amp;quot;Welf_Cash_IG_Local_Govts&amp;quot;     &amp;quot;Welf_Vend_Pmts_Medical&amp;quot;      
## [361] &amp;quot;Welf_Vend_Pmts_NEC&amp;quot;           &amp;quot;Welf_State_Share_Part_D&amp;quot;     
## [363] &amp;quot;Welf_Ins_Total_Exp&amp;quot;           &amp;quot;Welf_Ins_Cap_Outlay&amp;quot;         
## [365] &amp;quot;Welf_Ins_Current_Exp&amp;quot;         &amp;quot;Welf_Ins_Construction&amp;quot;       
## [367] &amp;quot;Welf_NEC_Total_Expend&amp;quot;        &amp;quot;Welf_NEC_Direct_Expend&amp;quot;      
## [369] &amp;quot;Welf_NEC_Cap_Outlay&amp;quot;          &amp;quot;Welf_NEC_Current_Exp&amp;quot;        
## [371] &amp;quot;Welf_NEC_Construction&amp;quot;        &amp;quot;Welf_NEC_IG_To_State&amp;quot;        
## [373] &amp;quot;Welf_NEC_IG_Local_Govts&amp;quot;      &amp;quot;Sewerage_Total_Expend&amp;quot;       
## [375] &amp;quot;Sewerage_Direct_Expend&amp;quot;       &amp;quot;Sewerage_Cap_Outlay&amp;quot;         
## [377] &amp;quot;Sewerage_Current_Exp&amp;quot;         &amp;quot;Sewerage_Construction&amp;quot;       
## [379] &amp;quot;Sewerage_IG_To_State&amp;quot;         &amp;quot;Sewerage_IG_Local_Govts&amp;quot;     
## [381] &amp;quot;SW_Mgmt_Total_Expend&amp;quot;         &amp;quot;SW_Mgmt_Direct_Expend&amp;quot;       
## [383] &amp;quot;SW_Mgmt_Capital_Outlay&amp;quot;       &amp;quot;SW_Mgmt_Current_Exp&amp;quot;         
## [385] &amp;quot;SW_Mgmt_Construction&amp;quot;         &amp;quot;SW_Mgmt_IG_To_State&amp;quot;         
## [387] &amp;quot;SW_Mgmt_IG_Local_Govts&amp;quot;       &amp;quot;Water_Trans_Total_Exp&amp;quot;       
## [389] &amp;quot;Water_Trans_Direct_Exp&amp;quot;       &amp;quot;Water_Trans_Cap_Outlay&amp;quot;      
## [391] &amp;quot;Water_Trans_Current_Exp&amp;quot;      &amp;quot;Water_Trans_Construct&amp;quot;       
## [393] &amp;quot;Water_Trans_IG_To_Sta&amp;quot;        &amp;quot;Water_Trans_IG_Loc_Govts&amp;quot;    
## [395] &amp;quot;Interest_on_Gen_Debt&amp;quot;         &amp;quot;General_NEC_Total_Exp&amp;quot;       
## [397] &amp;quot;General_NEC_Direct_Exp&amp;quot;       &amp;quot;VetBonus&amp;quot;                    
## [399] &amp;quot;General_NEC_Cap_Outlay&amp;quot;       &amp;quot;General_NEC_Current_Exp&amp;quot;     
## [401] &amp;quot;General_NEC_Construct&amp;quot;        &amp;quot;General_NEC_IG_To_St&amp;quot;        
## [403] &amp;quot;General_NEC_IG_Loc_Govts&amp;quot;     &amp;quot;General_NEC_IG_To_Fed&amp;quot;       
## [405] &amp;quot;Liquor_Stores_Tot_Exp&amp;quot;        &amp;quot;Liquor_Stores_Cap_Out&amp;quot;       
## [407] &amp;quot;Liquor_Stores_Current_Exp&amp;quot;    &amp;quot;Liquor_Stores_Constr&amp;quot;        
## [409] &amp;quot;Total_Util_Total_Exp&amp;quot;         &amp;quot;Total_Util_Inter_Exp&amp;quot;        
## [411] &amp;quot;Total_Util_Cap_Outlay&amp;quot;        &amp;quot;Total_Util_Current_Exp&amp;quot;      
## [413] &amp;quot;Total_Util_Construct&amp;quot;         &amp;quot;Water_Util_Total_Exp&amp;quot;        
## [415] &amp;quot;Water_Util_Inter_Exp&amp;quot;         &amp;quot;Water_Util_Cap_Outlay&amp;quot;       
## [417] &amp;quot;Water_Util_Current_Exp&amp;quot;       &amp;quot;Water_Util_Construct&amp;quot;        
## [419] &amp;quot;Elec_Util_Total_Exp&amp;quot;          &amp;quot;Elec_Util_Inter_Exp&amp;quot;         
## [421] &amp;quot;Elec_Util_Cap_Outlay&amp;quot;         &amp;quot;Elec_Util_Current_Exp&amp;quot;       
## [423] &amp;quot;Elec_Util_Construct&amp;quot;          &amp;quot;Gas_Util_Total_Exp&amp;quot;          
## [425] &amp;quot;Gas_Util_Inter_Exp&amp;quot;           &amp;quot;Gas_Util_Cap_Outlay&amp;quot;         
## [427] &amp;quot;Gas_Util_Current_Exp&amp;quot;         &amp;quot;Gas_Util_Construct&amp;quot;          
## [429] &amp;quot;Trans_Util_Total_Exp&amp;quot;         &amp;quot;Trans_Util_Inter_Exp&amp;quot;        
## [431] &amp;quot;Trans_Util_Cap_Outlay&amp;quot;        &amp;quot;Trans_Util_Current_Exp&amp;quot;      
## [433] &amp;quot;Trans_Util_Construct&amp;quot;         &amp;quot;Emp_Ret_Total_Expend&amp;quot;        
## [435] &amp;quot;Emp_Ret_Benefit_Paymts&amp;quot;       &amp;quot;Emp_Ret_Withdrawals&amp;quot;         
## [437] &amp;quot;Emp_Ret_Other_Paymts&amp;quot;         &amp;quot;Unemp_Comp_Total_Exp&amp;quot;        
## [439] &amp;quot;Unemp_Comp_Ben_Paymts&amp;quot;        &amp;quot;Unemp_Ext___Spec_Pmts&amp;quot;       
## [441] &amp;quot;Total_Debt_Outstanding&amp;quot;       &amp;quot;Total_Long_Term_Debt_Out&amp;quot;    
## [443] &amp;quot;ST_Debt_End_of_Year&amp;quot;          &amp;quot;Total_Beg_LTD_Out&amp;quot;           
## [445] &amp;quot;Beg_LTD_Out_Private_Purp&amp;quot;     &amp;quot;Beg_LTD_Out_All_Other&amp;quot;       
## [447] &amp;quot;Beg_LTD_Out_Utility&amp;quot;          &amp;quot;Beg_LTD_Out_Water_Util&amp;quot;      
## [449] &amp;quot;Beg_LTD_Out_Elec_Util&amp;quot;        &amp;quot;Beg_LTD_Out_Gas_Util&amp;quot;        
## [451] &amp;quot;Beg_LTD_Out_Trans_Util&amp;quot;       &amp;quot;Beg_LTD_Out_General&amp;quot;         
## [453] &amp;quot;Beg_LTD_Out_Education&amp;quot;        &amp;quot;Beg_LTD_Out_Priv_Purp&amp;quot;       
## [455] &amp;quot;Beg_LTD_Out_Other_NEC&amp;quot;        &amp;quot;Total_LTD_Issued&amp;quot;            
## [457] &amp;quot;LTD_Iss_Private_Purp&amp;quot;         &amp;quot;LTD_Iss_All_Other&amp;quot;           
## [459] &amp;quot;LTD_Iss_Utility&amp;quot;              &amp;quot;LTD_Iss_Util_Water&amp;quot;          
## [461] &amp;quot;LTD_Iss_Util_Electric&amp;quot;        &amp;quot;LTD_Iss_Util_Gas_Supply&amp;quot;     
## [463] &amp;quot;LTD_Iss_Util_Transit&amp;quot;         &amp;quot;LTD_Iss_General&amp;quot;             
## [465] &amp;quot;LTD_Iss_Gen_Elem_Educ&amp;quot;        &amp;quot;LTD_Iss_Gen_Other_Educ&amp;quot;      
## [467] &amp;quot;LTD_Iss_Gen_Other_NEC&amp;quot;        &amp;quot;Total_LTD_Iss_FFC&amp;quot;           
## [469] &amp;quot;LTD_Iss_FFC_Utility&amp;quot;          &amp;quot;LTD_Iss_FFC_Water_Util&amp;quot;      
## [471] &amp;quot;LTD_Iss_FFC_Elec_Util&amp;quot;        &amp;quot;LTD_Iss_FFC_Gas_Util&amp;quot;        
## [473] &amp;quot;LTD_Iss_FFC_Trans_Util&amp;quot;       &amp;quot;LTD_Iss_FFC_General&amp;quot;         
## [475] &amp;quot;LTD_Iss_FFC_Elem_Educ&amp;quot;        &amp;quot;LTD_Iss_FFC_Other_Educ&amp;quot;      
## [477] &amp;quot;LTD_Iss_FFC_Other_NEC&amp;quot;        &amp;quot;Total_LTD_Iss_NG&amp;quot;            
## [479] &amp;quot;LTD_Iss_NG_Utility&amp;quot;           &amp;quot;LTD_Iss_NG_Water_Util&amp;quot;       
## [481] &amp;quot;LTD_Iss_NG_Elec_Util&amp;quot;         &amp;quot;LTD_Iss_NG_Gas_Util&amp;quot;         
## [483] &amp;quot;LTD_Iss_NG_Trans_Util&amp;quot;        &amp;quot;LTD_Iss_NG_General&amp;quot;          
## [485] &amp;quot;LTD_Iss_NG_Elem_Educ&amp;quot;         &amp;quot;LTD_Iss_NG_Other_Educ&amp;quot;       
## [487] &amp;quot;LTD_Iss_NG_Private_Purp&amp;quot;      &amp;quot;LTD_Iss_NG_Other_NEC&amp;quot;        
## [489] &amp;quot;Total_LTD_Iss_Unsp&amp;quot;           &amp;quot;LTD_Iss_Unsp_Utility&amp;quot;        
## [491] &amp;quot;LTD_Iss_Unsp_Water_Util&amp;quot;      &amp;quot;LTD_Iss_Unsp_Elec_Util&amp;quot;      
## [493] &amp;quot;LTD_Iss_Unsp_Gas_Util&amp;quot;        &amp;quot;LTD_Iss_Unsp_Trans_Util&amp;quot;     
## [495] &amp;quot;LTD_Iss_Unsp_General&amp;quot;         &amp;quot;LTD_Iss_Unsp_Elem_Educ&amp;quot;      
## [497] &amp;quot;LTD_Iss_Unsp_Other_Educ&amp;quot;      &amp;quot;LTD_Iss_Unsp_Other_NEC&amp;quot;      
## [499] &amp;quot;Total_LTD_Retired&amp;quot;            &amp;quot;LTD_Ret_Private_Purp&amp;quot;        
## [501] &amp;quot;LTD_Ret_All_Other&amp;quot;            &amp;quot;LTD_Ret_Utility&amp;quot;             
## [503] &amp;quot;LTD_Ret_Util_Water&amp;quot;           &amp;quot;LTD_Ret_Util_Electric&amp;quot;       
## [505] &amp;quot;LTD_Ret_Util_Gas_Supply&amp;quot;      &amp;quot;LTD_Ret_Util_Transit&amp;quot;        
## [507] &amp;quot;LTD_Ret_General&amp;quot;              &amp;quot;LTD_Ret_Gen_Elem_Educ&amp;quot;       
## [509] &amp;quot;LTD_Ret_Gen_Other_Educ&amp;quot;       &amp;quot;LTD_Ret_Gen_Other_NEC&amp;quot;       
## [511] &amp;quot;Total_LTD_Ret_FFC&amp;quot;            &amp;quot;LTD_Ret_FFC_Utility&amp;quot;         
## [513] &amp;quot;LTD_Ret_FFC_Water_Util&amp;quot;       &amp;quot;LTD_Ret_FFC_Elec_Util&amp;quot;       
## [515] &amp;quot;LTD_Ret_FFC_Gas_Util&amp;quot;         &amp;quot;LTD_Ret_FFC_Trans_Util&amp;quot;      
## [517] &amp;quot;LTD_Ret_FFC_General&amp;quot;          &amp;quot;LTD_Ret_FFC_Elem_Educ&amp;quot;       
## [519] &amp;quot;LTD_Ret_FFC_Other_Educ&amp;quot;       &amp;quot;LTD_Ret_FFC_Other_NEC&amp;quot;       
## [521] &amp;quot;Total_LTD_Ret_NG&amp;quot;             &amp;quot;LTD_Ret_NG_Utility&amp;quot;          
## [523] &amp;quot;LTD_Ret_NG_Water_Util&amp;quot;        &amp;quot;LTD_Ret_NG_Elec_Util&amp;quot;        
## [525] &amp;quot;LTD_Ret_NG_Gas_Util&amp;quot;          &amp;quot;LTD_Ret_NG_Trans_Util&amp;quot;       
## [527] &amp;quot;LTD_Ret_NG_General&amp;quot;           &amp;quot;LTD_Ret_NG_Elem_Educ&amp;quot;        
## [529] &amp;quot;LTD_Ret_NG_Other_Educ&amp;quot;        &amp;quot;LTD_Ret_NG_Private_Purp&amp;quot;     
## [531] &amp;quot;LTD_Ret_NG_Other_NEC&amp;quot;         &amp;quot;Total_LTD_Ret_Unsp&amp;quot;          
## [533] &amp;quot;LTD_Ret_Unsp_Utility&amp;quot;         &amp;quot;LTD_Ret_Unsp_Water_Util&amp;quot;     
## [535] &amp;quot;LTD_Ret_Unsp_Elec_Utili&amp;quot;      &amp;quot;LTD_Ret_Unsp_Gas_Util&amp;quot;       
## [537] &amp;quot;LTD_Ret_Unsp_Trans_Util&amp;quot;      &amp;quot;LTD_Ret_Unsp_General&amp;quot;        
## [539] &amp;quot;LTD_Ret_Unsp_Elem_Educ&amp;quot;       &amp;quot;LTD_Ret_Unsp_Other_Educ&amp;quot;     
## [541] &amp;quot;LTD_Ret_Unsp_Other_NEC&amp;quot;       &amp;quot;Total_LTD_Out&amp;quot;               
## [543] &amp;quot;LTD_Out_Private_Purp&amp;quot;         &amp;quot;LTD_Out_All_Other&amp;quot;           
## [545] &amp;quot;Total_LTD_Out_Utility&amp;quot;        &amp;quot;LTD_Out_Util_Water&amp;quot;          
## [547] &amp;quot;LTD_Out_Util_Electric&amp;quot;        &amp;quot;LTD_Out_Util_Gas_Supply&amp;quot;     
## [549] &amp;quot;LTD_Out_Util_Transit&amp;quot;         &amp;quot;LTD_Out_General&amp;quot;             
## [551] &amp;quot;LTD_Out_Gen_Elem_Educ&amp;quot;        &amp;quot;LTD_Out_Gen_Other_Educ&amp;quot;      
## [553] &amp;quot;LTD_Out_Gen_Other_NEC&amp;quot;        &amp;quot;Total_LTD_Out_FFC&amp;quot;           
## [555] &amp;quot;LTD_Out_FFC_Utility&amp;quot;          &amp;quot;LTD_Out_FFC_Water_Util&amp;quot;      
## [557] &amp;quot;LTD_Out_FFC_Elec_Util&amp;quot;        &amp;quot;LTD_Out_FFC_Gas_Util&amp;quot;        
## [559] &amp;quot;LTD_Out_FFC_Trans_Util&amp;quot;       &amp;quot;LTD_Out_FFC_General&amp;quot;         
## [561] &amp;quot;LTD_Out_FFC_Elem_Educ&amp;quot;        &amp;quot;LTD_Out_FFC_Other_Educ&amp;quot;      
## [563] &amp;quot;LTD_Out_FFC_Other_NEC&amp;quot;        &amp;quot;Tot_LTD_Out_NG&amp;quot;              
## [565] &amp;quot;LTD_Out_NG_Utility&amp;quot;           &amp;quot;LTD_Out_NG_Water_Util&amp;quot;       
## [567] &amp;quot;LTD_Out_NG_Elec_Util&amp;quot;         &amp;quot;LTD_Out_NG_Gas_Util&amp;quot;         
## [569] &amp;quot;LTD_Out_NG_Trans_Util&amp;quot;        &amp;quot;LTD_Out_NG_General&amp;quot;          
## [571] &amp;quot;LTD_Out_NG_Elem_Educ&amp;quot;         &amp;quot;LTD_Out_NG_Other_Educ&amp;quot;       
## [573] &amp;quot;LTD_Out_NG_Private_Purp&amp;quot;      &amp;quot;LTD_Out_NG_Other_NEC&amp;quot;        
## [575] &amp;quot;Total_Cash___Securities&amp;quot;      &amp;quot;Insur_Trust_Cash___Sec&amp;quot;      
## [577] &amp;quot;Emp_Retire_Cash___Sec&amp;quot;        &amp;quot;Emp_Retire_Cash___Dep&amp;quot;       
## [579] &amp;quot;Emp_Retire_Total_Sec&amp;quot;         &amp;quot;Emp_Retire_Sec_Tot_Fed&amp;quot;      
## [581] &amp;quot;Emp_Retire_Sec_S_L_Secur&amp;quot;     &amp;quot;Emp_Retire_Sec_Tot_Nong&amp;quot;     
## [583] &amp;quot;Emp_Retire_Sec_Corp_Bds&amp;quot;      &amp;quot;Emp_Retire_Sec_Corp_Stk&amp;quot;     
## [585] &amp;quot;Emp_Retire_Sec_Mortgages&amp;quot;     &amp;quot;Emp_Retire_Sec_Misc_Inv&amp;quot;     
## [587] &amp;quot;Emp_Retire_Sec_Oth_Nong&amp;quot;      &amp;quot;Unemp_Comp_Cash___Sec&amp;quot;       
## [589] &amp;quot;Unemp_Comp_Bal_In_US_Trs&amp;quot;     &amp;quot;Unemp_Comp_Other_Balance&amp;quot;    
## [591] &amp;quot;Nonin_Trust_Cash___Sec&amp;quot;       &amp;quot;Sinking_Fd_Cash___Sec&amp;quot;       
## [593] &amp;quot;Bond_Fd_Cash___Sec&amp;quot;           &amp;quot;Oth_Nonin_Fd_Cash___Sec&amp;quot;     
## [595] &amp;quot;NameFixed&amp;quot;                    &amp;quot;NameFLower&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;revenues-per-capita&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Revenues per capita&lt;/h3&gt;
&lt;p&gt;Change it up with a recalcalculation of the data and a new map.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my.GFD.2015 &amp;lt;- State.Data %&amp;gt;% 
  filter(Year4==2015) %&amp;gt;%
  mutate(value = Total_Revenue / Population, region = NameFLower) %&amp;gt;%
  select(region, value)
state_choropleth(my.GFD.2015, title=&amp;quot;Total Revenues per capita&amp;quot;, num_colors=1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in self$bind(): The following regions were missing and are being
## set to NA: district of columbia&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.data.viajes/post/2018-02-25-mapping-with-the-government-finance-database_files/figure-html/Finished2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;putting-them-together-counties&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Putting them together: Counties&lt;/h1&gt;
&lt;p&gt;This part is slightly harder. It would appear as though the Government Finance Data doesn’t have a combined FIPS code; this is the region in map. But, the data contain the state and county fips codes and we can recreate them with the state code times 1000 plus the county. After that, plot the General Revenue.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;County.Data.2 &amp;lt;- read.csv(&amp;quot;~/Downloads/TheGovernmentFinanceDatabase_CountyData/CountyData.csv&amp;quot;)
County.Data.2 &amp;lt;- County.Data.2 %&amp;gt;% mutate(region = FIPS_Code_State* 1000 + FIPS_County)
my.County.GFD.2015 &amp;lt;- County.Data.2 %&amp;gt;% 
  filter(Year4==2012) %&amp;gt;% mutate(value = General_Revenue / Population) %&amp;gt;%
  select(region, value)
county_choropleth(my.County.GFD.2015, title=&amp;quot;General Revenue per capita&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.data.viajes/post/2018-02-25-mapping-with-the-government-finance-database_files/figure-html/County1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The zoom feature is pretty cool: there are two defined forms for the county and states. The naming conventions follow the &lt;code&gt;county.regions&lt;/code&gt; format and are lower case. Let me grab &lt;em&gt;oregon&lt;/em&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my.County.GFD.2015 &amp;lt;- County.Data.2 %&amp;gt;% 
  filter(Year4==2012) %&amp;gt;% mutate(value = General_Revenue / Population) %&amp;gt;%
  select(region, value)
county_choropleth(my.County.GFD.2015, title=&amp;quot;General Revenue per capita&amp;quot;, state_zoom=&amp;quot;oregon&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.data.viajes/post/2018-02-25-mapping-with-the-government-finance-database_files/figure-html/CountyMZ-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Longitudinal Panel Data R Packages</title>
      <link>https://www.data.viajes/post/panel-data-r-packages/</link>
      <pubDate>Sat, 24 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.data.viajes/post/panel-data-r-packages/</guid>
      <description>&lt;div id=&#34;longitudinal-and-panel-data-analysis-in-r&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Longitudinal and Panel Data Analysis in R&lt;/h1&gt;
&lt;p&gt;Goal: A CRAN task view for panel/longitudinal data analysis in R.&lt;/p&gt;
&lt;div id=&#34;what-is-panel-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What is Panel Data?&lt;/h2&gt;
&lt;p&gt;Panel data are variously called longitudinal, panel, cross-sectional time series, and pooled time series data. The most precise definition is two-dimensional data; invariably one of the dimensions is time. We can think about a general depiction of what a model with linear coefficients typical for such data structures, though ridiculously overparameterized, like so:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ y_{it} = \alpha_{it} + X_{it}\beta_{it} + \epsilon_{it} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For example, suppose that we have a set of countries, or US states, municipalities in a state, or even individual people, indexed by &lt;span class=&#34;math inline&#34;&gt;\(i \in N\)&lt;/span&gt;. We observe those data at &lt;span class=&#34;math inline&#34;&gt;\(t \in T\)&lt;/span&gt; points in time. To be clear, a standard cross-section of data in this notation suppresses &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; because there is no observed over time variation. A single time series suppresses the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;. Of course, &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt; are greater than 1.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;common-transformations-anova&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Common transformations: ANOVA&lt;/h2&gt;
&lt;p&gt;There are common transformations undertaken on the data prior to analysis. For example, the &lt;em&gt;within&lt;/em&gt; transformation that underlies a &lt;em&gt;fixed effects&lt;/em&gt; regression model transforms each vector of data (for &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;) as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ x^{W}_{it} = x_{it} - \overline{x}_{i} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The within-transformed data are of identical dimensions to the untransformed data. The other common transformation isolates the &lt;span class=&#34;math inline&#34;&gt;\(n-vector\)&lt;/span&gt; of unit means – the so-called &lt;em&gt;between&lt;/em&gt; data. Because the dimensions are non-overlapping/orthogonal; the key result in the applied statistics shows that the total variation in a variable &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is the sum of the &lt;em&gt;within&lt;/em&gt; variance and the &lt;em&gt;between&lt;/em&gt; variance.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;some-basic-summary&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Some Basic Summary&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;dplyr&lt;/em&gt; makes the basic summary and presentation quite straightforward. &lt;code&gt;plm&lt;/code&gt; contains a few datasets; I will choose an excerpt from Summers and Heston’s Penn World Tables – &lt;em&gt;SumHes&lt;/em&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(plm)
library(tidyverse)
library(dplyr)
data(SumHes)
summary(SumHes)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       year              country      opec       com      
##  Min.   :1960   ALGERIA     :  26   no :3146   no :3120  
##  1st Qu.:1966   ANGOLA      :  26   yes: 104   yes: 130  
##  Median :1972   BENIN       :  26                        
##  Mean   :1972   BOTSWANA    :  26                        
##  3rd Qu.:1979   BURKINA FASO:  26                        
##  Max.   :1985   BURUNDI     :  26                        
##                 (Other)     :3094                        
##       pop               gdp              sr       
##  Min.   :     42   Min.   :  257   Min.   :-4.50  
##  1st Qu.:   2266   1st Qu.:  942   1st Qu.: 8.90  
##  Median :   5919   Median : 1957   Median :16.30  
##  Mean   :  29436   Mean   : 3400   Mean   :16.91  
##  3rd Qu.:  18529   3rd Qu.: 4640   3rd Qu.:24.10  
##  Max.   :1051013   Max.   :16570   Max.   :45.50  
## &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To actually summarize the data in an appropriate fashion, we will use &lt;em&gt;dplyr&lt;/em&gt; and &lt;em&gt;summarise&lt;/em&gt; after grouping the data by &lt;em&gt;country&lt;/em&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(rlang)
library(haven)
nlswork &amp;lt;- read_stata(&amp;quot;http://www.stata-press.com/data/r13/nlswork.dta&amp;quot;)
nlswork2 &amp;lt;- nlswork
nlswork2$race &amp;lt;- as.character(nlswork$race)
xtsum &amp;lt;- function(formula, data) {
  require(rlang)
  require(tidyverse)
  pform &amp;lt;- terms(formula, data=data)
  unit &amp;lt;- pform[[2]]
  vars &amp;lt;- attr(pform, &amp;quot;term.labels&amp;quot;)
  cls &amp;lt;- sapply(data, class)
  data &amp;lt;- data %&amp;gt;% select(which(cls%in%c(&amp;quot;numeric&amp;quot;,&amp;quot;integer&amp;quot;)))
  varnames &amp;lt;- intersect(names(data),vars)
  sumfunc &amp;lt;- function(data=data, varname, unit) {
  loc.unit &amp;lt;- enquo(unit)
  varname &amp;lt;- ensym(varname)
    ores &amp;lt;- data %&amp;gt;% filter(!is.na(!! varname)==TRUE) %&amp;gt;% summarise(
    O.mean=round(mean(`$`(data, !! varname), na.rm=TRUE), digits=3),
    O.sd=round(sd(`$`(data, !! varname), na.rm=TRUE), digits=3), 
    O.min = min(`$`(data, !! varname), na.rm=TRUE), 
    O.max=max(`$`(data, !! varname), na.rm=TRUE), 
    O.SumSQ=round(sum(scale(`$`(data, !! varname), center=TRUE, scale=FALSE)^2, na.rm=TRUE), digits=3), 
    O.N=sum(as.numeric((!is.na(`$`(data, !! varname))))))
 bmeans &amp;lt;- data %&amp;gt;% filter(!is.na(!! varname)==TRUE) %&amp;gt;% group_by(!! loc.unit) %&amp;gt;% summarise(
   meanx=mean(`$`(.data, !! varname), na.rm=T), 
   t.count=sum(as.numeric(!is.na(`$`(.data, !! varname)))))
  bres &amp;lt;- bmeans %&amp;gt;% ungroup() %&amp;gt;% summarise(
    B.sd = round(sd(meanx, na.rm=TRUE), digits=3),
    B.min = min(meanx, na.rm=TRUE), 
    B.max=max(meanx, na.rm=TRUE), 
    Units=sum(as.numeric(!is.na(t.count))), 
    t.bar=round(mean(t.count, na.rm=TRUE), digits=3))
  wdat &amp;lt;- data %&amp;gt;% filter(!is.na(!! varname)==TRUE) %&amp;gt;% group_by(!! loc.unit) %&amp;gt;% mutate(
    W.x = scale(`$`(.data,!! varname), scale=FALSE))
  wres &amp;lt;- wdat %&amp;gt;% ungroup() %&amp;gt;% summarise(
    W.sd=round(sd(W.x, na.rm=TRUE), digits=3), 
    W.min=min(W.x, na.rm=TRUE), 
    W.max=max(W.x, na.rm=TRUE), 
    W.SumSQ=round(sum(W.x^2, na.rm=TRUE), digits=3))
    W.Ratio &amp;lt;- round(wres$W.SumSQ/ores$O.SumSQ, digits=3)
  return(c(ores,bres,wres,Within.Ovr.Ratio=W.Ratio))
  }
res1 &amp;lt;- sapply(varnames, function(x) {sumfunc(data, !!x, !!unit)})
return(t(res1))
}  
xtsum(idcode~., data=nlswork2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          O.mean O.sd   O.min O.max    O.SumSQ  O.N   B.sd   B.min B.max   
## year     77.959 6.384  68    88       1162831  28534 5.157  68    88      
## birth_yr 48.085 3.013  41    54       258999.4 28534 3.052  41    54      
## age      29.045 6.701  14    46       1279992  28510 5.486  14    45      
## msp      0.603  0.489  0     1        6827.437 28518 0.398  0     1       
## nev_mar  0.23   0.421  0     1        5045.599 28518 0.368  0     1       
## grade    12.533 2.324  0     18       154082.7 28532 2.567  0     18      
## collgrad 0.168  0.374  0     1        3989.224 28534 0.405  0     1       
## not_smsa 0.282  0.45   0     1        5781.348 28526 0.411  0     1       
## c_city   0.357  0.479  0     1        6549.949 28526 0.427  0     1       
## south    0.41   0.492  0     1        6898.155 28526 0.467  0     1       
## ind_code 7.693  2.994  1     12       252718.4 28193 2.543  1     12      
## occ_code 4.778  3.065  1     13       266984.6 28413 2.865  1     13      
## union    0.234  0.424  0     1        3452.712 19238 0.334  0     1       
## wks_ue   2.548  7.294  0     76       1214713  22830 5.181  0     76      
## ttl_exp  6.215  4.652  0     28.88461 617516.8 28534 3.724  0     24.7062 
## tenure   3.124  3.751  0     25.91667 395453.2 28101 2.797  0     21.16667
## hours    36.56  9.87   1     168      2772858  28467 7.847  1     83.5    
## wks_work 53.989 29.032 0     104      23457236 27831 20.645 0     104     
## ln_wage  1.675  0.478  0     5.263916 6521.884 28534 0.425  0     3.912023
##          Units t.bar W.sd  W.min      W.max     W.SumSQ  Within.Ovr.Ratio
## year     4711  6.057 5.138 -14.16667  14.75     753323.2 0.648           
## birth_yr 4711  6.057 0     0          0         0        0               
## age      4710  6.053 5.169 -14.25     14.75     761852.2 0.595           
## msp      4711  6.053 0.324 -0.9333333 0.9333333 2991.619 0.438           
## nev_mar  4711  6.053 0.246 -0.9333333 0.9333333 1720.909 0.341           
## grade    4709  6.059 0     0          0         0        0               
## collgrad 4711  6.057 0     0          0         0        0               
## not_smsa 4711  6.055 0.183 -0.9285714 0.9333333 959.921  0.166           
## c_city   4711  6.055 0.249 -0.9333333 0.9333333 1768.61  0.27            
## south    4711  6.055 0.16  -0.9333333 0.9333333 728.353  0.106           
## ind_code 4695  6.005 1.708 -9.2       9.428571  82284.83 0.326           
## occ_code 4699  6.047 1.65  -10.3      10.66667  77374.88 0.29            
## union    4150  4.636 0.267 -0.9166667 0.9166667 1369.971 0.397           
## wks_ue   4645  4.915 6.054 -36.5      61.83333  836703.7 0.689           
## ttl_exp  4711  6.057 3.484 -15.85799  14.1656   346367.2 0.561           
## tenure   4699  5.98  2.66  -17.40278  12.5      198792.1 0.503           
## hours    4710  6.044 7.521 -38.71429  93.5      1610069  0.581           
## wks_work 4686  5.939 23.97 -72.42857  77.16667  15990016 0.682           
## ln_wage  4711  6.057 0.293 -2.082629  3.108762  2443.848 0.375&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The previous command takes a panel dataset organized by the unit and decomposes the within and between variance and standard deviation, etc. To summarize an individual variable in the same fashion, the command XTSUM below should accomplish the task.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;XTSUM &amp;lt;- function(data, varname, unit) {
  varname &amp;lt;- enquo(varname)
  unit &amp;lt;- enquo(unit)
  ores &amp;lt;- nlswork %&amp;gt;% summarise(ovr.mean=mean(!! varname, na.rm=TRUE), ovr.sd=sd(!! varname, na.rm=TRUE), ovr.min = min(!! varname, na.rm=TRUE), ovr.max=max(!! varname, na.rm=TRUE))
  bmeans &amp;lt;- nlswork %&amp;gt;% group_by(!!unit) %&amp;gt;% summarise(meanx=mean(!! varname, na.rm=T))
  bres &amp;lt;- bmeans %&amp;gt;% summarise(between.sd = sd(meanx, na.rm=TRUE), between.min = min(meanx, na.rm=TRUE), between.max=max(meanx, na.rm=TRUE), Groups=n())
  wdat &amp;lt;- nlswork %&amp;gt;% group_by(!!unit) %&amp;gt;% mutate(W.x = scale(!! varname, scale=FALSE))
  wres &amp;lt;- wdat %&amp;gt;% ungroup() %&amp;gt;% summarise(within.sd=sd(W.x, na.rm=TRUE), within.min=min(W.x, na.rm=TRUE), within.max=max(W.x, na.rm=TRUE))
  return(list(ores=ores,bres=bres,wres=wres))
}
XTSUM(nlswork, varname=hours, unit=idcode)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $ores
## # A tibble: 1 x 4
##   ovr.mean ovr.sd ovr.min ovr.max
##      &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
## 1     36.6   9.87       1     168
## 
## $bres
## # A tibble: 1 x 4
##   between.sd between.min between.max Groups
##        &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;  &amp;lt;int&amp;gt;
## 1       7.85           1        83.5   4711
## 
## $wres
## # A tibble: 1 x 3
##   within.sd within.min within.max
##       &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;
## 1      7.52      -38.7       93.5&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;models-for-panel-data-standard-models-in-plm&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Models for panel data: Standard models in &lt;em&gt;plm()&lt;/em&gt;&lt;/h1&gt;
&lt;p&gt;The workhorse package/library for the basic panel data models in R is &lt;em&gt;plm&lt;/em&gt;. The &lt;em&gt;JStatSoft&lt;/em&gt; &lt;a href=&#34;https://cran.r-project.org/web/packages/plm/vignettes/plm.pdf&#34;&gt;article was modified to become the vignette&lt;/a&gt; and is quite detailed, though dated. The previous equation can be simplified and detailed to derive the commonly deployed models.&lt;/p&gt;
&lt;div id=&#34;a-pooled-regression&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;A Pooled Regression&lt;/h3&gt;
&lt;p&gt;Simply suppresses all subscripts on &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; to write:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ y_{it} = \alpha + X_{it}\beta + \epsilon_{it} \]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-within-regression&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;A Within Regression&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ y_{it} = \alpha + X_{it}\beta^{W} + \epsilon^{W}_{it} \]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-between-regression&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;A Between Regression&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \overline{y}_{i} = \alpha + \overline{X}_{i}\beta^{B} + \epsilon^{B}_{i} \]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;models-with-heterogeneous-time-trends-phtt&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Models with heterogeneous time trends: &lt;em&gt;phtt()&lt;/em&gt;&lt;/h1&gt;
&lt;p&gt;An &lt;a href=&#34;https://arxiv.org/pdf/1407.6484.pdf&#34;&gt;article&lt;/a&gt; on panel data analysis with heterogeneous time trends appears in &lt;em&gt;JStatSoft&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;multi-state-models-in-continuous-time-msm&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Multi-state models in continuous time &lt;em&gt;msm()&lt;/em&gt;&lt;/h1&gt;
&lt;p&gt;The &lt;em&gt;msm&lt;/em&gt; library fits homogenous and inhomogenous Markov models for multinomial outcomes.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;latent-markov-models-with-lmest&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Latent Markov models with &lt;em&gt;LMest()&lt;/em&gt;&lt;/h1&gt;
&lt;pre&gt;&lt;code&gt;LM models are designed to deal with univariate and multivariate longitudinal data based on the repeated observation of a panel of subjects across time. More in detail, LM models are specially tailored to study the evolution of an individual characteristic of interest that is not directly observable. This characteristic is represented by a latent process following a Markov chain as in a Hidden Markov (HM) model (Zucchini and MacDonald, 2009). These models also allow us to account for time-varying unobserved heterogeneity in addition to the effect of observable covariates on the response variables.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;a href=&#34;https://arxiv.org/pdf/1501.04448.pdf&#34;&gt;preprint&lt;/a&gt; is here.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cquad&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;em&gt;cquad&lt;/em&gt;&lt;/h1&gt;
&lt;p&gt;In the study of non-linear panel data models, unobserved heterogeneity and state dependence are hardest with the least information – binary outcomes. The &lt;em&gt;cquad&lt;/em&gt; package for Stata and R utilizes an approach by Bartolucci and Nigro described in &lt;a href=&#34;https://www.jstatsoft.org/article/view/v078i07/v78i07.pdf&#34;&gt;Bartolucci and Pigini&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Black Boxes: A Gender Gap Example</title>
      <link>https://www.data.viajes/post/black-boxes-a-gender-gap-example/</link>
      <pubDate>Thu, 22 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.data.viajes/post/black-boxes-a-gender-gap-example/</guid>
      <description>&lt;div id=&#34;variance-in-the-outcome-the-black-box&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Variance in the Outcome: The Black Box&lt;/h2&gt;
&lt;p&gt;Regression models engage an exercise in variance accounting. How much of the outcome is explained by the inputs, individually (slope divided by standard error is t) and collectively (Average explained/Average unexplained with averaging over degrees of freedom is F). This, of course, assumes normal errors. This document provides a function for making use of the black box. Just as in common parlance, a black box is the unexplained. Let’s take an example.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;options(scipen=10)
OregonSalaries &amp;lt;- structure(list(Obs = 1:32, Salary = c(41514.38701, 40964.06985, 
39170.19178, 37936.57206, 33981.77752, 36077.27107, 39174.05733, 
39037.372, 29131.74865, 36200.44592, 38561.3987, 33247.92306, 
33609.4874, 33669.22275, 37805.83017, 35846.13454, 47342.65909, 
46382.3851, 45812.91029, 46409.65664, 43796.05285, 43124.02135, 
49443.81792, 44805.79217, 44440.32001, 46679.59218, 47337.09786, 
47298.72531, 41461.0474, 43598.293, 43431.18499, 49266.41189), 
    Gender = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L), .Label = c(&amp;quot;Female&amp;quot;, &amp;quot;Male&amp;quot;
    ), class = &amp;quot;factor&amp;quot;)), .Names = c(&amp;quot;Obs&amp;quot;, &amp;quot;Salary&amp;quot;, &amp;quot;Gender&amp;quot;
), class = &amp;quot;data.frame&amp;quot;, row.names = c(NA, -32L))
black.box.maker &amp;lt;- function(mod1) {
            d1 &amp;lt;- dim(mod1$model)[[1]]
            sumsq1 &amp;lt;- var(mod1$model[,1], na.rm=TRUE)*(d1-1)
            rt1 &amp;lt;- sqrt(sumsq1)
            sumsq2 &amp;lt;- var(mod1$fitted.values, na.rm=TRUE)*(d1-1)
            rsquare &amp;lt;- round(sumsq2/sumsq1, digits=4)
            rt2 &amp;lt;- sqrt(sumsq2)
            plot(x=NA, y=NA, xlim=c(0,rt1), ylim=c(0,rt1), main=paste(&amp;quot;R-squared:&amp;quot;,rsquare), xlab=&amp;quot;&amp;quot;, ylab=&amp;quot;&amp;quot;, bty=&amp;quot;n&amp;quot;, cex=0.5)
            polygon(x=c(0,0,rt1,rt1), y=c(0,rt1,rt1,0), col=&amp;quot;black&amp;quot;)
            polygon(x=c(0,0,rt2,rt2), y=c(0,rt2,rt2,0), col=&amp;quot;green&amp;quot;)
            }&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;OregonSalaries contains 32 observations: 16 males and 16 females. The mean of all salaries is 41142.433; let me put that in a plot in blue. The total sum of squares can be represented as the sum of all the squared distances to the blue line; each vertical distance is demarcated with an arrow below in blue. The distance from the point to the line is also shown in blue. It simply shows how far that individual’s salary is from the overall average. The total sum of squares: the total area of the black box in the original metric (squared dollars) is: 892955385.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ORSalScale &amp;lt;- scale(OregonSalaries$Salary, scale=FALSE)
plot(y=OregonSalaries$Salary, x=c(1:32), ylab=&amp;quot;Salary&amp;quot;, col=as.factor(OregonSalaries$Gender), xlab=&amp;quot;&amp;quot;, pch=c(rep(&amp;quot;F&amp;quot;,16),rep(&amp;quot;M&amp;quot;,16)))
abline(h=mean(OregonSalaries$Salary), col=&amp;quot;blue&amp;quot;)
arrows(x0=c(1:32), x1=c(1:32), y1=OregonSalaries$Salary,y0=mean(OregonSalaries$Salary), col=&amp;quot;blue&amp;quot;, code=3, length=0.05)
text(x=seq(1,16), y=rep(c(47000,48000,49000,50000),4), labels = paste(ceiling(ORSalScale[c(1:16)])), cex=0.7, col=&amp;quot;blue&amp;quot;)
text(x=c(17:32), y=rep(c(30000,31000,32000,33000),4), labels = paste(ceiling(ORSalScale[c(17:32)])), cex=0.7, col=&amp;quot;blue&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.data.viajes/post/2018-02-22-black-boxes-a-gender-gap-example_files/figure-html/PlotG-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;invoking-the-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Invoking the Function&lt;/h2&gt;
&lt;p&gt;To represent the black box,let me draw it. The length of each side will be the square root of the black box; our total sum of squares is just under 900 million squared dollars so each side will be approximately 30000. The box appears below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod1 &amp;lt;- lm(Salary~1, data=OregonSalaries)
black.box.maker(mod1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.data.viajes/post/2018-02-22-black-boxes-a-gender-gap-example_files/figure-html/BBBase-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-regression-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A Regression Model&lt;/h2&gt;
&lt;p&gt;First, a regression model. I will estimate the following regression:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Salary_{i} = \alpha + \beta_{1}*Gender_{i} + \epsilon \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;What does the regression imply? That salary for each individual &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; is a function of a constant and gender. Given the way that R works, &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; will represent the average for females and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; will represent the difference between male and female average salaries. The &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; will capture the difference between the individual salary and the group mean (the mean of males or females).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-new-residual-sum-of-squares&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A New Residual Sum of Squares&lt;/h2&gt;
&lt;p&gt;The picture will now have a red line and a black line and the residual/leftover/unexplained sum of squares is now the difference between the point and the respective vertical line (red arrows or black arrows). What is the relationship between the datum and the group mean? The answer is shown in black/red. The sum of the remaining squared vertical distances is 238621277 is obtained by squaring the difference between each black/red number. The amount explained by gender is the difference between each blue and the respective black/red number. It is important to notice that the highest paid females and the lowest paid males may be less well explained by two averages but the different averages, overall, lead to far less explained variation than a single average for all salaries.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;resids &amp;lt;- residuals(lm(Salary~Gender, data=OregonSalaries))
plot(y=OregonSalaries$Salary, x=c(1:32), ylab=&amp;quot;Salary&amp;quot;, col=as.factor(OregonSalaries$Gender), xlab=&amp;quot;&amp;quot;, pch=c(rep(&amp;quot;F&amp;quot;,16),rep(&amp;quot;M&amp;quot;,16)))
abline(h=mean(OregonSalaries$Salary), col=&amp;quot;blue&amp;quot;)
abline(h=mean(subset(OregonSalaries, Gender==&amp;quot;Female&amp;quot;)$Salary, na.rm=T), col=&amp;quot;black&amp;quot;)
abline(h=mean(subset(OregonSalaries, Gender==&amp;quot;Male&amp;quot;)$Salary, na.rm=T), col=&amp;quot;red&amp;quot;)
arrows(x0=c(1:32), x1=c(1:32), y1=OregonSalaries$Salary,y0=mean(OregonSalaries$Salary), col=&amp;quot;blue&amp;quot;, code=3, length=0.05)
arrows(x0=c(1:16), x1=c(1:16), y1=OregonSalaries$Salary[c(1:16)],y0=mean(subset(OregonSalaries, Gender==&amp;quot;Female&amp;quot;)$Salary, na.rm=T), col=&amp;quot;black&amp;quot;, code=3, length=0.05)
arrows(x0=c(17:32), x1=c(17:32), y1=OregonSalaries$Salary[c(17:32)],y0=mean(subset(OregonSalaries, Gender==&amp;quot;Male&amp;quot;)$Salary, na.rm=T), col=&amp;quot;red&amp;quot;, code=3, length=0.05)
text(x=seq(1,16), y=rep(c(47000,48000,49000,50000),4), labels = paste(ceiling(ORSalScale[c(1:16)])), cex=0.7, col=&amp;quot;blue&amp;quot;)
text(x=c(17:32), y=rep(c(30000,31000,32000,33000),4), labels = paste(ceiling(ORSalScale[c(17:32)])), cex=0.7, col=&amp;quot;blue&amp;quot;)
text(x=seq(1,16), y=rep(c(42000,43000,44000,45000),4), labels = paste(ceiling(resids[c(1:16)])), cex=0.7, col=&amp;quot;black&amp;quot;)
text(x=c(17:32), y=rep(c(36000,37000,38000,39000),4), labels = paste(ceiling(resids[c(17:32)])), cex=0.7, col=&amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.data.viajes/post/2018-02-22-black-boxes-a-gender-gap-example_files/figure-html/BasePlot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The details of the regression estimates and the analysis of variance – the sums of squares – completes the rendering.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod1 &amp;lt;- lm(Salary~Gender, data=OregonSalaries)
black.box.maker(mod1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.data.viajes/post/2018-02-22-black-boxes-a-gender-gap-example_files/figure-html/BBReg-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(mod1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = Salary ~ Gender, data = OregonSalaries)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -7488.7 -2107.9   433.3  1743.9  4893.9 
## 
## Coefficients:
##             Estimate Std. Error t value       Pr(&amp;gt;|t|)    
## (Intercept)  36620.5      705.1   51.94        &amp;lt; 2e-16 ***
## GenderMale    9043.9      997.1    9.07 0.000000000422 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 2820 on 30 degrees of freedom
## Multiple R-squared:  0.7328, Adjusted R-squared:  0.7239 
## F-statistic: 82.26 on 1 and 30 DF,  p-value: 0.0000000004223&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anova(mod1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Analysis of Variance Table
## 
## Response: Salary
##           Df    Sum Sq   Mean Sq F value          Pr(&amp;gt;F)    
## Gender     1 654334108 654334108  82.264 0.0000000004223 ***
## Residuals 30 238621277   7954043                            
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Correlation Function</title>
      <link>https://www.data.viajes/post/correlation-function/</link>
      <pubDate>Thu, 22 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.data.viajes/post/correlation-function/</guid>
      <description>&lt;div id=&#34;correlations-and-the-impact-on-sums-and-differences&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Correlations and the Impact on Sums and Differences&lt;/h1&gt;
&lt;p&gt;I will use a simple R function to illustrate the effect of correlation on sums and differences of random variables. In general, the variance [and standard deviation] of a sum of random variables is the variance of the individual variables plus twice the covariance; the variance [and standard deviation] of a difference in random variables is the variance of the individual variables minus twice the (signed) covariance.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Var (\sum_{i=1}^{n} X_{i}) = \sum_{i=1}^{n} Var(X_{i}) + 2 \sum_{1 \leq i \leq j \leq n} Cov(X_{i},X_{j}) \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Now for the function and two examples.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;correlation-is-0.8&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Correlation is 0.8&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(MASS)
plot.cor &amp;lt;- function(cor) {
  if(-1 &amp;lt; cor &amp;amp; cor &amp;lt; 1) {
mean.vec &amp;lt;- c(0,0)
sig.mat &amp;lt;- matrix(c(1,cor,cor,1), nrow=2)
df &amp;lt;- data.frame(mvrnorm(n=1000, mean.vec, sig.mat))
df$sum &amp;lt;- rowSums(df)
df$diff &amp;lt;- with(df, X1-X2)
plot(x=df$X1, y=df$X2, xlab=&amp;quot;x1&amp;quot;, ylab=&amp;quot;x2&amp;quot;, main=paste(&amp;quot;Correlation:&amp;quot;,cor), sub=paste(&amp;quot;Std. Dev: Sum&amp;quot;,round(sd(df$sum), digits=3),&amp;quot; Difference:&amp;quot;,round(sd(df$diff), digits=3)))
  }
  else { cat(&amp;quot;Correlation must be between -1 and 1&amp;quot;) }
}
plot.cor(cor=0.8)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.data.viajes/post/2018-02-22-correlation-function_files/figure-html/CorF-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The correlation above is 0.8&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;correlation-is--0.8&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Correlation is -0.8&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot.cor(cor=-0.8)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.data.viajes/post/2018-02-22-correlation-function_files/figure-html/CorF2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>tidytext is neat! White House Communications</title>
      <link>https://www.data.viajes/post/tidytext-is-neat/</link>
      <pubDate>Wed, 21 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.data.viajes/post/tidytext-is-neat/</guid>
      <description>&lt;div id=&#34;presidential-press&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Presidential Press&lt;/h1&gt;
&lt;p&gt;The language of presidential communications is interesting and I know very little about &lt;em&gt;text as data&lt;/em&gt;. I have a number of applications in mind for these tools but I have to learn how to use them. What does the website look like?&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.whitehouse.gov/news/&#34;&gt;White House News&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The site is split in four parts: all news, articles, presidential actions, and briefings and statements. The first one is a catch all and the second is news links. I will take the last two to process. To create a proper workflow, I will separate the investigation into two types of communications: briefing statements and presidential actions. For each, I will have to build a table of links and then I can extract the actual text.&lt;/p&gt;
&lt;div id=&#34;processing-the-communications-links&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Processing the Communications: Links&lt;/h2&gt;
&lt;p&gt;First, let me take on briefing statements. I will build a database of URLs to then process as text. This works for the design of the White House website currently; the only relevant hard-coding is the number of browsable pages. I captured this manually.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rvest)
n.BSt &amp;lt;- 208
BSt.seq &amp;lt;- as.list(seq(1,n.BSt))
BSt.fun &amp;lt;- function(val) {
my.URL &amp;lt;- paste(&amp;quot;https://www.whitehouse.gov/briefings-statements/page/&amp;quot;,val,&amp;quot;/&amp;quot;,sep=&amp;quot;&amp;quot;)
temp.l1 &amp;lt;- read_html(my.URL)
my.links &amp;lt;- html_nodes(temp.l1, &amp;#39;h2&amp;#39;) %&amp;gt;% html_nodes(&amp;quot;a&amp;quot;) %&amp;gt;% html_attr(&amp;#39;href&amp;#39;)
my.links2 &amp;lt;-html_nodes(temp.l1, &amp;#39;h2&amp;#39;) %&amp;gt;% html_text(&amp;quot;a&amp;quot;) 
data.frame(link=my.links,title=my.links2)
}
n.PAct &amp;lt;- 46
PAct.seq &amp;lt;- as.list(seq(1,n.PAct))
PAct.fun &amp;lt;- function(val) {
my.URL &amp;lt;- paste(&amp;quot;https://www.whitehouse.gov/presidential-actions/page/&amp;quot;,val,&amp;quot;/&amp;quot;,sep=&amp;quot;&amp;quot;)
temp.l1 &amp;lt;- read_html(my.URL)
my.links &amp;lt;- html_nodes(temp.l1, &amp;#39;h2&amp;#39;) %&amp;gt;% html_nodes(&amp;quot;a&amp;quot;) %&amp;gt;% html_attr(&amp;#39;href&amp;#39;)
my.links2 &amp;lt;-html_nodes(temp.l1, &amp;#39;h2&amp;#39;) %&amp;gt;% html_nodes(&amp;quot;a&amp;quot;) %&amp;gt;% html_text(&amp;quot;a&amp;quot;) 
data.frame(link=my.links,title=my.links2)
}
BriefState.linkset &amp;lt;- do.call(&amp;quot;rbind&amp;quot;,rapply(BSt.seq, BSt.fun, how=&amp;quot;list&amp;quot;))
PresAct.linkset &amp;lt;- do.call(&amp;quot;rbind&amp;quot;,rapply(PAct.seq,PAct.fun, how=&amp;quot;list&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I now have all the links. I cannot do much with that.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;text-extraction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Text Extraction&lt;/h2&gt;
&lt;p&gt;I will first write a simple function to download a URL and extract the text that I want.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rlist)
library(stringr)
PPR.Filter &amp;lt;- function(file) {
temp.res &amp;lt;- str_replace_all(html_text(html_nodes(file, xpath=&amp;#39;(//*[contains(concat( &amp;quot; &amp;quot;, @class, &amp;quot; &amp;quot; ), concat( &amp;quot; &amp;quot;, &amp;quot;editor&amp;quot;, &amp;quot; &amp;quot; ))])//*[not(ancestor::aside or name()=&amp;quot;aside&amp;quot;)]/text()&amp;#39;)), &amp;quot;[\t\n]&amp;quot; , &amp;quot;&amp;quot;)
temp.res
}
web.fetch &amp;lt;- function(URL) {
temp.web &amp;lt;- read_html(URL)
}
PPR.Filter.Wrap &amp;lt;- function(URL) {
  temp.res &amp;lt;- PPR.Filter(web.fetch(URL))
  temp.res &amp;lt;- list.clean(temp.res, function(x) nchar(x) == 0, TRUE)
  temp.res 
}
#Res1 &amp;lt;- PPR.Filter.Wrap(&amp;quot;https://www.whitehouse.gov/briefings-statements/president-donald-j-trumps-first-year-of-foreign-policy-accomplishments/&amp;quot;)
#Res2 &amp;lt;- PPR.Filter.Wrap(&amp;quot;https://www.whitehouse.gov/briefings-statements/press-briefing-press-secretary-sarah-sanders-121917/&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;scraping-presidential-actions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Scraping Presidential Actions&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Pres.Acts &amp;lt;- lapply(as.character(PresAct.linkset$link), PPR.Filter.Wrap)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;scraping-the-briefings-and-statements&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Scraping the Briefings and Statements&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Statements.Briefings &amp;lt;- lapply(as.character(BriefState.linkset$link), PPR.Filter.Wrap)
save(Pres.Acts,PresAct.linkset,Statements.Briefings,BriefState.linkset, file=&amp;quot;data/PresText.RData&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;tidying-the-text&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Tidying the Text&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;load(&amp;quot;~/R/MyNLWeb/data/PresText.RData&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The hard work is in cleaning up the text. When the document was compiled, there were 2074 statements and briefings and there were 457 Presidential actions with each as a list in the bigger list. I will unlist each individual document and transform it to character. For housekeeping, I will also tally the docs and the line/paragraph numbers; this fails for a misalignment in one of the two examples.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(magrittr)
library(tidytext)
text_df &amp;lt;- data_frame(text=as.character(unlist(Pres.Acts))) # Create characters
k &amp;lt;- NULL
for (i in 1:length(Pres.Acts)) {
  k &amp;lt;- c(k,length(Pres.Acts[[i]]))
}
mydoc &amp;lt;- data.frame(rep(c(seq(1,length(Pres.Acts))),k))
myline &amp;lt;- data.frame(unlist(as.vector(sapply(k, function(x) {cbind(seq(1,x))}))))
ind.df &amp;lt;- data.frame(doc=mydoc,line=myline)
myPA.df &amp;lt;- data.frame(doc=mydoc,line=myline,text=text_df) # A full dataset
names(myPA.df) &amp;lt;- c(&amp;quot;doc&amp;quot;,&amp;quot;line&amp;quot;,&amp;quot;text&amp;quot;)
tidy.PA &amp;lt;-myPA.df %&amp;gt;%
# group_by(doc) %&amp;gt;%
 unnest_tokens(word, text)
text_df &amp;lt;- data_frame(text=as.character(unlist(Statements.Briefings)))
k &amp;lt;- NULL
for (i in 1:length(Statements.Briefings)) {
  k &amp;lt;- c(k,length(Statements.Briefings[[i]]))
}
mydoc &amp;lt;- rep(c(seq(1,length(Statements.Briefings))),k)
# myline &amp;lt;- unlist(as.vector(sapply(k, function(x) {cbind(seq(1,x))})))
# ind.df &amp;lt;- data.frame(doc=mydoc,line=myline)
mySB.df &amp;lt;- data.frame(doc=mydoc,text=text_df)
names(mySB.df) &amp;lt;- c(&amp;quot;doc&amp;quot;,&amp;quot;text&amp;quot;)
tidy.SB &amp;lt;-mySB.df %&amp;gt;%
# group_by(doc) %&amp;gt;%
 unnest_tokens(word, text)
data(stop_words)
# Remove stop words
tidy.SB &amp;lt;- tidy.SB %&amp;gt;%
  anti_join(stop_words)
tidy.PA &amp;lt;- tidy.PA %&amp;gt;%
  anti_join(stop_words)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;what-does-the-president-talk-about&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;what does the President talk about?&lt;/h1&gt;
&lt;p&gt;Word frequencies can be tabulated for each set of data. I will plot the barplots.&lt;/p&gt;
&lt;div id=&#34;statements-and-briefings&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Statements and Briefings&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
tidy.SB %&amp;gt;%
  count(word, sort = TRUE) %&amp;gt;%
  filter(n &amp;gt; 5000) %&amp;gt;%
  mutate(word = reorder(word, n)) %&amp;gt;%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.data.viajes/post/2018-02-21-tidytext-is-neat_files/figure-html/SBplot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;presidential-actions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Presidential Actions&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
tidy.PA %&amp;gt;%
  count(word, sort = TRUE) %&amp;gt;%
  filter(n &amp;gt; 500) %&amp;gt;%
  mutate(word = reorder(word, n)) %&amp;gt;%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.data.viajes/post/2018-02-21-tidytext-is-neat_files/figure-html/PAplot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;word-clouds&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Word Clouds&lt;/h2&gt;
&lt;div id=&#34;presidential-actions-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Presidential Actions&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(wordcloud)
set.seed(1234)
wc &amp;lt;- tidy.PA %&amp;gt;% count(word, sort = TRUE)
wordcloud(wc$word,  wc$n, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, &amp;quot;Dark2&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.data.viajes/post/2018-02-21-tidytext-is-neat_files/figure-html/WCPA-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;statements-and-briefings-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Statements and Briefings&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1234)
wc &amp;lt;- tidy.SB %&amp;gt;% count(word, sort = TRUE)
wordcloud(wc$word,  wc$n, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, &amp;quot;Dark2&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.data.viajes/post/2018-02-21-tidytext-is-neat_files/figure-html/WCSB-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Example Talk</title>
      <link>https://www.data.viajes/talk/example-talk/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 -0800</pubDate>
      
      <guid>https://www.data.viajes/talk/example-talk/</guid>
      <description>&lt;p&gt;Embed your slides or video here using &lt;a href=&#34;https://sourcethemes.com/academic/post/writing-markdown-latex/&#34; target=&#34;_blank&#34;&gt;shortcodes&lt;/a&gt;. Further details can easily be added using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
