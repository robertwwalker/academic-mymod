<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>tidyverse on Robert W. Walker</title>
    <link>/tags/tidyverse/</link>
    <description>Recent content in tidyverse on Robert W. Walker</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Thu, 10 Oct 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/tidyverse/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Fariss Human Rights Data with Animation</title>
      <link>/post/fariss-human-rights-data-with-animation/</link>
      <pubDate>Thu, 10 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/fariss-human-rights-data-with-animation/</guid>
      <description>Fariss Data Is neat and complete.
load(&amp;quot;~/FarissHRData.RData&amp;quot;) skimr::skim(HR.Data) ## Skim summary statistics ## n obs: 11717 ## n variables: 27 ## ## ── Variable type:factor ──────────────────────────────────────────────────────────────────────── ## variable missing complete n n_unique top_counts ## COW_YEAR 0 11717 11717 11717 100: 1, 100: 1, 100: 1, 100: 1 ## ordered ## FALSE ## ## ── Variable type:integer ─────────────────────────────────────────────────────────────────────── ## variable missing complete n mean sd p0 p25 ## Amnesty 6140 5577 11717 2.</description>
    </item>
    
    <item>
      <title>Tables, Pivots, Bars, and Mosaics</title>
      <link>/r/tables-pivots-bars-and-mosaics/</link>
      <pubDate>Wed, 09 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/r/tables-pivots-bars-and-mosaics/</guid>
      <description>R Markdown There is detailed help for all that Markdown can do under Help in the RStudio. The key to it is knitting documents with the Knit button in the RStudio. If we use helpers like the R Commander, Radiant, or esquisse, we will need the R code implanted in the Markdown document in particular ways. I will use Markdown for everything. I even use a close relation of Markdown in my scholarly pursuits.</description>
    </item>
    
    <item>
      <title>The Economist&#39;s Visualization Errors</title>
      <link>/post/the-economist-s-visualization-errors/</link>
      <pubDate>Wed, 09 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/the-economist-s-visualization-errors/</guid>
      <description>The Economist’s Errors and Credit Where Credit is Due The Economist is serious about their use of data visualization and they have occasionally owned up to errors in their visualizations. They can be deceptive, uninformative, confusing, excessively busy, and present a host of other barriers to clean communication. Their blog post on their errors is great.
I have drawn the following example from a #tidyTuesday earlier this year that explores this.</description>
    </item>
    
    <item>
      <title>nflscrapR is amazing</title>
      <link>/post/nflscrapr-is-amazing/</link>
      <pubDate>Tue, 30 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/nflscrapr-is-amazing/</guid>
      <description>Scraping NFL data Note: An original version of this post had issues induced by overtime games. There is a better way to handle all of this that I learned from a brief analysis of a tie game between Cleveland and Pittsburgh in Week One.
The nflscrapR package is designed to make data on NFL games more easily available. To install the package, we need to grab it from github.</description>
    </item>
    
    <item>
      <title>Visualisation with Archigos: Leaders of the World</title>
      <link>/post/visualisation-with-archigos-leaders-of-the-world/</link>
      <pubDate>Sun, 14 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/visualisation-with-archigos-leaders-of-the-world/</guid>
      <description>Archigos Is an amazing collaboration that produced a comprehensive dataset of world leaders going pretty far back; see Archigos on the web. For thinking about leadership, it is quite natural. In this post, I want to do some reshaping into country year and leader year datasets and explore the basic confines of Archigos. I also want to use gganimate for a few things. So what do we know?
library(lubridate) library(tidyverse) library(ggthemes) library(stringr) library(gganimate) library(emoGG) library(emojifont) library(haven) Archigos &amp;lt;- read_dta(url(&amp;quot;http://www.</description>
    </item>
    
    <item>
      <title>Trump Tweet Word Clouds</title>
      <link>/post/trump-tweet-word-clouds/</link>
      <pubDate>Tue, 18 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/trump-tweet-word-clouds/</guid>
      <description>Mining Twitter Data Is rather easy. You have to arrange a developer account with Twitter and set up an app. After that, Twitter gives you access to a consumer key and secret and an access token and access secret. My tool of choice for this is rtweet because it automagically processes tweet elements and makes them easy to slice and dice. I also played with twitteR but it was harder to work with for what I wanted.</description>
    </item>
    
    <item>
      <title>tidyTuesday: coffee chains</title>
      <link>/post/tidytuesday-coffee-chains/</link>
      <pubDate>Wed, 09 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/tidytuesday-coffee-chains/</guid>
      <description>The tidyTuesday for this week is coffee chain locations For this week: 1. The basic link to the #tidyTuesday shows an original article for Week 6.
First, let’s import the data; it is a single Excel spreadsheet. The page notes that starbucks, Tim Horton, and Dunkin Donuts have raw data available.
library(readxl) library(tidyverse) ## ── Attaching packages ───────────────────────────────────────────────── tidyverse 1.2.1 ── ## ✔ ggplot2 3.2.1 ✔ purrr 0.3.2 ## ✔ tibble 2.</description>
    </item>
    
    <item>
      <title>Global mortality tidyTuesday</title>
      <link>/post/tidytuesday-takes-on-global-mortality/</link>
      <pubDate>Wed, 18 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/tidytuesday-takes-on-global-mortality/</guid>
      <description>tidyTuesday on Global Mortality The three generic challenge graphics involve two global summaries, a raw count by type and a percentage by type. The individual county breakdowns are recorded for a predetermined year below. This can all be seen in the original. For whatever reason, I cannot open this data remotely.
Here is this week’s tidyTuesday.
library(skimr) library(tidyverse) library(rlang) # global_mortality &amp;lt;- readRDS(&amp;quot;../../data/global_mortality.rds&amp;quot;) global_mortality &amp;lt;- readRDS(url(&amp;quot;https://github.com/robertwwalker/academic-mymod/raw/master/data/global_mortality.rds&amp;quot;)) skim(global_mortality) ## Skim summary statistics ## n obs: 6156 ## n variables: 35 ## ## ── Variable type:character ─────────────────────────────────────────────────────────────── ## variable missing complete n min max empty n_unique ## country 0 6156 6156 4 32 0 228 ## country_code 864 5292 6156 3 8 0 196 ## ## ── Variable type:numeric ───────────────────────────────────────────────────────────────── ## variable missing complete n mean ## Alcohol disorders (%) 0 6156 6156 0.</description>
    </item>
    
    <item>
      <title>Scraping the NFL Salary Cap Data with Python and R</title>
      <link>/post/scraping-the-nfl-salary-cap-data-with-python-and-r/</link>
      <pubDate>Wed, 04 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/scraping-the-nfl-salary-cap-data-with-python-and-r/</guid>
      <description>The NFL Data [SporTrac](http://www.sportrac.com] has a wonderful array of financial data on sports. A student going to work for the Seattle Seahawks wanted the NFL salary cap data and I also found data on the English Premier League there. Now I have a source to scrape the data from.
With a source in hand, the key tool is the SelectorGadget. SelectorGadget is a browser add-in for Chrome that allows us to select text and identify the css or xpath selector to scrape the data.</description>
    </item>
    
    <item>
      <title>tidyTuesday - Tuition</title>
      <link>/post/tidytuesday-tuition/</link>
      <pubDate>Tue, 03 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/tidytuesday-tuition/</guid>
      <description>I found a great example on tidyTuesday that I wanted to work on. @JakeKaupp tweeted his #tidyTuesday: a very cool slope plot of tuition changes averaged by state over the last decade. It is a very informative graphic. The only tweak is a simple embedded line plot that uses color in a creative way to show growth rates. All of the R code for this is on Jake Kaupp’s GitHub.</description>
    </item>
    
    <item>
      <title>tidytext is neat! White House Communications</title>
      <link>/post/tidytext-is-neat/</link>
      <pubDate>Wed, 21 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/tidytext-is-neat/</guid>
      <description>Presidential Press The language of presidential communications is interesting and I know very little about text as data. I have a number of applications in mind for these tools but I have to learn how to use them. What does the website look like?
White House News
The site is split in four parts: all news, articles, presidential actions, and briefings and statements. The first one is a catch all and the second is news links.</description>
    </item>
    
  </channel>
</rss>